[
  {
    "objectID": "posts/z2h_makemore_pt1/index.html",
    "href": "posts/z2h_makemore_pt1/index.html",
    "title": "z2h 2. Intro to language modeling: building makemore",
    "section": "",
    "text": "import torch\nfrom torch import Generator\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nmakemore: autoregressive (using past data to predict future behavior) character-level language model that “makes more” of things that you give it\nExample: can learn to make more names (or things that sound name-like) if you train makemore on a dataset of names:\nhttps://github.com/karpathy/makemore/blob/master/names.txt\n\neach line is an example (e.g. “iris”)\nwithin each example, treat it as a sequence of individual characters (the letters in “iris”)\n\n\n#read in everything into one giant string -&gt; splitlines to get a list of strings\nwords = open('../z2h_data/names.txt', 'r').read().splitlines()\n\n\nwords[:10]\n\n['emma',\n 'olivia',\n 'ava',\n 'isabella',\n 'sophia',\n 'charlotte',\n 'mia',\n 'amelia',\n 'harper',\n 'evelyn']\n\n\n\nlen(words) #around 32k names\n\n32033\n\n\n\nmin(len(word) for word in words) #shortest is 2 characters\n\n2\n\n\n\nmax(len(word) for word in words) #longest: 15\n\n15\n\n\nCharacter-level language model: predicting next character in sequence given already some concrete sequence before\nWhat does the existence of a word (e.g. “isabella”) in this dataset tell us?\n\nthe character “i” is a very likely character to come first in sequence of a name\ncharacter “s” is likely to come after “i”\n…“a” after “is”\n…“b” after “isa”\n…\none more example: after “isabella” the word is likely to end\n\n*a lot of info packed into this statistical structure of what’s likely to followy in character sequence!\nStart by building bigram language model: works only w/ 2 characters at a time\n\nonly looking at one character (given)\ntry to predict next character in sequence\nvery small local structure (just looking at previous to predict next)\n\n\n#ex\nfor word in words[:1]:\n    #iterate 2 characters at a time -- slide across (zip will halt+return when shorter list is exhausted)\n    for ch1, ch2 in zip(word, word[1:]):\n        print(ch1, ch2)\n\ne m\nm m\nm a\n\n\nWhat we see from this above word: “e” is very likely to come first; “a” very likely to come last\n\nzip will halt and return when we exhaust the shorter list\nneed to find a way to mark beginning and end so that everything is fully “paired”\n\nPrinted below: we start with the bigram of the special start character and “e”, end with the bigram of “a” an the special end character\n\nfor word in words[:1]:\n    #special array of characters\n    #hallucinate special start token\n    #list(w) to get list of individual characters instead of just a string of whole word\n    chs = ['&lt;S&gt;'] + list(word) + ['&lt;E&gt;']\n    for ch1, ch2 in zip(chs, chs[1:]):\n        print(ch1, ch2)\n\n&lt;S&gt; e\ne m\nm m\nm a\na &lt;E&gt;\n\n\n\n#now with more words\nfor word in words[:3]:\n    chs = ['&lt;S&gt;'] + list(word) + ['&lt;E&gt;']\n    for ch1, ch2 in zip(chs, chs[1:]):\n        print(ch1, ch2) #print individual character bigrams\n\n&lt;S&gt; e\ne m\nm m\nm a\na &lt;E&gt;\n&lt;S&gt; o\no l\nl i\ni v\nv i\ni a\na &lt;E&gt;\n&lt;S&gt; a\na v\nv a\na &lt;E&gt;\n\n\nTo learn statistics about which characters are likely to follow other characters, simplest way is to count. i.e. count how often any one of these combinations occurs in training set in these words\n\nneed dictionary to maintain counts to map bigrams (tuple of character 1, character 2) to their frequency\n\n\nbigram_counts = {}\nfor word in words:\n    chs = ['&lt;S&gt;'] + list(word) + ['&lt;E&gt;']\n    for ch1, ch2 in zip(chs, chs[1:]):\n        bigram = (ch1, ch2)\n        bigram_counts[bigram] = bigram_counts.get(bigram, 0)+1\n\n\n#alternative method (not from demo)\ndef sort_dict(unsorted_dict):\n    \"\"\"sort dictionary using swapped keys and values\"\"\"\n    unsorted_list = [(value, key) for (key, value) in unsorted_dict.items()]\n    sorted_list = sorted(unsorted_list, reverse=True)\n    sorted_dict = {key:value for value, key in sorted_list}\n    return sorted_dict\n\n\nsorted(bigram_counts.items(), key = lambda kv: -kv[1]) #list with entries from most frequent to least\n\n[(('n', '&lt;E&gt;'), 6763),\n (('a', '&lt;E&gt;'), 6640),\n (('a', 'n'), 5438),\n (('&lt;S&gt;', 'a'), 4410),\n (('e', '&lt;E&gt;'), 3983),\n (('a', 'r'), 3264),\n (('e', 'l'), 3248),\n (('r', 'i'), 3033),\n (('n', 'a'), 2977),\n (('&lt;S&gt;', 'k'), 2963),\n (('l', 'e'), 2921),\n (('e', 'n'), 2675),\n (('l', 'a'), 2623),\n (('m', 'a'), 2590),\n (('&lt;S&gt;', 'm'), 2538),\n (('a', 'l'), 2528),\n (('i', '&lt;E&gt;'), 2489),\n (('l', 'i'), 2480),\n (('i', 'a'), 2445),\n (('&lt;S&gt;', 'j'), 2422),\n (('o', 'n'), 2411),\n (('h', '&lt;E&gt;'), 2409),\n (('r', 'a'), 2356),\n (('a', 'h'), 2332),\n (('h', 'a'), 2244),\n (('y', 'a'), 2143),\n (('i', 'n'), 2126),\n (('&lt;S&gt;', 's'), 2055),\n (('a', 'y'), 2050),\n (('y', '&lt;E&gt;'), 2007),\n (('e', 'r'), 1958),\n (('n', 'n'), 1906),\n (('y', 'n'), 1826),\n (('k', 'a'), 1731),\n (('n', 'i'), 1725),\n (('r', 'e'), 1697),\n (('&lt;S&gt;', 'd'), 1690),\n (('i', 'e'), 1653),\n (('a', 'i'), 1650),\n (('&lt;S&gt;', 'r'), 1639),\n (('a', 'm'), 1634),\n (('l', 'y'), 1588),\n (('&lt;S&gt;', 'l'), 1572),\n (('&lt;S&gt;', 'c'), 1542),\n (('&lt;S&gt;', 'e'), 1531),\n (('j', 'a'), 1473),\n (('r', '&lt;E&gt;'), 1377),\n (('n', 'e'), 1359),\n (('l', 'l'), 1345),\n (('i', 'l'), 1345),\n (('i', 's'), 1316),\n (('l', '&lt;E&gt;'), 1314),\n (('&lt;S&gt;', 't'), 1308),\n (('&lt;S&gt;', 'b'), 1306),\n (('d', 'a'), 1303),\n (('s', 'h'), 1285),\n (('d', 'e'), 1283),\n (('e', 'e'), 1271),\n (('m', 'i'), 1256),\n (('s', 'a'), 1201),\n (('s', '&lt;E&gt;'), 1169),\n (('&lt;S&gt;', 'n'), 1146),\n (('a', 's'), 1118),\n (('y', 'l'), 1104),\n (('e', 'y'), 1070),\n (('o', 'r'), 1059),\n (('a', 'd'), 1042),\n (('t', 'a'), 1027),\n (('&lt;S&gt;', 'z'), 929),\n (('v', 'i'), 911),\n (('k', 'e'), 895),\n (('s', 'e'), 884),\n (('&lt;S&gt;', 'h'), 874),\n (('r', 'o'), 869),\n (('e', 's'), 861),\n (('z', 'a'), 860),\n (('o', '&lt;E&gt;'), 855),\n (('i', 'r'), 849),\n (('b', 'r'), 842),\n (('a', 'v'), 834),\n (('m', 'e'), 818),\n (('e', 'i'), 818),\n (('c', 'a'), 815),\n (('i', 'y'), 779),\n (('r', 'y'), 773),\n (('e', 'm'), 769),\n (('s', 't'), 765),\n (('h', 'i'), 729),\n (('t', 'e'), 716),\n (('n', 'd'), 704),\n (('l', 'o'), 692),\n (('a', 'e'), 692),\n (('a', 't'), 687),\n (('s', 'i'), 684),\n (('e', 'a'), 679),\n (('d', 'i'), 674),\n (('h', 'e'), 674),\n (('&lt;S&gt;', 'g'), 669),\n (('t', 'o'), 667),\n (('c', 'h'), 664),\n (('b', 'e'), 655),\n (('t', 'h'), 647),\n (('v', 'a'), 642),\n (('o', 'l'), 619),\n (('&lt;S&gt;', 'i'), 591),\n (('i', 'o'), 588),\n (('e', 't'), 580),\n (('v', 'e'), 568),\n (('a', 'k'), 568),\n (('a', 'a'), 556),\n (('c', 'e'), 551),\n (('a', 'b'), 541),\n (('i', 't'), 541),\n (('&lt;S&gt;', 'y'), 535),\n (('t', 'i'), 532),\n (('s', 'o'), 531),\n (('m', '&lt;E&gt;'), 516),\n (('d', '&lt;E&gt;'), 516),\n (('&lt;S&gt;', 'p'), 515),\n (('i', 'c'), 509),\n (('k', 'i'), 509),\n (('o', 's'), 504),\n (('n', 'o'), 496),\n (('t', '&lt;E&gt;'), 483),\n (('j', 'o'), 479),\n (('u', 's'), 474),\n (('a', 'c'), 470),\n (('n', 'y'), 465),\n (('e', 'v'), 463),\n (('s', 's'), 461),\n (('m', 'o'), 452),\n (('i', 'k'), 445),\n (('n', 't'), 443),\n (('i', 'd'), 440),\n (('j', 'e'), 440),\n (('a', 'z'), 435),\n (('i', 'g'), 428),\n (('i', 'm'), 427),\n (('r', 'r'), 425),\n (('d', 'r'), 424),\n (('&lt;S&gt;', 'f'), 417),\n (('u', 'r'), 414),\n (('r', 'l'), 413),\n (('y', 's'), 401),\n (('&lt;S&gt;', 'o'), 394),\n (('e', 'd'), 384),\n (('a', 'u'), 381),\n (('c', 'o'), 380),\n (('k', 'y'), 379),\n (('d', 'o'), 378),\n (('&lt;S&gt;', 'v'), 376),\n (('t', 't'), 374),\n (('z', 'e'), 373),\n (('z', 'i'), 364),\n (('k', '&lt;E&gt;'), 363),\n (('g', 'h'), 360),\n (('t', 'r'), 352),\n (('k', 'o'), 344),\n (('t', 'y'), 341),\n (('g', 'e'), 334),\n (('g', 'a'), 330),\n (('l', 'u'), 324),\n (('b', 'a'), 321),\n (('d', 'y'), 317),\n (('c', 'k'), 316),\n (('&lt;S&gt;', 'w'), 307),\n (('k', 'h'), 307),\n (('u', 'l'), 301),\n (('y', 'e'), 301),\n (('y', 'r'), 291),\n (('m', 'y'), 287),\n (('h', 'o'), 287),\n (('w', 'a'), 280),\n (('s', 'l'), 279),\n (('n', 's'), 278),\n (('i', 'z'), 277),\n (('u', 'n'), 275),\n (('o', 'u'), 275),\n (('n', 'g'), 273),\n (('y', 'd'), 272),\n (('c', 'i'), 271),\n (('y', 'o'), 271),\n (('i', 'v'), 269),\n (('e', 'o'), 269),\n (('o', 'm'), 261),\n (('r', 'u'), 252),\n (('f', 'a'), 242),\n (('b', 'i'), 217),\n (('s', 'y'), 215),\n (('n', 'c'), 213),\n (('h', 'y'), 213),\n (('p', 'a'), 209),\n (('r', 't'), 208),\n (('q', 'u'), 206),\n (('p', 'h'), 204),\n (('h', 'r'), 204),\n (('j', 'u'), 202),\n (('g', 'r'), 201),\n (('p', 'e'), 197),\n (('n', 'l'), 195),\n (('y', 'i'), 192),\n (('g', 'i'), 190),\n (('o', 'd'), 190),\n (('r', 's'), 190),\n (('r', 'd'), 187),\n (('h', 'l'), 185),\n (('s', 'u'), 185),\n (('a', 'x'), 182),\n (('e', 'z'), 181),\n (('e', 'k'), 178),\n (('o', 'v'), 176),\n (('a', 'j'), 175),\n (('o', 'h'), 171),\n (('u', 'e'), 169),\n (('m', 'm'), 168),\n (('a', 'g'), 168),\n (('h', 'u'), 166),\n (('x', '&lt;E&gt;'), 164),\n (('u', 'a'), 163),\n (('r', 'm'), 162),\n (('a', 'w'), 161),\n (('f', 'i'), 160),\n (('z', '&lt;E&gt;'), 160),\n (('u', '&lt;E&gt;'), 155),\n (('u', 'm'), 154),\n (('e', 'c'), 153),\n (('v', 'o'), 153),\n (('e', 'h'), 152),\n (('p', 'r'), 151),\n (('d', 'd'), 149),\n (('o', 'a'), 149),\n (('w', 'e'), 149),\n (('w', 'i'), 148),\n (('y', 'm'), 148),\n (('z', 'y'), 147),\n (('n', 'z'), 145),\n (('y', 'u'), 141),\n (('r', 'n'), 140),\n (('o', 'b'), 140),\n (('k', 'l'), 139),\n (('m', 'u'), 139),\n (('l', 'd'), 138),\n (('h', 'n'), 138),\n (('u', 'd'), 136),\n (('&lt;S&gt;', 'x'), 134),\n (('t', 'l'), 134),\n (('a', 'f'), 134),\n (('o', 'e'), 132),\n (('e', 'x'), 132),\n (('e', 'g'), 125),\n (('f', 'e'), 123),\n (('z', 'l'), 123),\n (('u', 'i'), 121),\n (('v', 'y'), 121),\n (('e', 'b'), 121),\n (('r', 'h'), 121),\n (('j', 'i'), 119),\n (('o', 't'), 118),\n (('d', 'h'), 118),\n (('h', 'm'), 117),\n (('c', 'l'), 116),\n (('o', 'o'), 115),\n (('y', 'c'), 115),\n (('o', 'w'), 114),\n (('o', 'c'), 114),\n (('f', 'r'), 114),\n (('b', '&lt;E&gt;'), 114),\n (('m', 'b'), 112),\n (('z', 'o'), 110),\n (('i', 'b'), 110),\n (('i', 'u'), 109),\n (('k', 'r'), 109),\n (('g', '&lt;E&gt;'), 108),\n (('y', 'v'), 106),\n (('t', 'z'), 105),\n (('b', 'o'), 105),\n (('c', 'y'), 104),\n (('y', 't'), 104),\n (('u', 'b'), 103),\n (('u', 'c'), 103),\n (('x', 'a'), 103),\n (('b', 'l'), 103),\n (('o', 'y'), 103),\n (('x', 'i'), 102),\n (('i', 'f'), 101),\n (('r', 'c'), 99),\n (('c', '&lt;E&gt;'), 97),\n (('m', 'r'), 97),\n (('n', 'u'), 96),\n (('o', 'p'), 95),\n (('i', 'h'), 95),\n (('k', 's'), 95),\n (('l', 's'), 94),\n (('u', 'k'), 93),\n (('&lt;S&gt;', 'q'), 92),\n (('d', 'u'), 92),\n (('s', 'm'), 90),\n (('r', 'k'), 90),\n (('i', 'x'), 89),\n (('v', '&lt;E&gt;'), 88),\n (('y', 'k'), 86),\n (('u', 'w'), 86),\n (('g', 'u'), 85),\n (('b', 'y'), 83),\n (('e', 'p'), 83),\n (('g', 'o'), 83),\n (('s', 'k'), 82),\n (('u', 't'), 82),\n (('a', 'p'), 82),\n (('e', 'f'), 82),\n (('i', 'i'), 82),\n (('r', 'v'), 80),\n (('f', '&lt;E&gt;'), 80),\n (('t', 'u'), 78),\n (('y', 'z'), 78),\n (('&lt;S&gt;', 'u'), 78),\n (('l', 't'), 77),\n (('r', 'g'), 76),\n (('c', 'r'), 76),\n (('i', 'j'), 76),\n (('w', 'y'), 73),\n (('z', 'u'), 73),\n (('l', 'v'), 72),\n (('h', 't'), 71),\n (('j', '&lt;E&gt;'), 71),\n (('x', 't'), 70),\n (('o', 'i'), 69),\n (('e', 'u'), 69),\n (('o', 'k'), 68),\n (('b', 'd'), 65),\n (('a', 'o'), 63),\n (('p', 'i'), 61),\n (('s', 'c'), 60),\n (('d', 'l'), 60),\n (('l', 'm'), 60),\n (('a', 'q'), 60),\n (('f', 'o'), 60),\n (('p', 'o'), 59),\n (('n', 'k'), 58),\n (('w', 'n'), 58),\n (('u', 'h'), 58),\n (('e', 'j'), 55),\n (('n', 'v'), 55),\n (('s', 'r'), 55),\n (('o', 'z'), 54),\n (('i', 'p'), 53),\n (('l', 'b'), 52),\n (('i', 'q'), 52),\n (('w', '&lt;E&gt;'), 51),\n (('m', 'c'), 51),\n (('s', 'p'), 51),\n (('e', 'w'), 50),\n (('k', 'u'), 50),\n (('v', 'r'), 48),\n (('u', 'g'), 47),\n (('o', 'x'), 45),\n (('u', 'z'), 45),\n (('z', 'z'), 45),\n (('j', 'h'), 45),\n (('b', 'u'), 45),\n (('o', 'g'), 44),\n (('n', 'r'), 44),\n (('f', 'f'), 44),\n (('n', 'j'), 44),\n (('z', 'h'), 43),\n (('c', 'c'), 42),\n (('r', 'b'), 41),\n (('x', 'o'), 41),\n (('b', 'h'), 41),\n (('p', 'p'), 39),\n (('x', 'l'), 39),\n (('h', 'v'), 39),\n (('b', 'b'), 38),\n (('m', 'p'), 38),\n (('x', 'x'), 38),\n (('u', 'v'), 37),\n (('x', 'e'), 36),\n (('w', 'o'), 36),\n (('c', 't'), 35),\n (('z', 'm'), 35),\n (('t', 's'), 35),\n (('m', 's'), 35),\n (('c', 'u'), 35),\n (('o', 'f'), 34),\n (('u', 'x'), 34),\n (('k', 'w'), 34),\n (('p', '&lt;E&gt;'), 33),\n (('g', 'l'), 32),\n (('z', 'r'), 32),\n (('d', 'n'), 31),\n (('g', 't'), 31),\n (('g', 'y'), 31),\n (('h', 's'), 31),\n (('x', 's'), 31),\n (('g', 's'), 30),\n (('x', 'y'), 30),\n (('y', 'g'), 30),\n (('d', 'm'), 30),\n (('d', 's'), 29),\n (('h', 'k'), 29),\n (('y', 'x'), 28),\n (('q', '&lt;E&gt;'), 28),\n (('g', 'n'), 27),\n (('y', 'b'), 27),\n (('g', 'w'), 26),\n (('n', 'h'), 26),\n (('k', 'n'), 26),\n (('g', 'g'), 25),\n (('d', 'g'), 25),\n (('l', 'c'), 25),\n (('r', 'j'), 25),\n (('w', 'u'), 25),\n (('l', 'k'), 24),\n (('m', 'd'), 24),\n (('s', 'w'), 24),\n (('s', 'n'), 24),\n (('h', 'd'), 24),\n (('w', 'h'), 23),\n (('y', 'j'), 23),\n (('y', 'y'), 23),\n (('r', 'z'), 23),\n (('d', 'w'), 23),\n (('w', 'r'), 22),\n (('t', 'n'), 22),\n (('l', 'f'), 22),\n (('y', 'h'), 22),\n (('r', 'w'), 21),\n (('s', 'b'), 21),\n (('m', 'n'), 20),\n (('f', 'l'), 20),\n (('w', 's'), 20),\n (('k', 'k'), 20),\n (('h', 'z'), 20),\n (('g', 'd'), 19),\n (('l', 'h'), 19),\n (('n', 'm'), 19),\n (('x', 'z'), 19),\n (('u', 'f'), 19),\n (('f', 't'), 18),\n (('l', 'r'), 18),\n (('p', 't'), 17),\n (('t', 'c'), 17),\n (('k', 't'), 17),\n (('d', 'v'), 17),\n (('u', 'p'), 16),\n (('p', 'l'), 16),\n (('l', 'w'), 16),\n (('p', 's'), 16),\n (('o', 'j'), 16),\n (('r', 'q'), 16),\n (('y', 'p'), 15),\n (('l', 'p'), 15),\n (('t', 'v'), 15),\n (('r', 'p'), 14),\n (('l', 'n'), 14),\n (('e', 'q'), 14),\n (('f', 'y'), 14),\n (('s', 'v'), 14),\n (('u', 'j'), 14),\n (('v', 'l'), 14),\n (('q', 'a'), 13),\n (('u', 'y'), 13),\n (('q', 'i'), 13),\n (('w', 'l'), 13),\n (('p', 'y'), 12),\n (('y', 'f'), 12),\n (('c', 'q'), 11),\n (('j', 'r'), 11),\n (('n', 'w'), 11),\n (('n', 'f'), 11),\n (('t', 'w'), 11),\n (('m', 'z'), 11),\n (('u', 'o'), 10),\n (('f', 'u'), 10),\n (('l', 'z'), 10),\n (('h', 'w'), 10),\n (('u', 'q'), 10),\n (('j', 'y'), 10),\n (('s', 'z'), 10),\n (('s', 'd'), 9),\n (('j', 'l'), 9),\n (('d', 'j'), 9),\n (('k', 'm'), 9),\n (('r', 'f'), 9),\n (('h', 'j'), 9),\n (('v', 'n'), 8),\n (('n', 'b'), 8),\n (('i', 'w'), 8),\n (('h', 'b'), 8),\n (('b', 's'), 8),\n (('w', 't'), 8),\n (('w', 'd'), 8),\n (('v', 'v'), 7),\n (('v', 'u'), 7),\n (('j', 's'), 7),\n (('m', 'j'), 7),\n (('f', 's'), 6),\n (('l', 'g'), 6),\n (('l', 'j'), 6),\n (('j', 'w'), 6),\n (('n', 'x'), 6),\n (('y', 'q'), 6),\n (('w', 'k'), 6),\n (('g', 'm'), 6),\n (('x', 'u'), 5),\n (('m', 'h'), 5),\n (('m', 'l'), 5),\n (('j', 'm'), 5),\n (('c', 's'), 5),\n (('j', 'v'), 5),\n (('n', 'p'), 5),\n (('d', 'f'), 5),\n (('x', 'd'), 5),\n (('z', 'b'), 4),\n (('f', 'n'), 4),\n (('x', 'c'), 4),\n (('m', 't'), 4),\n (('t', 'm'), 4),\n (('z', 'n'), 4),\n (('z', 't'), 4),\n (('p', 'u'), 4),\n (('c', 'z'), 4),\n (('b', 'n'), 4),\n (('z', 's'), 4),\n (('f', 'w'), 4),\n (('d', 't'), 4),\n (('j', 'd'), 4),\n (('j', 'c'), 4),\n (('y', 'w'), 4),\n (('v', 'k'), 3),\n (('x', 'w'), 3),\n (('t', 'j'), 3),\n (('c', 'j'), 3),\n (('q', 'w'), 3),\n (('g', 'b'), 3),\n (('o', 'q'), 3),\n (('r', 'x'), 3),\n (('d', 'c'), 3),\n (('g', 'j'), 3),\n (('x', 'f'), 3),\n (('z', 'w'), 3),\n (('d', 'k'), 3),\n (('u', 'u'), 3),\n (('m', 'v'), 3),\n (('c', 'x'), 3),\n (('l', 'q'), 3),\n (('p', 'b'), 2),\n (('t', 'g'), 2),\n (('q', 's'), 2),\n (('t', 'x'), 2),\n (('f', 'k'), 2),\n (('b', 't'), 2),\n (('j', 'n'), 2),\n (('k', 'c'), 2),\n (('z', 'k'), 2),\n (('s', 'j'), 2),\n (('s', 'f'), 2),\n (('z', 'j'), 2),\n (('n', 'q'), 2),\n (('f', 'z'), 2),\n (('h', 'g'), 2),\n (('w', 'w'), 2),\n (('k', 'j'), 2),\n (('j', 'k'), 2),\n (('w', 'm'), 2),\n (('z', 'c'), 2),\n (('z', 'v'), 2),\n (('w', 'f'), 2),\n (('q', 'm'), 2),\n (('k', 'z'), 2),\n (('j', 'j'), 2),\n (('z', 'p'), 2),\n (('j', 't'), 2),\n (('k', 'b'), 2),\n (('m', 'w'), 2),\n (('h', 'f'), 2),\n (('c', 'g'), 2),\n (('t', 'f'), 2),\n (('h', 'c'), 2),\n (('q', 'o'), 2),\n (('k', 'd'), 2),\n (('k', 'v'), 2),\n (('s', 'g'), 2),\n (('z', 'd'), 2),\n (('q', 'r'), 1),\n (('d', 'z'), 1),\n (('p', 'j'), 1),\n (('q', 'l'), 1),\n (('p', 'f'), 1),\n (('q', 'e'), 1),\n (('b', 'c'), 1),\n (('c', 'd'), 1),\n (('m', 'f'), 1),\n (('p', 'n'), 1),\n (('w', 'b'), 1),\n (('p', 'c'), 1),\n (('h', 'p'), 1),\n (('f', 'h'), 1),\n (('b', 'j'), 1),\n (('f', 'g'), 1),\n (('z', 'g'), 1),\n (('c', 'p'), 1),\n (('p', 'k'), 1),\n (('p', 'm'), 1),\n (('x', 'n'), 1),\n (('s', 'q'), 1),\n (('k', 'f'), 1),\n (('m', 'k'), 1),\n (('x', 'h'), 1),\n (('g', 'f'), 1),\n (('v', 'b'), 1),\n (('j', 'p'), 1),\n (('g', 'z'), 1),\n (('v', 'd'), 1),\n (('d', 'b'), 1),\n (('v', 'h'), 1),\n (('h', 'h'), 1),\n (('g', 'v'), 1),\n (('d', 'q'), 1),\n (('x', 'b'), 1),\n (('w', 'z'), 1),\n (('h', 'q'), 1),\n (('j', 'b'), 1),\n (('x', 'm'), 1),\n (('w', 'g'), 1),\n (('t', 'b'), 1),\n (('z', 'x'), 1)]\n\n\nWant to store this info in 2d array:\n\nrows: first character of bigram\ncolumns: second character\neach entry = how often that first character follows second character in this dataset\n\ncan use Tensors w/ PyTorch\n\n#create a 3x5 tensor of zeros\na = torch.zeros((3, 5))\na\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\n\na.dtype\n\ntorch.float32\n\n\nBy default, a.dtype (data type) is .float32. This can be changed:\n\n#32-bit integers\na = torch.zeros((3, 5), dtype=torch.int32)\na\n\ntensor([[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]], dtype=torch.int32)\n\n\nTensors allow us to easily manipulate any of the individual entries.\n\na[1, 3] = 1\na\n\ntensor([[0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0]], dtype=torch.int32)\n\n\n\na[1, 3] += 1\na\n\ntensor([[0, 0, 0, 0, 0],\n        [0, 0, 0, 2, 0],\n        [0, 0, 0, 0, 0]], dtype=torch.int32)\n\n\n\na[0, 0] = 5\na\n\ntensor([[5, 0, 0, 0, 0],\n        [0, 0, 0, 2, 0],\n        [0, 0, 0, 0, 0]], dtype=torch.int32)\n\n\n26 letters in the alphabet+2 special characters (start/end) = 28\nNeed to be able to index integers for the characters – construct a character array:\n\nN = torch.zeros((28, 28), dtype=torch.int32)\n\n\nchars = sorted(list(set(''.join(words))))\n\n#s (string) to I (integer) mapping\nchar_to_id = {char:id for id, char in enumerate(chars)}\n\n#manually map special chars\nchar_to_id['&lt;S&gt;'] = 26\nchar_to_id['&lt;E&gt;'] = 27\n\n\nfor word in words:\n    chs = ['&lt;S&gt;'] + list(word) + ['&lt;E&gt;']\n    for ch1, ch2 in zip(chs, chs[1:]):\n        id1 = char_to_id[ch1]\n        id2 = char_to_id[ch2]\n        N[id1, id2] += 1\n\n\nplt.imshow(N)\n\n&lt;matplotlib.image.AxesImage at 0x12f783010&gt;\n\n\n\n\n\nAbove: a bit ugly\nto make a nicer visualization, first reversing our mappings so we can reverse our array and label w/ characters we want\n\nid_to_char = {id:char for char, id in char_to_id.items()}\n\n\nplt.figure(figsize=(16,16))\nplt.imshow(N, cmap='Blues')\nfor i in range(28):\n    for j in range(28):\n        chstr = id_to_char[i] + id_to_char[j] #i to j (char 1 to char 2)\n        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n        #.item() since we are indexing tensors and want individual integer\n        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\nplt.axis('off')\n\n(-0.5, 27.5, 27.5, -0.5)\n\n\n\n\n\nThe issue here – we have an extra column and extra row since the start character will never be the second character of a bigram; end character will never be the first character of a bigram. The only exception if there is a word with no letters and the start char follows end char.\nTo remedy this:\n\nN = torch.zeros((27, 27), dtype=torch.int32)\n\n\nchars = sorted(list(set(''.join(words))))\nchar_to_id = {char:id+1 for id, char in enumerate(chars)}\nchar_to_id['.'] = 0 #offset all other letters\nid_to_char = {id:char for char, id in char_to_id.items()}\n\n\nfor word in words:\n  chs = ['.'] + list(word) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    id1 = char_to_id[ch1]\n    id2 = char_to_id[ch2]\n    N[id1, id2] += 1\n\n\nplt.figure(figsize=(16,16))\nplt.imshow(N, cmap='Blues')\nfor i in range(27):\n    for j in range(27):\n        chstr = id_to_char[i] + id_to_char[j] #i to j (char 1 to char 2)\n        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n        #.item() since we are indexing tensors and want individual integer\n        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\nplt.axis('off')\n\n(-0.5, 26.5, 26.5, -0.5)\n\n\n\n\n\n\nalong first row: characters that start a word\nalong first column: characters that end word\nin between: what characters follow each other\n\nThis counts array has all info necessary for us to sample from for our bigram model. Start following these probabilities and sample from these counts.\nEx:\nuse first row to see how likely the different characters are to start a word:\n\nN[0]\n\ntensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n         134,  535,  929], dtype=torch.int32)\n\n\nTake frequency vector and convert to float so that we can normalize these counts (and turn into probability); frequency \\(\\rightarrow\\) probability\n\np = N[0].float()\np = p/p.sum()\np #probability for any single character to be the first in a word\n\ntensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])\n\n\nNow we get a probability distribution vector p.\nWant to sample from this distribution – use torch.multinomial to return samples from a multinomial probability distribution\n\nuse torch.Generator to make this deterministic/seed it for reproducible reasons – using this generator object can always give you the same random numbers\n\n\n#ex\ng = Generator().manual_seed(2147483647)\np = torch.rand(3, generator=g)\np = p/p.sum()\np #can draw reproducible samples\n\ntensor([0.6064, 0.3033, 0.0903])\n\n\ntorch.multinomial will take a tensor of probability distributions\n\nwe can ask it for a number of samples (e.g. 100 samples)\ncan specify replacement=True if we want to sample with replacement (default is w/o replacement)\n\nlooking at our tensor prob distribution, expect to see a lot of 0’s since its probability is 0.6064, half as many 1’s, very few 2’s\n\ntorch.multinomial(p, num_samples=100, replacement=True, generator=g)\n\ntensor([1, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0,\n        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n        0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0,\n        0, 1, 1, 1])\n\n\nNow taking a sample of 1 from the original frequency vector\n\ng = Generator().manual_seed(2147483647)\n#tensor that wraps 10 -- need `.item()` to get integer\nix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n\n#check which character we're sampling w/ dict\nid_to_char[ix]\n\n'.'\n\n\ninterpretation: first character is “m” in our generation\nTo draw next character: look at row vector of counts where we start with “m” as first character\n\ng = Generator().manual_seed(2147483647)\n\n#begin at index 0 (start token)\nix = 0\nwhile True:\n    #get row for index we are currently on \n    p = N[ix].float()\n    #normalize -&gt; prob\n    p = p/p.sum()\n    #draw a sample of 1\n    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n    print(id_to_char[ix])\n    #end token\n    if ix==0:\n        break\n\nj\nu\nn\ni\nd\ne\n.\n\n\nJoin word together and iterate:\n\ng = Generator().manual_seed(2147483647)\n\nfor i in range(20):\n    out = []\n    ix = 0\n    while True:\n        p = N[ix].float()\n        p = p/p.sum()\n        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n        out.append(id_to_char[ix])\n        if ix==0:\n            break\n    print(''.join(out))\n\njunide.\njanasah.\np.\ncony.\na.\nnn.\nkohin.\ntolian.\njuee.\nksahnaauranilevias.\ndedainrwieta.\nssonielylarte.\nfaveumerifontume.\nphynslenaruani.\ncore.\nyaenon.\nka.\njabdinerimikimaynin.\nanaasn.\nssorionsush.\n\n\nWe can see that this model is not great.\nInstead of p having structure, what if it was a uniform distribution? – so this model is slightly better, just not great\nWhat we’re doing every iteration:\n\nfetch row form counts matrix\nconverting to float and renormalizing\n\nThis is very efficient.\nSolution: prepare a matrix that normalizes upfront\n*Be careful with P.sum() – we want to simultaneously+in parallel divide all the rows by their sums\ntorch.sum() takes in a parameter keepdim=False by default.\n\nIf True, the output tensor is of same size as input except where dimensions are size 1.\nIf false, the dimension is squeezed out -&gt; output tenser has 1 or len(dim) fewer dimension(s)\n\nso torch.sum() collapses dimension to be size 1 and squeeze out that dimension.\nWant to specify dimension to take sum across:\n\n#similar to our previous for loop contents\n#grab floating point copy of N\nP = N.float()\nP\n\ntensor([[0.0000e+00, 4.4100e+03, 1.3060e+03, 1.5420e+03, 1.6900e+03, 1.5310e+03,\n         4.1700e+02, 6.6900e+02, 8.7400e+02, 5.9100e+02, 2.4220e+03, 2.9630e+03,\n         1.5720e+03, 2.5380e+03, 1.1460e+03, 3.9400e+02, 5.1500e+02, 9.2000e+01,\n         1.6390e+03, 2.0550e+03, 1.3080e+03, 7.8000e+01, 3.7600e+02, 3.0700e+02,\n         1.3400e+02, 5.3500e+02, 9.2900e+02],\n        [6.6400e+03, 5.5600e+02, 5.4100e+02, 4.7000e+02, 1.0420e+03, 6.9200e+02,\n         1.3400e+02, 1.6800e+02, 2.3320e+03, 1.6500e+03, 1.7500e+02, 5.6800e+02,\n         2.5280e+03, 1.6340e+03, 5.4380e+03, 6.3000e+01, 8.2000e+01, 6.0000e+01,\n         3.2640e+03, 1.1180e+03, 6.8700e+02, 3.8100e+02, 8.3400e+02, 1.6100e+02,\n         1.8200e+02, 2.0500e+03, 4.3500e+02],\n        [1.1400e+02, 3.2100e+02, 3.8000e+01, 1.0000e+00, 6.5000e+01, 6.5500e+02,\n         0.0000e+00, 0.0000e+00, 4.1000e+01, 2.1700e+02, 1.0000e+00, 0.0000e+00,\n         1.0300e+02, 0.0000e+00, 4.0000e+00, 1.0500e+02, 0.0000e+00, 0.0000e+00,\n         8.4200e+02, 8.0000e+00, 2.0000e+00, 4.5000e+01, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 8.3000e+01, 0.0000e+00],\n        [9.7000e+01, 8.1500e+02, 0.0000e+00, 4.2000e+01, 1.0000e+00, 5.5100e+02,\n         0.0000e+00, 2.0000e+00, 6.6400e+02, 2.7100e+02, 3.0000e+00, 3.1600e+02,\n         1.1600e+02, 0.0000e+00, 0.0000e+00, 3.8000e+02, 1.0000e+00, 1.1000e+01,\n         7.6000e+01, 5.0000e+00, 3.5000e+01, 3.5000e+01, 0.0000e+00, 0.0000e+00,\n         3.0000e+00, 1.0400e+02, 4.0000e+00],\n        [5.1600e+02, 1.3030e+03, 1.0000e+00, 3.0000e+00, 1.4900e+02, 1.2830e+03,\n         5.0000e+00, 2.5000e+01, 1.1800e+02, 6.7400e+02, 9.0000e+00, 3.0000e+00,\n         6.0000e+01, 3.0000e+01, 3.1000e+01, 3.7800e+02, 0.0000e+00, 1.0000e+00,\n         4.2400e+02, 2.9000e+01, 4.0000e+00, 9.2000e+01, 1.7000e+01, 2.3000e+01,\n         0.0000e+00, 3.1700e+02, 1.0000e+00],\n        [3.9830e+03, 6.7900e+02, 1.2100e+02, 1.5300e+02, 3.8400e+02, 1.2710e+03,\n         8.2000e+01, 1.2500e+02, 1.5200e+02, 8.1800e+02, 5.5000e+01, 1.7800e+02,\n         3.2480e+03, 7.6900e+02, 2.6750e+03, 2.6900e+02, 8.3000e+01, 1.4000e+01,\n         1.9580e+03, 8.6100e+02, 5.8000e+02, 6.9000e+01, 4.6300e+02, 5.0000e+01,\n         1.3200e+02, 1.0700e+03, 1.8100e+02],\n        [8.0000e+01, 2.4200e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2300e+02,\n         4.4000e+01, 1.0000e+00, 1.0000e+00, 1.6000e+02, 0.0000e+00, 2.0000e+00,\n         2.0000e+01, 0.0000e+00, 4.0000e+00, 6.0000e+01, 0.0000e+00, 0.0000e+00,\n         1.1400e+02, 6.0000e+00, 1.8000e+01, 1.0000e+01, 0.0000e+00, 4.0000e+00,\n         0.0000e+00, 1.4000e+01, 2.0000e+00],\n        [1.0800e+02, 3.3000e+02, 3.0000e+00, 0.0000e+00, 1.9000e+01, 3.3400e+02,\n         1.0000e+00, 2.5000e+01, 3.6000e+02, 1.9000e+02, 3.0000e+00, 0.0000e+00,\n         3.2000e+01, 6.0000e+00, 2.7000e+01, 8.3000e+01, 0.0000e+00, 0.0000e+00,\n         2.0100e+02, 3.0000e+01, 3.1000e+01, 8.5000e+01, 1.0000e+00, 2.6000e+01,\n         0.0000e+00, 3.1000e+01, 1.0000e+00],\n        [2.4090e+03, 2.2440e+03, 8.0000e+00, 2.0000e+00, 2.4000e+01, 6.7400e+02,\n         2.0000e+00, 2.0000e+00, 1.0000e+00, 7.2900e+02, 9.0000e+00, 2.9000e+01,\n         1.8500e+02, 1.1700e+02, 1.3800e+02, 2.8700e+02, 1.0000e+00, 1.0000e+00,\n         2.0400e+02, 3.1000e+01, 7.1000e+01, 1.6600e+02, 3.9000e+01, 1.0000e+01,\n         0.0000e+00, 2.1300e+02, 2.0000e+01],\n        [2.4890e+03, 2.4450e+03, 1.1000e+02, 5.0900e+02, 4.4000e+02, 1.6530e+03,\n         1.0100e+02, 4.2800e+02, 9.5000e+01, 8.2000e+01, 7.6000e+01, 4.4500e+02,\n         1.3450e+03, 4.2700e+02, 2.1260e+03, 5.8800e+02, 5.3000e+01, 5.2000e+01,\n         8.4900e+02, 1.3160e+03, 5.4100e+02, 1.0900e+02, 2.6900e+02, 8.0000e+00,\n         8.9000e+01, 7.7900e+02, 2.7700e+02],\n        [7.1000e+01, 1.4730e+03, 1.0000e+00, 4.0000e+00, 4.0000e+00, 4.4000e+02,\n         0.0000e+00, 0.0000e+00, 4.5000e+01, 1.1900e+02, 2.0000e+00, 2.0000e+00,\n         9.0000e+00, 5.0000e+00, 2.0000e+00, 4.7900e+02, 1.0000e+00, 0.0000e+00,\n         1.1000e+01, 7.0000e+00, 2.0000e+00, 2.0200e+02, 5.0000e+00, 6.0000e+00,\n         0.0000e+00, 1.0000e+01, 0.0000e+00],\n        [3.6300e+02, 1.7310e+03, 2.0000e+00, 2.0000e+00, 2.0000e+00, 8.9500e+02,\n         1.0000e+00, 0.0000e+00, 3.0700e+02, 5.0900e+02, 2.0000e+00, 2.0000e+01,\n         1.3900e+02, 9.0000e+00, 2.6000e+01, 3.4400e+02, 0.0000e+00, 0.0000e+00,\n         1.0900e+02, 9.5000e+01, 1.7000e+01, 5.0000e+01, 2.0000e+00, 3.4000e+01,\n         0.0000e+00, 3.7900e+02, 2.0000e+00],\n        [1.3140e+03, 2.6230e+03, 5.2000e+01, 2.5000e+01, 1.3800e+02, 2.9210e+03,\n         2.2000e+01, 6.0000e+00, 1.9000e+01, 2.4800e+03, 6.0000e+00, 2.4000e+01,\n         1.3450e+03, 6.0000e+01, 1.4000e+01, 6.9200e+02, 1.5000e+01, 3.0000e+00,\n         1.8000e+01, 9.4000e+01, 7.7000e+01, 3.2400e+02, 7.2000e+01, 1.6000e+01,\n         0.0000e+00, 1.5880e+03, 1.0000e+01],\n        [5.1600e+02, 2.5900e+03, 1.1200e+02, 5.1000e+01, 2.4000e+01, 8.1800e+02,\n         1.0000e+00, 0.0000e+00, 5.0000e+00, 1.2560e+03, 7.0000e+00, 1.0000e+00,\n         5.0000e+00, 1.6800e+02, 2.0000e+01, 4.5200e+02, 3.8000e+01, 0.0000e+00,\n         9.7000e+01, 3.5000e+01, 4.0000e+00, 1.3900e+02, 3.0000e+00, 2.0000e+00,\n         0.0000e+00, 2.8700e+02, 1.1000e+01],\n        [6.7630e+03, 2.9770e+03, 8.0000e+00, 2.1300e+02, 7.0400e+02, 1.3590e+03,\n         1.1000e+01, 2.7300e+02, 2.6000e+01, 1.7250e+03, 4.4000e+01, 5.8000e+01,\n         1.9500e+02, 1.9000e+01, 1.9060e+03, 4.9600e+02, 5.0000e+00, 2.0000e+00,\n         4.4000e+01, 2.7800e+02, 4.4300e+02, 9.6000e+01, 5.5000e+01, 1.1000e+01,\n         6.0000e+00, 4.6500e+02, 1.4500e+02],\n        [8.5500e+02, 1.4900e+02, 1.4000e+02, 1.1400e+02, 1.9000e+02, 1.3200e+02,\n         3.4000e+01, 4.4000e+01, 1.7100e+02, 6.9000e+01, 1.6000e+01, 6.8000e+01,\n         6.1900e+02, 2.6100e+02, 2.4110e+03, 1.1500e+02, 9.5000e+01, 3.0000e+00,\n         1.0590e+03, 5.0400e+02, 1.1800e+02, 2.7500e+02, 1.7600e+02, 1.1400e+02,\n         4.5000e+01, 1.0300e+02, 5.4000e+01],\n        [3.3000e+01, 2.0900e+02, 2.0000e+00, 1.0000e+00, 0.0000e+00, 1.9700e+02,\n         1.0000e+00, 0.0000e+00, 2.0400e+02, 6.1000e+01, 1.0000e+00, 1.0000e+00,\n         1.6000e+01, 1.0000e+00, 1.0000e+00, 5.9000e+01, 3.9000e+01, 0.0000e+00,\n         1.5100e+02, 1.6000e+01, 1.7000e+01, 4.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 1.2000e+01, 0.0000e+00],\n        [2.8000e+01, 1.3000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3000e+01, 0.0000e+00, 0.0000e+00,\n         1.0000e+00, 2.0000e+00, 0.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00,\n         1.0000e+00, 2.0000e+00, 0.0000e+00, 2.0600e+02, 0.0000e+00, 3.0000e+00,\n         0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [1.3770e+03, 2.3560e+03, 4.1000e+01, 9.9000e+01, 1.8700e+02, 1.6970e+03,\n         9.0000e+00, 7.6000e+01, 1.2100e+02, 3.0330e+03, 2.5000e+01, 9.0000e+01,\n         4.1300e+02, 1.6200e+02, 1.4000e+02, 8.6900e+02, 1.4000e+01, 1.6000e+01,\n         4.2500e+02, 1.9000e+02, 2.0800e+02, 2.5200e+02, 8.0000e+01, 2.1000e+01,\n         3.0000e+00, 7.7300e+02, 2.3000e+01],\n        [1.1690e+03, 1.2010e+03, 2.1000e+01, 6.0000e+01, 9.0000e+00, 8.8400e+02,\n         2.0000e+00, 2.0000e+00, 1.2850e+03, 6.8400e+02, 2.0000e+00, 8.2000e+01,\n         2.7900e+02, 9.0000e+01, 2.4000e+01, 5.3100e+02, 5.1000e+01, 1.0000e+00,\n         5.5000e+01, 4.6100e+02, 7.6500e+02, 1.8500e+02, 1.4000e+01, 2.4000e+01,\n         0.0000e+00, 2.1500e+02, 1.0000e+01],\n        [4.8300e+02, 1.0270e+03, 1.0000e+00, 1.7000e+01, 0.0000e+00, 7.1600e+02,\n         2.0000e+00, 2.0000e+00, 6.4700e+02, 5.3200e+02, 3.0000e+00, 0.0000e+00,\n         1.3400e+02, 4.0000e+00, 2.2000e+01, 6.6700e+02, 0.0000e+00, 0.0000e+00,\n         3.5200e+02, 3.5000e+01, 3.7400e+02, 7.8000e+01, 1.5000e+01, 1.1000e+01,\n         2.0000e+00, 3.4100e+02, 1.0500e+02],\n        [1.5500e+02, 1.6300e+02, 1.0300e+02, 1.0300e+02, 1.3600e+02, 1.6900e+02,\n         1.9000e+01, 4.7000e+01, 5.8000e+01, 1.2100e+02, 1.4000e+01, 9.3000e+01,\n         3.0100e+02, 1.5400e+02, 2.7500e+02, 1.0000e+01, 1.6000e+01, 1.0000e+01,\n         4.1400e+02, 4.7400e+02, 8.2000e+01, 3.0000e+00, 3.7000e+01, 8.6000e+01,\n         3.4000e+01, 1.3000e+01, 4.5000e+01],\n        [8.8000e+01, 6.4200e+02, 1.0000e+00, 0.0000e+00, 1.0000e+00, 5.6800e+02,\n         0.0000e+00, 0.0000e+00, 1.0000e+00, 9.1100e+02, 0.0000e+00, 3.0000e+00,\n         1.4000e+01, 0.0000e+00, 8.0000e+00, 1.5300e+02, 0.0000e+00, 0.0000e+00,\n         4.8000e+01, 0.0000e+00, 0.0000e+00, 7.0000e+00, 7.0000e+00, 0.0000e+00,\n         0.0000e+00, 1.2100e+02, 0.0000e+00],\n        [5.1000e+01, 2.8000e+02, 1.0000e+00, 0.0000e+00, 8.0000e+00, 1.4900e+02,\n         2.0000e+00, 1.0000e+00, 2.3000e+01, 1.4800e+02, 0.0000e+00, 6.0000e+00,\n         1.3000e+01, 2.0000e+00, 5.8000e+01, 3.6000e+01, 0.0000e+00, 0.0000e+00,\n         2.2000e+01, 2.0000e+01, 8.0000e+00, 2.5000e+01, 0.0000e+00, 2.0000e+00,\n         0.0000e+00, 7.3000e+01, 1.0000e+00],\n        [1.6400e+02, 1.0300e+02, 1.0000e+00, 4.0000e+00, 5.0000e+00, 3.6000e+01,\n         3.0000e+00, 0.0000e+00, 1.0000e+00, 1.0200e+02, 0.0000e+00, 0.0000e+00,\n         3.9000e+01, 1.0000e+00, 1.0000e+00, 4.1000e+01, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 3.1000e+01, 7.0000e+01, 5.0000e+00, 0.0000e+00, 3.0000e+00,\n         3.8000e+01, 3.0000e+01, 1.9000e+01],\n        [2.0070e+03, 2.1430e+03, 2.7000e+01, 1.1500e+02, 2.7200e+02, 3.0100e+02,\n         1.2000e+01, 3.0000e+01, 2.2000e+01, 1.9200e+02, 2.3000e+01, 8.6000e+01,\n         1.1040e+03, 1.4800e+02, 1.8260e+03, 2.7100e+02, 1.5000e+01, 6.0000e+00,\n         2.9100e+02, 4.0100e+02, 1.0400e+02, 1.4100e+02, 1.0600e+02, 4.0000e+00,\n         2.8000e+01, 2.3000e+01, 7.8000e+01],\n        [1.6000e+02, 8.6000e+02, 4.0000e+00, 2.0000e+00, 2.0000e+00, 3.7300e+02,\n         0.0000e+00, 1.0000e+00, 4.3000e+01, 3.6400e+02, 2.0000e+00, 2.0000e+00,\n         1.2300e+02, 3.5000e+01, 4.0000e+00, 1.1000e+02, 2.0000e+00, 0.0000e+00,\n         3.2000e+01, 4.0000e+00, 4.0000e+00, 7.3000e+01, 2.0000e+00, 3.0000e+00,\n         1.0000e+00, 1.4700e+02, 4.5000e+01]])\n\n\n\np.shape\n\ntorch.Size([27])\n\n\n\nP.shape\n\ntorch.Size([27, 27])\n\n\n\nP.sum(0) \n\ntensor([32033., 33885.,  2645.,  3532.,  5496., 20423.,   905.,  1927.,  7616.,\n        17701.,  2900.,  5040., 13958.,  6642., 18327.,  7934.,  1026.,   272.,\n        12700.,  8106.,  5570.,  3135.,  2573.,   929.,   697.,  9776.,  2398.])\n\n\n\nP.sum(0, keepdim=True)\n\ntensor([[32033., 33885.,  2645.,  3532.,  5496., 20423.,   905.,  1927.,  7616.,\n         17701.,  2900.,  5040., 13958.,  6642., 18327.,  7934.,  1026.,   272.,\n         12700.,  8106.,  5570.,  3135.,  2573.,   929.,   697.,  9776.,  2398.]])\n\n\n\n#ex:\nP.sum(0).shape #sums along the 1st entry for P.shape; does not keep dimensions -&gt; squeezes out that dimension and we get a 1d vector of size 27\n\ntorch.Size([27])\n\n\n\nP.sum(0, keepdim=True).shape #still has 2 dimensions\n\ntorch.Size([1, 27])\n\n\nWe don’t want this 1x27 row vector b/c this will give us sums down the columns – set dimension to 1 instead to sum across rows.\n\nP.sum(1, keepdim=True) #27x1 vector of counts\n\ntensor([[32033.],\n        [33885.],\n        [ 2645.],\n        [ 3532.],\n        [ 5496.],\n        [20423.],\n        [  905.],\n        [ 1927.],\n        [ 7616.],\n        [17701.],\n        [ 2900.],\n        [ 5040.],\n        [13958.],\n        [ 6642.],\n        [18327.],\n        [ 7934.],\n        [ 1026.],\n        [  272.],\n        [12700.],\n        [ 8106.],\n        [ 5570.],\n        [ 3135.],\n        [ 2573.],\n        [  929.],\n        [  697.],\n        [ 9776.],\n        [ 2398.]])\n\n\n\nP.sum(1, keepdim=True).shape\n\ntorch.Size([27, 1])\n\n\nBe careful – is it possible to take a 27x27 array and divide it by 27x1 array?\nDepends on broadcasting rules, which determine if 2 tensors can be combined in a binary operation.\nConditions:\n\neach tensor has at least 1 dimension\nwhen iterating over dimension sizes, starting at trailing dimension, dimension sizes must be =, or one of them is 1, or one of them dne\n\ntakes this 27x1 column vector and copies it 27 times to make both be 27x27 before element-wise division\n\nnormalizes it\n\nEnd up w/ same counts as when keepdim=True but the shape is not the same – when one of the dimensions doesn’t exist, internally it is 1, which gets copied. We end up normalizing the columns rather than rows.\nBroadcasting starts from right to left (aligns right) and if dimension dne, create a 1\n\nwhen keepdim=False the column vector of sums turns into a row vector\nrow vector gets replicated vertically\n\n\nP = N.float()\nP = P / P.sum(1, keepdim=True) \n#for efficiency, don't want to do this -- use in place operations\n\n\nP = N.float()\nP /= P.sum(1, keepdim=True) \n\n\ng = Generator().manual_seed(2147483647)\n\nfor i in range(5):\n    out = []\n    ix = 0\n    while True:\n        p = P[ix].float()\n        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n        out.append(id_to_char[ix])\n        if ix==0:\n            break\n    print(''.join(out))\n\njunide.\njanasah.\np.\ncony.\na.\n\n\nElements of our array P are parameters of our language model – summarize statistics\n\nsample iteratively the next character\n\nWant to evaluate quality of this model\n\ntraining loss tells us this in a single number\n\n\nfor word in words[:3]:\n  chs = ['.'] + list(word) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    id1 = char_to_id[ch1]\n    id2 = char_to_id[ch2]\n    #prob that model assigns to each bigram\n    prob = P[id1, id2]\n    print(f'{ch1}{ch2}: {prob:.4f}')\n\n.e: 0.0478\nem: 0.0377\nmm: 0.0253\nma: 0.3899\na.: 0.1960\n.o: 0.0123\nol: 0.0780\nli: 0.1777\niv: 0.0152\nvi: 0.3541\nia: 0.1381\na.: 0.1960\n.a: 0.1377\nav: 0.0246\nva: 0.2495\na.: 0.1960\n\n\nif everything was equally likely, would expect everything to be ~4% but that is not the case here\n\nw/ a very good model, would expect these probabilities to be near 1 (correctly predicting what comes next)\n\nhow to summarize these probabilities into a single number measuring the quality of the model?\nLikelihood: product of all probabilities; probability of the entire dataset assigned by our trained model (higher=better)\n\nif you take product of all probabilities, will get a very small number\nsolution: log likelihood\ntake log of probability\nlog: monotonic transformation\n\n\nfor word in words[:3]:\n  chs = ['.'] + list(word) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    id1 = char_to_id[ch1]\n    id2 = char_to_id[ch2]\n    #prob that model assigns to each bigram\n    prob = P[id1, id2]\n    logprob = torch.log(prob)\n    print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n\n.e: 0.0478 -3.0408\nem: 0.0377 -3.2793\nmm: 0.0253 -3.6772\nma: 0.3899 -0.9418\na.: 0.1960 -1.6299\n.o: 0.0123 -4.3982\nol: 0.0780 -2.5508\nli: 0.1777 -1.7278\niv: 0.0152 -4.1867\nvi: 0.3541 -1.0383\nia: 0.1381 -1.9796\na.: 0.1960 -1.6299\n.a: 0.1377 -1.9829\nav: 0.0246 -3.7045\nva: 0.2495 -1.3882\na.: 0.1960 -1.6299\n\n\n\nhigher probabilities –&gt; get closer and closer to 0\nlower probabilites –&gt; more and more negative number\n\nlog likelihood = sum of logs of each probability\n\\(\\log(a*b*c) = \\log(a) + \\log(b) + \\log(c)\\)\nimplementation: start log likelihood at 0 and accumulate\n\nlog_likelihood = 0.0\nfor word in words[:3]:\n  chs = ['.'] + list(word) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    id1 = char_to_id[ch1]\n    id2 = char_to_id[ch2]\n    #prob that model assigns to each bigram\n    prob = P[id1, id2]\n    logprob = torch.log(prob)\n    log_likelihood += logprob\n    print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n\nprint(f'{log_likelihood}')\n\n.e: 0.0478 -3.0408\nem: 0.0377 -3.2793\nmm: 0.0253 -3.6772\nma: 0.3899 -0.9418\na.: 0.1960 -1.6299\n.o: 0.0123 -4.3982\nol: 0.0780 -2.5508\nli: 0.1777 -1.7278\niv: 0.0152 -4.1867\nvi: 0.3541 -1.0383\nia: 0.1381 -1.9796\na.: 0.1960 -1.6299\n.a: 0.1377 -1.9829\nav: 0.0246 -3.7045\nva: 0.2495 -1.3882\na.: 0.1960 -1.6299\n-38.78563690185547\n\n\nhighest log likelihood = 0\nwe want a loss fn (to minimize) –&gt; invert this –&gt; negative log likelihood\n\nlog_likelihood = 0.0\nfor word in words[:3]:\n  chs = ['.'] + list(word) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    id1 = char_to_id[ch1]\n    id2 = char_to_id[ch2]\n    prob = P[id1, id2]\n    logprob = torch.log(prob)\n    log_likelihood += logprob\n    print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n\nprint(f'{log_likelihood=}')\nneg_log_likelihood = -log_likelihood\nprint(f'{neg_log_likelihood=}')\n\n.e: 0.0478 -3.0408\nem: 0.0377 -3.2793\nmm: 0.0253 -3.6772\nma: 0.3899 -0.9418\na.: 0.1960 -1.6299\n.o: 0.0123 -4.3982\nol: 0.0780 -2.5508\nli: 0.1777 -1.7278\niv: 0.0152 -4.1867\nvi: 0.3541 -1.0383\nia: 0.1381 -1.9796\na.: 0.1960 -1.6299\n.a: 0.1377 -1.9829\nav: 0.0246 -3.7045\nva: 0.2495 -1.3882\na.: 0.1960 -1.6299\nlog_likelihood=tensor(-38.7856)\nneg_log_likelihood=tensor(38.7856)\n\n\nnice loss fn b/c the lowest it can get is 0; higher = worse predictions\nimportant note: want to make it average instead of a sum\n\nlog_likelihood = 0.0\nn = 0\n\nfor word in words[:3]:\n  chs = ['.'] + list(word) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    id1 = char_to_id[ch1]\n    id2 = char_to_id[ch2]\n    prob = P[id1, id2]\n    logprob = torch.log(prob)\n    log_likelihood += logprob\n    n += 1\n    print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n\nprint(f'{log_likelihood=}')\nneg_log_likelihood = -log_likelihood\nprint(f'{neg_log_likelihood=}')\n#normalized\nprint(f'{neg_log_likelihood/n}')\n\n.e: 0.0478 -3.0408\nem: 0.0377 -3.2793\nmm: 0.0253 -3.6772\nma: 0.3899 -0.9418\na.: 0.1960 -1.6299\n.o: 0.0123 -4.3982\nol: 0.0780 -2.5508\nli: 0.1777 -1.7278\niv: 0.0152 -4.1867\nvi: 0.3541 -1.0383\nia: 0.1381 -1.9796\na.: 0.1960 -1.6299\n.a: 0.1377 -1.9829\nav: 0.0246 -3.7045\nva: 0.2495 -1.3882\na.: 0.1960 -1.6299\nlog_likelihood=tensor(-38.7856)\nneg_log_likelihood=tensor(38.7856)\n2.424102306365967\n\n\n2.424102306365967 is loss for training\n\nwant to find parameters that minimize this loss\ngoal: maximize likelihood wrt model parameters\nin this case model params = probabilites\nin future: won’t be in table format but calculated by NNs\n\nmaximizing likelihood equivalent to maximizing log likelihood (scaling), minimizing negative log likelihood\n\nthis summarizes the quality of your model\nlower = better\n\nCan take our loop and apply any word to it:\n\nlog_likelihood = 0.0\nn = 0\n\nfor word in [\"andrej\"]:\n  chs = ['.'] + list(word) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    id1 = char_to_id[ch1]\n    id2 = char_to_id[ch2]\n    prob = P[id1, id2]\n    logprob = torch.log(prob)\n    log_likelihood += logprob\n    n += 1\n    print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n\nprint(f'{log_likelihood=}')\nneg_log_likelihood = -log_likelihood\nprint(f'{neg_log_likelihood=}')\nprint(f'{neg_log_likelihood/n}')\n\n#can see that ej is very unlikely\n\n.a: 0.1377 -1.9829\nan: 0.1605 -1.8296\nnd: 0.0384 -3.2594\ndr: 0.0771 -2.5620\nre: 0.1336 -2.0127\nej: 0.0027 -5.9171\nj.: 0.0245 -3.7098\nlog_likelihood=tensor(-21.2735)\nneg_log_likelihood=tensor(21.2735)\n3.03906512260437\n\n\n\nlog_likelihood = 0.0\nn = 0\n\nfor word in [\"andrejq\"]:\n  chs = ['.'] + list(word) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    id1 = char_to_id[ch1]\n    id2 = char_to_id[ch2]\n    prob = P[id1, id2]\n    logprob = torch.log(prob)\n    log_likelihood += logprob\n    n += 1\n    print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n\nprint(f'{log_likelihood=}')\nneg_log_likelihood = -log_likelihood\nprint(f'{neg_log_likelihood=}')\nprint(f'{neg_log_likelihood/n}')\n\n#get infinity since \"jq\" has prob of 0 for our model (infinite loss)\n\n.a: 0.1377 -1.9829\nan: 0.1605 -1.8296\nnd: 0.0384 -3.2594\ndr: 0.0771 -2.5620\nre: 0.1336 -2.0127\nej: 0.0027 -5.9171\njq: 0.0000 -inf\nq.: 0.1029 -2.2736\nlog_likelihood=tensor(-inf)\nneg_log_likelihood=tensor(inf)\ninf\n\n\nSolution: model smoothing\nadd fake counts\n\nP = (N+1).float()\nP /= P.sum(1, keepdim=True) \n\n\n#add 100\nP = (N+100).float()\nP /= P.sum(1, keepdim=True) \n\nthe more you add, the more uniform model you’re going to\n\n1 is a decent count to add\n\nRecap:\n\ntrained bigram character-level language model\ntrained bigram model by looking at counts of all bigrams and normalizing rows to get prob distributions\ncan use parameters of model to sample new words\ncan evaluate quality of the model in a single number\n\nAlternatively, cast into NN framework\n\nreceive a single character as input\noutputs probability distribution over next character in a sequence\ncan evaluate any setting of the parameters of NN using loss fn\n\ncompare prob assigned by model and the labels\n\nusing gradient-based optimization to tune params of network\n\ntune weights so NN is correctly probabilities for next character\n\n\nfirst compile training set\n\n#create train set\n\n#2 lists\nxs, ys = [], [] #inputs, labels\n\n#iterates over all bigrams\nfor word in words[:1]:\n  chs = ['.'] + list(word) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    id1 = char_to_id[ch1]\n    id2 = char_to_id[ch2]\n    print(ch1, ch2)\n    xs.append(id1)\n    ys.append(id2)\n\n#want tensors, not lists\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\n\n. e\ne m\nm m\nm a\na .\n\n\nabove: 5 examples for our NN; can summarize them below:\n\nxs\n\ntensor([ 0,  5, 13, 13,  1])\n\n\n\nys\n\ntensor([ 5, 13, 13,  1,  0])\n\n\nex. when input to NN is 5, we want its weights to be arranged so that 13 has a high probability\nAside: tensor vs Tensor – tensor infers dtype automatically\nhow to feed in examples into NN?\n\ncan’t just plug in integer index\nweights act multiplicatively on input\none hot encoding using torch.nn.functional.one_hot\n\ntakes in how long you want your tensor to be\n\n\n\nxenc = F.one_hot(xs, num_classes=27)\n\n\nxenc.shape\n\ntorch.Size([5, 27])\n\n\n\nplt.imshow(xenc)\n\n&lt;matplotlib.image.AxesImage at 0x12fa84e90&gt;\n\n\n\n\n\n5 examples –&gt; 5 rows\n\nappropriate bit turned on as 1 (the integer you are encoding) – everything else is 0\nthen vectors can feed into NN\n\n\nxenc.dtype\n\ntorch.int64\n\n\ndon’t want data type being plugged in to be integers\n\ncast to float (can’t specify data type otherwise)\n\n\n#cast to float\nxenc = F.one_hot(xs, num_classes=27).float()\n\n\nxenc.dtype\n\ntorch.float32\n\n\nNN perform \\(\\mathbf{Wx}+\\mathbf{b}\\) where \\(\\mathbf{Wx}\\) is a dot product\n\n#initalize weights by drawing from normal distribution randomly\n\nW = torch.randn((27, 1))\nW\n\ntensor([[ 0.3728],\n        [ 0.6661],\n        [-1.3309],\n        [-0.1947],\n        [ 0.4815],\n        [ 0.8087],\n        [ 1.5211],\n        [ 0.1900],\n        [ 1.8205],\n        [-0.2856],\n        [ 0.3763],\n        [ 0.7953],\n        [-0.4781],\n        [ 1.0884],\n        [ 0.9759],\n        [ 0.1490],\n        [-1.1983],\n        [-0.3366],\n        [-2.4450],\n        [ 0.1783],\n        [ 0.7675],\n        [-0.8531],\n        [ 2.5560],\n        [ 1.6381],\n        [ 0.5793],\n        [ 1.1928],\n        [-0.8738]])\n\n\nthese weights then multiplied by inputs:\n\nxenc @ W\n\ntensor([[0.3728],\n        [0.8087],\n        [1.0884],\n        [1.0884],\n        [0.6661]])\n\n\noutput: 5x1\n(5, 27) @ (27, 1)\n\nthe 27 will multiply and add\nwe are seeing the 5 activations of this neuron on these 5 inputs+evaluate in parallel\n\nfed into the same neuron and in parallel, PyTorch evaluates \\(\\mathbf{Wx}\\)\n\n\nwe want 27 neurons\n\nwill evaluate all 27 neurons on all 5 inputs\noutput will be 5x27\nfor every 1 of 27 neurons, what is the firing rate of those five examples?\n\n\nW = torch.randn((27, 27))\nxenc @ W\n\ntensor([[ 0.0127, -1.2721, -0.4441,  0.6871,  0.3346,  0.6414, -1.3974, -0.3442,\n         -0.3517, -2.0894,  0.1947,  1.6428, -0.0271, -1.2485,  0.9322, -0.3985,\n         -1.5168,  1.2239,  0.0095,  0.5157, -0.3521, -0.2780, -1.4493,  1.4259,\n          0.5597, -0.7602,  0.4888],\n        [ 1.2436, -1.4493,  0.1234, -1.7376, -0.1972, -0.3674,  0.3938,  0.7951,\n          0.7886,  0.2017, -0.0832, -0.6972,  1.0421,  0.2409,  0.6100, -0.4380,\n         -0.7845, -0.4896, -0.2760,  1.7668, -0.8636,  0.2208, -1.0639,  1.5194,\n         -0.1063,  0.2183, -0.1574],\n        [-0.1304,  0.2048, -1.2809, -0.5326,  0.5429, -0.5297, -0.5725, -1.0901,\n          1.6027,  0.5139,  1.5645,  1.1537,  0.4501,  0.2249,  0.1239,  0.5280,\n         -0.1994,  1.4687,  1.1634,  1.1873,  0.7968, -1.9044, -0.3376,  0.5999,\n         -0.1986, -0.4938, -0.5324],\n        [-0.1304,  0.2048, -1.2809, -0.5326,  0.5429, -0.5297, -0.5725, -1.0901,\n          1.6027,  0.5139,  1.5645,  1.1537,  0.4501,  0.2249,  0.1239,  0.5280,\n         -0.1994,  1.4687,  1.1634,  1.1873,  0.7968, -1.9044, -0.3376,  0.5999,\n         -0.1986, -0.4938, -0.5324],\n        [-2.3533, -1.5248,  0.3334, -1.0110,  1.1808,  0.0563,  1.5598, -0.5443,\n         -0.4854,  0.1344,  0.3298,  1.9121, -0.0591,  0.1370, -0.1254, -1.0686,\n         -0.6022, -0.7270, -0.1394, -0.1541, -1.2129,  1.4275, -0.4483, -0.4349,\n         -0.0274, -0.1073, -0.7638]])\n\n\n\n#ex\n(xenc @ W)[3, 13]\n\n#firing rate of 13th neuron looking at 3rd input\n#achieved w/ dot product between 3rd input and 13th column of W matrix\n\ntensor(0.2249)\n\n\n\nxenc[3]\n\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\nW[:, 13]\n\ntensor([-1.2485,  0.1370,  0.3096,  1.3527, -0.6874,  0.2409, -1.0772,  1.1542,\n         0.0684,  0.0760, -1.5029, -1.3511,  0.7832,  0.2249,  0.4600,  1.2742,\n        -0.1361, -0.0859, -1.0860,  0.8799,  0.6354, -0.3532,  0.8553, -0.9668,\n         1.9522, -0.5757, -0.7297])\n\n\n\n(xenc[3] * W[:, 13]).sum()\n#shows it's being done efficiently by matrix multiplication\n\ntensor(0.2249)\n\n\n27 inputs, 27 neurons\n\nleave this to be a linear layer\nfor every single input example, trying to produce some kind of probability distribution for next character in the sequence\nneed to figure out how to interpret numbers that neurons take on\n\nwhat should the NN output?\n\ncan’t output counts; some are negative; not integers\nprobabailities: positive, sum to 1\n27 numbers are giving us log counts (not direct counts)\nto get counts, take log count and exponentiate them\n\nyou will get numbers below 1 if you plug in neg numbers\ngreater than 1 if you plug in pos numbers\n\n\ninstead of numbers being all over the place, interpret as log counts and element-wise exponentiate\n\n(xenc @ W).exp()\n\ntensor([[1.0128, 0.2802, 0.6414, 1.9879, 1.3974, 1.8991, 0.2472, 0.7088, 0.7035,\n         0.1238, 1.2150, 5.1697, 0.9733, 0.2869, 2.5402, 0.6713, 0.2194, 3.4003,\n         1.0096, 1.6748, 0.7032, 0.7573, 0.2347, 4.1616, 1.7501, 0.4676, 1.6304],\n        [3.4681, 0.2347, 1.1313, 0.1759, 0.8210, 0.6925, 1.4826, 2.2147, 2.2003,\n         1.2234, 0.9202, 0.4980, 2.8351, 1.2724, 1.8405, 0.6453, 0.4563, 0.6129,\n         0.7588, 5.8518, 0.4217, 1.2470, 0.3451, 4.5693, 0.8992, 1.2440, 0.8544],\n        [0.8778, 1.2273, 0.2778, 0.5871, 1.7210, 0.5888, 0.5641, 0.3362, 4.9663,\n         1.6717, 4.7804, 3.1699, 1.5685, 1.2522, 1.1319, 1.6955, 0.8192, 4.3437,\n         3.2007, 3.2783, 2.2184, 0.1489, 0.7135, 1.8220, 0.8199, 0.6103, 0.5872],\n        [0.8778, 1.2273, 0.2778, 0.5871, 1.7210, 0.5888, 0.5641, 0.3362, 4.9663,\n         1.6717, 4.7804, 3.1699, 1.5685, 1.2522, 1.1319, 1.6955, 0.8192, 4.3437,\n         3.2007, 3.2783, 2.2184, 0.1489, 0.7135, 1.8220, 0.8199, 0.6103, 0.5872],\n        [0.0951, 0.2177, 1.3957, 0.3639, 3.2569, 1.0579, 4.7577, 0.5803, 0.6155,\n         1.1439, 1.3907, 6.7671, 0.9427, 1.1469, 0.8821, 0.3435, 0.5476, 0.4834,\n         0.8699, 0.8572, 0.2973, 4.1684, 0.6387, 0.6474, 0.9730, 0.8982, 0.4659]])\n\n\nnegative numbers –&gt; less than 1; positive numbers –&gt; more positive\nlog counts: logits\n\nlogits = xenc @ W #log-counts\ncounts = logits.exp() #equivalent N\nprobs = counts/counts.sum(1, keepdim=True) #normalize probs\nprobs\n\ntensor([[0.0282, 0.0078, 0.0179, 0.0554, 0.0390, 0.0529, 0.0069, 0.0198, 0.0196,\n         0.0035, 0.0339, 0.1441, 0.0271, 0.0080, 0.0708, 0.0187, 0.0061, 0.0948,\n         0.0281, 0.0467, 0.0196, 0.0211, 0.0065, 0.1160, 0.0488, 0.0130, 0.0455],\n        [0.0891, 0.0060, 0.0291, 0.0045, 0.0211, 0.0178, 0.0381, 0.0569, 0.0565,\n         0.0314, 0.0236, 0.0128, 0.0729, 0.0327, 0.0473, 0.0166, 0.0117, 0.0157,\n         0.0195, 0.1504, 0.0108, 0.0320, 0.0089, 0.1174, 0.0231, 0.0320, 0.0220],\n        [0.0195, 0.0273, 0.0062, 0.0131, 0.0383, 0.0131, 0.0125, 0.0075, 0.1104,\n         0.0372, 0.1063, 0.0705, 0.0349, 0.0278, 0.0252, 0.0377, 0.0182, 0.0966,\n         0.0712, 0.0729, 0.0493, 0.0033, 0.0159, 0.0405, 0.0182, 0.0136, 0.0131],\n        [0.0195, 0.0273, 0.0062, 0.0131, 0.0383, 0.0131, 0.0125, 0.0075, 0.1104,\n         0.0372, 0.1063, 0.0705, 0.0349, 0.0278, 0.0252, 0.0377, 0.0182, 0.0966,\n         0.0712, 0.0729, 0.0493, 0.0033, 0.0159, 0.0405, 0.0182, 0.0136, 0.0131],\n        [0.0027, 0.0061, 0.0390, 0.0102, 0.0910, 0.0295, 0.1329, 0.0162, 0.0172,\n         0.0319, 0.0388, 0.1890, 0.0263, 0.0320, 0.0246, 0.0096, 0.0153, 0.0135,\n         0.0243, 0.0239, 0.0083, 0.1164, 0.0178, 0.0181, 0.0272, 0.0251, 0.0130]])\n\n\n\nprobs[0].sum() #every row sums to 1\n\ntensor(1.0000)\n\n\n\nprobs.shape\n\ntorch.Size([5, 27])\n\n\n\nprobs[0]\n\ntensor([0.0282, 0.0078, 0.0179, 0.0554, 0.0390, 0.0529, 0.0069, 0.0198, 0.0196,\n        0.0035, 0.0339, 0.1441, 0.0271, 0.0080, 0.0708, 0.0187, 0.0061, 0.0948,\n        0.0281, 0.0467, 0.0196, 0.0211, 0.0065, 0.1160, 0.0488, 0.0130, 0.0455])\n\n\nfor every one of our five examples, we now have a row that came out of the NN\n\nb/c of the transformations here, we made sure that output of neural net are now probabilities\n\\(\\mathbf{Wx}\\) gives us logits –&gt; interpret as log counts –&gt; exponentiate to get something like counts –&gt; normalize for probabilities\nall of these are differentiable operations that we can backprop through\n\nexample walkthrough:\n\n0th example – .e\nfeed in . into NN (by getting index, one hot encoding it)\ndistr of probabilities come out (shape of 27) – 27 numbers – NN assignment for how likely the 27 characters are to come next\nas we tune weights \\(\\mathbf{W}\\), we will get different probabilities out for any character that you input\noptimize+find a good \\(\\mathbf{W}\\) so probabilities are good\nmeasure performance w/ loss fn\n\nSummary:\n\n#randomly initialize 27 neurons' weights. each neuron receives 27 inputs\ng = Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g)\n\n\n#forward pass\n\n#input to the network: one-hot encoding\nxenc = F.one_hot(xs, num_classes=27).float() #becomes 5x27 array 0s except for a few ones\n#predict log-counts\nlogits = xenc @ W \n\n#softmax:\n\n#counts, equivalent to N\ncounts = logits.exp()\n#probabilities for next character\nprobs = counts / counts.sum(1, keepdims=True)\n\nsoftmax activation fn: \\(\\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\\)\n\ntakes \\(z\\)s (logits), exponentiates them, and divides/normalizes them\noutputs probability distributions (something that always sums to 1)\nnormalization fn\ncan put on top of any other linear layer and NN will output probabilities\n\nNote that everything above in forward pass is made up of differentiable equations. We know how to backprop through (sum, division, multiplication, exponentiation)\nwe get probabilities in 5x27 array\n\nfor each example, we have a vector of probs that sum to 1\n\n\nneg_log_likelihood = torch.zeros(5)\nfor i in range(5):\n    # i-th bigram:\n    x = xs[i].item() # input character index\n    y = ys[i].item() # label character index\n    print('--------')\n    print(f'bigram example {i+1}: {id_to_char[x]}{id_to_char[y]} (indexes {x},{y})')\n    print('input to the neural net:', x)\n    print('output probabilities from the neural net:', probs[i])\n    print('label (actual next character):', y)\n    p = probs[i, y]\n    print('probability assigned by the net to the the correct character:', p.item())\n    logp = torch.log(p)\n    print('log likelihood:', logp.item())\n    nll = -logp\n    print('negative log likelihood:', nll.item())\n    neg_log_likelihood[i] = nll\n\nprint('=========')\nprint('average negative log likelihood, i.e. loss =', neg_log_likelihood.mean().item())\n\n--------\nbigram example 1: .e (indexes 0,5)\ninput to the neural net: 0\noutput probabilities from the neural net: tensor([0.0282, 0.0078, 0.0179, 0.0554, 0.0390, 0.0529, 0.0069, 0.0198, 0.0196,\n        0.0035, 0.0339, 0.1441, 0.0271, 0.0080, 0.0708, 0.0187, 0.0061, 0.0948,\n        0.0281, 0.0467, 0.0196, 0.0211, 0.0065, 0.1160, 0.0488, 0.0130, 0.0455])\nlabel (actual next character): 5\nprobability assigned by the net to the the correct character: 0.0529467947781086\nlog likelihood: -2.9384677410125732\nnegative log likelihood: 2.9384677410125732\n--------\nbigram example 2: em (indexes 5,13)\ninput to the neural net: 5\noutput probabilities from the neural net: tensor([0.0891, 0.0060, 0.0291, 0.0045, 0.0211, 0.0178, 0.0381, 0.0569, 0.0565,\n        0.0314, 0.0236, 0.0128, 0.0729, 0.0327, 0.0473, 0.0166, 0.0117, 0.0157,\n        0.0195, 0.1504, 0.0108, 0.0320, 0.0089, 0.1174, 0.0231, 0.0320, 0.0220])\nlabel (actual next character): 13\nprobability assigned by the net to the the correct character: 0.03269490599632263\nlog likelihood: -3.4205360412597656\nnegative log likelihood: 3.4205360412597656\n--------\nbigram example 3: mm (indexes 13,13)\ninput to the neural net: 13\noutput probabilities from the neural net: tensor([0.0195, 0.0273, 0.0062, 0.0131, 0.0383, 0.0131, 0.0125, 0.0075, 0.1104,\n        0.0372, 0.1063, 0.0705, 0.0349, 0.0278, 0.0252, 0.0377, 0.0182, 0.0966,\n        0.0712, 0.0729, 0.0493, 0.0033, 0.0159, 0.0405, 0.0182, 0.0136, 0.0131])\nlabel (actual next character): 13\nprobability assigned by the net to the the correct character: 0.02783992886543274\nlog likelihood: -3.5812840461730957\nnegative log likelihood: 3.5812840461730957\n--------\nbigram example 4: ma (indexes 13,1)\ninput to the neural net: 13\noutput probabilities from the neural net: tensor([0.0195, 0.0273, 0.0062, 0.0131, 0.0383, 0.0131, 0.0125, 0.0075, 0.1104,\n        0.0372, 0.1063, 0.0705, 0.0349, 0.0278, 0.0252, 0.0377, 0.0182, 0.0966,\n        0.0712, 0.0729, 0.0493, 0.0033, 0.0159, 0.0405, 0.0182, 0.0136, 0.0131])\nlabel (actual next character): 1\nprobability assigned by the net to the the correct character: 0.0272862259298563\nlog likelihood: -3.6013731956481934\nnegative log likelihood: 3.6013731956481934\n--------\nbigram example 5: a. (indexes 1,0)\ninput to the neural net: 1\noutput probabilities from the neural net: tensor([0.0027, 0.0061, 0.0390, 0.0102, 0.0910, 0.0295, 0.1329, 0.0162, 0.0172,\n        0.0319, 0.0388, 0.1890, 0.0263, 0.0320, 0.0246, 0.0096, 0.0153, 0.0135,\n        0.0243, 0.0239, 0.0083, 0.1164, 0.0178, 0.0181, 0.0272, 0.0251, 0.0130])\nlabel (actual next character): 0\nprobability assigned by the net to the the correct character: 0.0026549557223916054\nlog likelihood: -5.9313273429870605\nnegative log likelihood: 5.9313273429870605\n=========\naverage negative log likelihood, i.e. loss = 3.8945975303649902\n\n\noverall loss: 3.8945975303649902\noptimize a NN by starting w/ some random guess\n\nloss made up of differentiable operations\nminimize loss by tuning \\(\\mathbf{W}\\)\n\ncomputing gradients of loss wrt these matrices\nfind good setting of \\(\\mathbf{W}\\) using gradient-based optimization\n\n\nwe want to convert integers to vector\n\nour NN: single linear layer and softmax\nloss.backward (from micrograd) at output node initiated backprop; nudge parameters in opposite direction of gradient\n\nuse negative log likelihood since we are doing classification\n\nprobs.shape\n\ntorch.Size([5, 27])\n\n\nwant to pluck out the right probabilities\ne.g. first example – looking at probability at index 5; second example at index 13, etc.\n\nprobs[0, 5], probs[1, 13], probs[2, 13], probs[3, 1], probs[4, 0]\n#probabilities we want\n\n(tensor(0.0529),\n tensor(0.0327),\n tensor(0.0278),\n tensor(0.0273),\n tensor(0.0027))\n\n\nwant a more efficient way to access these probs – in PyTorch, can pass in all of these integers in the vectors\n\ntorch.arange(5)\n\ntensor([0, 1, 2, 3, 4])\n\n\n\nprobs[torch.arange(5), ys]\n#plucks out probs that NN assigns to the correct next character\n\ntensor([0.0529, 0.0327, 0.0278, 0.0273, 0.0027])\n\n\n\nloss = -probs[torch.arange(5), ys].log().mean()\nloss #vectorized form of earlier expression\n\ntensor(3.8946)\n\n\npass in requires_grad=True to your weight matrix\n\ng = Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g, requires_grad=True)\n\n\n#forward pass\nxenc = F.one_hot(xs, num_classes=27).float() \nlogits = xenc @ W \ncounts = logits.exp()\nprobs = counts / counts.sum(1, keepdims=True)\nloss = -probs[torch.arange(5), ys].log().mean()\n\n\n#backward pass\n#make sure gradients are reset (set to None more efficent than 0)\nW.grad = None\nloss.backward()\n\n.backward() fills in gradients all the way back to parameters in \\(\\mathbf{W}\\)\n\nW.grad\n\ntensor([[ 0.0121,  0.0020,  0.0025,  0.0008,  0.0034, -0.1975,  0.0005,  0.0046,\n          0.0027,  0.0063,  0.0016,  0.0056,  0.0018,  0.0016,  0.0100,  0.0476,\n          0.0121,  0.0005,  0.0050,  0.0011,  0.0068,  0.0022,  0.0006,  0.0040,\n          0.0024,  0.0307,  0.0292],\n        [-0.1970,  0.0017,  0.0079,  0.0020,  0.0121,  0.0062,  0.0217,  0.0026,\n          0.0025,  0.0010,  0.0205,  0.0017,  0.0198,  0.0022,  0.0046,  0.0041,\n          0.0082,  0.0016,  0.0180,  0.0106,  0.0093,  0.0062,  0.0010,  0.0066,\n          0.0131,  0.0101,  0.0018],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0058,  0.0159,  0.0050,  0.0104,  0.0398,  0.0058,  0.0019,  0.0067,\n          0.0019,  0.0060,  0.0140,  0.0046,  0.0023, -0.1964,  0.0022,  0.0063,\n          0.0058,  0.0009,  0.0183,  0.0043,  0.0097,  0.0060,  0.0100,  0.0005,\n          0.0024,  0.0004,  0.0094],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0125, -0.1705,  0.0194,  0.0133,  0.0270,  0.0080,  0.0105,  0.0100,\n          0.0490,  0.0066,  0.0030,  0.0316,  0.0052, -0.1893,  0.0059,  0.0045,\n          0.0234,  0.0049,  0.0260,  0.0023,  0.0083,  0.0031,  0.0053,  0.0081,\n          0.0482,  0.0187,  0.0051],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000]])\n\n\n\nW.grad.shape\n\ntorch.Size([27, 27])\n\n\n\nW.shape\n\ntorch.Size([27, 27])\n\n\n\nprint(loss.item())\n\n3.7693049907684326\n\n\n27x27 – same as W.grad’s shape\nevery element of W.grad tells us the influence of that weight on the loss fn\nEx. 0.0121 tells us this has a positive influence in loss; increases loss\nUse gradient info to update weights of NN (no need to do a loop since we just have 1 tensor)\n\nW.data += -0.1*W.grad #update\n\n\n#expect loss to decrease\n#forward again\nxenc = F.one_hot(xs, num_classes=27).float() \nlogits = xenc @ W \ncounts = logits.exp()\nprobs = counts / counts.sum(1, keepdims=True)\nloss = -probs[torch.arange(5), ys].log().mean()\n\n\nprint(loss.item()) #slightly lower\n\n3.7492127418518066\n\n\n\n#reset W.grad; backward pass\nW.grad = None\nloss.backward()\n\n\n#update\nW.data += -0.1*W.grad \n\n\n#forward again...\nxenc = F.one_hot(xs, num_classes=27).float() \nlogits = xenc @ W \ncounts = logits.exp()\nprobs = counts / counts.sum(1, keepdims=True)\nloss = -probs[torch.arange(5), ys].log().mean()\n\n\nprint(loss.item()) #lower\n\n3.7291626930236816\n\n\nwe are now doing gradient descent\nstill only iterating first word “emma”:\n\n#create the dataset\nxs, ys = [], []\nfor w in words[:1]:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = char_to_id[ch1]\n    ix2 = char_to_id[ch2]\n    xs.append(ix1)\n    ys.append(ix2)\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\nnum = xs.nelement() #counts # examples -- 5 bigrams in \"emma\"\nprint('number of examples: ', num)\n\n#initialize the 'network'\ng = Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g, requires_grad=True)\n\nnumber of examples:  5\n\n\n\n#gradient descent\nfor k in range(10): #10 iteractions of forward backward update\n  \n  #forward pass\n  xenc = F.one_hot(xs, num_classes=27).float() #input to the network: one-hot encoding\n  logits = xenc @ W #predict log-counts\n  counts = logits.exp() #counts, equivalent to N\n  probs = counts / counts.sum(1, keepdims=True) #probs for next character\n  loss = -probs[torch.arange(num), ys].log().mean()\n  print(loss.item())\n  \n  #backward pass\n  W.grad = None #zero the gradient\n  loss.backward()\n  \n  #update\n  W.data += -0.1*W.grad\n\n3.7693049907684326\n3.7492127418518066\n3.7291626930236816\n3.7091546058654785\n3.6891887187957764\n3.6692662239074707\n3.6493873596191406\n3.629552125930786\n3.6097614765167236\n3.5900158882141113\n\n\ngives us some improvement on loss fn\nnow w/ whole dataset:\n\n#whole dataset\nxs, ys = [], []\nfor w in words:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = char_to_id[ch1]\n    ix2 = char_to_id[ch2]\n    xs.append(ix1)\n    ys.append(ix2)\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\nnum = xs.nelement()\nprint('number of examples: ', num)\n\n#initialize the 'network'\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g, requires_grad=True)\n\nnumber of examples:  228146\n\n\n\n#gradient descent\nfor k in range(10):\n  \n  #forward pass\n  xenc = F.one_hot(xs, num_classes=27).float() \n  logits = xenc @ W #predict log-counts\n  counts = logits.exp() #counts &lt;=&gt; N\n  probs = counts / counts.sum(1, keepdims=True) #probs for next character\n  loss = -probs[torch.arange(num), ys].log().mean()\n  print(loss.item())\n  \n  #backward pass\n  W.grad = None\n  loss.backward()\n  \n  #update\n  W.data += -0.1*W.grad\n\n3.758953332901001\n3.758039712905884\n3.757127523422241\n3.756216526031494\n3.7553062438964844\n3.754396915435791\n3.7534890174865723\n3.7525813579559326\n3.7516753673553467\n3.750770330429077\n\n\nnow optimizing entire training set\ncan afford larger learning rate\n\n#whole dataset\nxs, ys = [], []\nfor w in words:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = char_to_id[ch1]\n    ix2 = char_to_id[ch2]\n    xs.append(ix1)\n    ys.append(ix2)\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\nnum = xs.nelement()\nprint('number of examples: ', num)\n\n#initialize the 'network'\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g, requires_grad=True)\n\nnumber of examples:  228146\n\n\n\n#gradient descent\nfor k in range(100):\n  \n  #forward pass\n  xenc = F.one_hot(xs, num_classes=27).float() \n  logits = xenc @ W #predict log-counts\n  counts = logits.exp() #counts &lt;=&gt; N\n  probs = counts / counts.sum(1, keepdims=True) #probs for next character\n  loss = -probs[torch.arange(num), ys].log().mean()\n  print(loss.item())\n  \n  #backward pass\n  W.grad = None\n  loss.backward()\n  \n  #update\n  W.data += -50*W.grad\n\n2.4726526737213135\n2.4724342823028564\n2.4722204208374023\n2.472010850906372\n2.4718058109283447\n2.4716055393218994\n2.4714088439941406\n2.4712166786193848\n2.4710280895233154\n2.47084379196167\n2.4706625938415527\n2.4704854488372803\n2.4703118801116943\n2.4701414108276367\n2.4699742794036865\n2.4698104858398438\n2.4696500301361084\n2.4694924354553223\n2.4693377017974854\n2.4691858291625977\n2.4690372943878174\n2.468891143798828\n2.468747615814209\n2.46860671043396\n2.468468427658081\n2.4683327674865723\n2.4681990146636963\n2.4680681228637695\n2.4679393768310547\n2.4678127765655518\n2.46768856048584\n2.46756649017334\n2.4674463272094727\n2.4673283100128174\n2.467211961746216\n2.467097759246826\n2.4669857025146484\n2.4668753147125244\n2.466766595840454\n2.4666597843170166\n2.466554880142212\n2.466451406478882\n2.4663493633270264\n2.4662492275238037\n2.4661500453948975\n2.4660532474517822\n2.4659576416015625\n2.4658634662628174\n2.465770721435547\n2.465679407119751\n2.4655892848968506\n2.465500593185425\n2.4654133319854736\n2.465327501296997\n2.465242624282837\n2.4651589393615723\n2.4650766849517822\n2.4649956226348877\n2.4649155139923096\n2.4648361206054688\n2.46475887298584\n2.464682102203369\n2.464606285095215\n2.464531660079956\n2.4644579887390137\n2.464385509490967\n2.4643137454986572\n2.464243173599243\n2.4641737937927246\n2.464104652404785\n2.464036703109741\n2.4639699459075928\n2.4639036655426025\n2.4638383388519287\n2.463773727416992\n2.463710308074951\n2.4636473655700684\n2.463585376739502\n2.4635238647460938\n2.463463544845581\n2.4634039402008057\n2.4633448123931885\n2.4632863998413086\n2.463228702545166\n2.463172197341919\n2.4631154537200928\n2.463060140609741\n2.4630050659179688\n2.4629504680633545\n2.4628970623016357\n2.462843894958496\n2.462791681289673\n2.4627397060394287\n2.462688684463501\n2.4626379013061523\n2.462587833404541\n2.462538242340088\n2.462489128112793\n2.4624407291412354\n2.462393045425415\n\n\nachieving roughly same result w/ gradient-based optimization (loss is still around 2.47) as explicit approach\nGradient-based approach a lot more flexible – can make NN more complex\n\ntake multiple previous characters and feeding them into more complex NN\noutputs will still be logits (go through same process)\neven as we complexify all the way to transformers, only thing that will change: forward pass where we take in some previous characters and calculate logits for next character in sequence (get more complex)\nsame optimization machine\ntable approach would get too large once we have many previous characters\n\nAdditional notes:\n\nxenc (one hot vectors) multiplied by \\(\\mathbf{W}\\) matrix – e.g. if you have a one hot vector in 5th dimension, multiplying that with w ends up plucking out fifth row of \\(\\mathbf{W}\\)\n\nlogits becomes fifth row of \\(\\mathbf{W}\\) from matrix multiplication\narray from earlier approach = array \\(\\mathbf{W}\\) at the end of optimization w/ gradient-based approach (using loss fn to guide us)\n\nsmoothing – increasing the count –&gt; probability gets more and more uniform –&gt; more and more even\n\ngradient-based framework has an equivalent to smoothing:\n\\(\\mathbf{W}\\) – initialized randomly; think about it as initalizating all entries of \\(\\mathbf{W}\\) to 0 –&gt; logits become all zero –&gt; exponentiating those logits becomes all 1 –&gt; probabilities become all uniform\nwhen entires of \\(\\mathbf{W}\\) are all 0, probabilities will be uniform\ntrying to incentivize \\(\\mathbf{W}\\) to be near 0 &lt;=&gt; label smoothing\n\nmore you incentivize that in loss fn, the smoother the distribution you’re going to achieve\n=&gt; brings us to regularization where we can augment loss fn to have a small component called regularization loss\nex. can take \\(\\mathbf{W}\\) and square all of its entries (no more signs – neg and pos get squashed to be positive numbers), then sum (or mean)\n\nachieve zero loss if \\(\\mathbf{W}\\) is exactly 0; but if non-zero, you accumulate loss\ncan add to loss\n\n\n\n\n\n#whole dataset\nxs, ys = [], []\nfor w in words:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = char_to_id[ch1]\n    ix2 = char_to_id[ch2]\n    xs.append(ix1)\n    ys.append(ix2)\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\nnum = xs.nelement()\nprint('number of examples: ', num)\n\n#initialize the 'network'\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g, requires_grad=True)\n\nnumber of examples:  228146\n\n\n\n#gradient descent\nfor k in range(100):\n  \n  #forward pass\n  xenc = F.one_hot(xs, num_classes=27).float() \n  logits = xenc @ W #predict log-counts\n  counts = logits.exp() #counts &lt;=&gt; N\n  probs = counts / counts.sum(1, keepdims=True) #probs for next character\n  #regularization strength: 0.01\n  loss = -probs[torch.arange(num), ys].log().mean() +  0.01*(W**2).mean()\n  print(loss.item())\n  \n  #backward pass\n  W.grad = None\n  loss.backward()\n  \n  #update\n  W.data += -50*W.grad\n\n3.768618583679199\n3.3788068294525146\n3.161090850830078\n3.0271859169006348\n2.9344840049743652\n2.867231607437134\n2.8166542053222656\n2.777146577835083\n2.745253801345825\n2.7188305854797363\n2.696505308151245\n2.6773719787597656\n2.6608052253723145\n2.6463515758514404\n2.633665084838867\n2.622471570968628\n2.6125476360321045\n2.6037068367004395\n2.595794439315796\n2.5886807441711426\n2.5822560787200928\n2.576429843902588\n2.5711236000061035\n2.566272735595703\n2.5618226528167725\n2.5577261447906494\n2.5539441108703613\n2.550442695617676\n2.5471930503845215\n2.5441696643829346\n2.5413522720336914\n2.538722038269043\n2.536262035369873\n2.5339579582214355\n2.531797409057617\n2.529768228530884\n2.527860164642334\n2.5260636806488037\n2.5243704319000244\n2.522773265838623\n2.52126407623291\n2.519836664199829\n2.5184857845306396\n2.5172054767608643\n2.515990734100342\n2.5148372650146484\n2.5137407779693604\n2.51269793510437\n2.511704921722412\n2.5107579231262207\n2.509855031967163\n2.5089924335479736\n2.5081686973571777\n2.507380247116089\n2.5066258907318115\n2.5059030055999756\n2.5052106380462646\n2.5045459270477295\n2.5039076805114746\n2.503295421600342\n2.5027058124542236\n2.5021398067474365\n2.501594305038452\n2.5010695457458496\n2.500562906265259\n2.500075578689575\n2.4996049404144287\n2.499150514602661\n2.4987120628356934\n2.49828839302063\n2.4978787899017334\n2.497483015060425\n2.4970996379852295\n2.4967286586761475\n2.496370315551758\n2.4960227012634277\n2.4956860542297363\n2.4953596591949463\n2.4950432777404785\n2.494736433029175\n2.494438886642456\n2.494149684906006\n2.4938690662384033\n2.4935970306396484\n2.4933321475982666\n2.493075132369995\n2.4928252696990967\n2.492582321166992\n2.4923462867736816\n2.492116928100586\n2.4918932914733887\n2.491675853729248\n2.491464614868164\n2.491258382797241\n2.491057872772217\n2.4908623695373535\n2.4906721115112305\n2.4904870986938477\n2.4903063774108887\n2.490130662918091\n\n\nnow optimization has 2 components:\n\ntrying to make all probs work out\ntrying to make all \\(\\mathbf{W}\\) entries be 0\n\nif non-zero, you feel a loss; only way to achieve minimization of this is for \\(\\mathbf{W}\\) to be 0\nwill want to be 0; probs will want to be uniform\nsimultaneously want to match up probs as indicated by the data\nstrength of regularization is controlling amount of counts added (to “smooth”) –&gt; the more the second part of regularization loss (“penalty term”) will dominate –&gt; the more the weights unable to grow (since they will accumulate way too much loss)\n\n\nSampling from this NN\n\nbefore, we would grab a row of P (probability row) from which we sampled the next index; break when 0\nnow: p comes from NN; encode ix into one hot row of xenc\n\nxenc multiplies with W –&gt; plucls out corresponding row of W corresponding to ix –&gt; gets logits\nnormalize logits –&gt; exponentiate to get counts –&gt; normalize to get distribution –&gt; sample from the distribution\n\nend up w/ same result and previous approach since they are identical models; \\(\\mathbf{W}\\) is log counts of what we’ve estimated before\n\n\ng = torch.Generator().manual_seed(2147483647)\n\nfor i in range(5):\n  \n  out = []\n  ix = 0\n  while True:\n    \n    # ----------\n    # BEFORE:\n    #p = P[ix]\n    # ----------\n    # NOW:\n    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n    logits = xenc @ W # predict log-counts\n    counts = logits.exp() # counts, equivalent to N\n    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n    # ----------\n    \n    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n    out.append(id_to_char[ix])\n    if ix == 0:\n      break\n  print(''.join(out))\n\njunide.\njanasah.\np.\ncfay.\na."
  },
  {
    "objectID": "posts/mx_alg_basics_pt1/index.html",
    "href": "posts/mx_alg_basics_pt1/index.html",
    "title": "Matrix algebra basics (numpy) - part 1",
    "section": "",
    "text": "from numpy import diag, array, eye, outer, trace, kron, allclose, linalg as la\n\nCreating a matrix\n\n\\[\n\\mathbf{L} = \\begin{pmatrix}    1&2&3\\\\    4&5&6\\end{pmatrix}\n\\]\nmx_L = array([[1,2,3],[4,5,6]])\n\nMatrix addition\n\n\\[\n\\begin{align*}\n\\mathbf{A} &=\n\\begin{pmatrix}\n1&2\\\\\n3&4\n\\end{pmatrix}\\\\\n\\mathbf{B} &=\n\\begin{pmatrix}\n5&6\\\\\n7&8\n\\end{pmatrix}\\\\\n\\mathbf{A}+\\mathbf{B} &=\n\\begin{pmatrix}\n6&8\\\\\n10&12\n\\end{pmatrix}\n\\end{align*}\n\\]\nmx_A = array([[1,2],[3,4]])\nmx_B = array([[5,6],[7,8]])\nmx_Y = mx_A+mx_B\n\nMatrix multiplication\n\n\\[\n\\mathbf{AB} =\n\\begin{pmatrix}\n19&22\\\\\n47&50\n\\end{pmatrix}\n\\]\nmx_F = mx_A.dot(mx_B)\n\nMatrix transpose\n\n\\[\n\\mathbf{A}^T =\n\\begin{pmatrix}\n1&3\\\\\n2&4\n\\end{pmatrix}\n\\]\nmx_A_t = mx_A.T\n\nIdentity matrix\n\n\\[\n\\mathbf{I} =\n\\begin{pmatrix}\n1&0&0\\\\\n0&1&0\\\\\n0&0&1\n\\end{pmatrix}\n\\]\nmx_I = eye(3,3)\n\nInverse matrix\n\n\\[\n\\begin{align*}\n\\mathbf{C} &=\n\\begin{pmatrix}\n1&2\\\\\n3&4\n\\end{pmatrix}\\\\\n\\mathbf{C}^{-1} &=\n\\begin{pmatrix}\n-2&1\\\\\n\\frac{3}{2}&\\frac{-1}{2}\n\\end{pmatrix}\\\\\n\\end{align*}\n\\]\nmx_C = array([[1,2],[3,4]])\nmx_C_inv = la.inv(mx_C)\n\nEigenvalues+eigenvectors (\\(\\lambda_i\\), \\(\\mathbf{x_i}\\))\n\n\\[\n\\mathbf{Cx} = \\lambda\\mathbf{x}\n\\]\nC_evals, C_evecs = la.eig(mx_C)\n\nMatrix reshape\n\n\\[\n\\begin{align*}\n\\mathbf{g} &=\n\\begin{pmatrix}\n1&2&3&4&5&6&7&8&9\n\\end{pmatrix}\\\\\n\\mathbf{G} &=\n\\begin{pmatrix}\n1&2&3\\\\\n4&5&6\\\\\n7&8&9\n\\end{pmatrix}\n\\end{align*}\n\\]\nvec_g = array([1,2,3,4,5,6,7,8,9])\nmx_G = vec_g.reshape(3,3)\n\nElement-wise matrix multiplication\n\n\\[\n\\mathbf{A}\\odot\\mathbf{B} =\n\\begin{pmatrix}\n5&12\\\\\n21&32\n\\end{pmatrix}\n\\]\nmx_H = mx_A*mx_B\n\nMatrix determinant\n\n\\[\n\\begin{align*}\n\\det(\\mathbf{C}) &=\n\\begin{vmatrix}\n\\mathbf{C}\n\\end{vmatrix}\\\\\n\\begin{vmatrix}\n1&2\\\\\n3&4\n\\end{vmatrix}\n&= -2\n\\end{align*}\n\\]\ndet_mx_C = la.det(mx_C)"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html",
    "href": "posts/transcriptome_qgt_lab/index.html",
    "title": "Transcriptome QGT lab",
    "section": "",
    "text": "Predict whole blood expression\nCheck how well the prediction works with GEUVADIS expression data\nRun association between predicted expression and a phenotype\nCalculate association between expression levels and coronary artery disease risk using s-predixcan\nFine-map the coronary artery disease gwas results using torus\nCalculate colocalization probability using fastenloc\n(skip) Run transcriptome-wide mendelian randomization in one locus of interest\nRun cTWAS (fine-map SNPs and genes jointly)"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#questionnaire-01",
    "href": "posts/transcriptome_qgt_lab/index.html#questionnaire-01",
    "title": "Transcriptome QGT lab",
    "section": "Questionnaire 01",
    "text": "Questionnaire 01\n\nOpen and start filling questionnaire 01 Preliminary questionnaire https://forms.gle/fhNJAyjx7MJTy3yt8\nInstall packages as needed\n\n\n# List of packages you want to install\npackages &lt;- c(\"tidyverse\", \"data.table\", \"BEDMatrix\", \"Rfast\", \"susieR\", \"coloc\")\n\n# Function to check and install any missing packages\ncheck_and_install &lt;- function(pkg){\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg, dependencies = TRUE)\n    library(pkg, character.only = TRUE)\n  }\n}\n\n# Use the function to check and install packages\nsapply(packages, check_and_install)\n\n\nLoad Rstudio Libraries\n\n\nlibrary(tidyverse)\n\n## packages needed for susie+coloc\nlibrary(data.table)\nlibrary(BEDMatrix)\nlibrary(Rfast)\nlibrary(susieR)\nlibrary(coloc)\n##library(tidyverse)\nlibrary(R.utils)\n\n\nNavigate to starting directory\n\n\ncd \"/cloud/project/\"\n\n\nactivate the the imlabtools environment, which will make sure that the right version of python modules are available\n\n\nconda activate imlabtools\n\n\nTo define some variables to access the data more easily within the R session, run the following r chunk\n\n\nprint(getwd())\n\nlab=\"/cloud/project/QGT-Columbia-HKI-repo/\"\nCODE=glue::glue(\"{lab}/code\")\nsource(glue::glue(\"{CODE}/load_data_functions.R\"))\nsource(glue::glue(\"{CODE}/plotting_utils_functions.R\"))\n\nPRE=\"/cloud/project/QGT-Columbia-HKI-repo/box_files\"\nMODEL=glue::glue(\"{PRE}/models\")\nDATA=glue::glue(\"{PRE}/data\")\nRESULTS=glue::glue(\"{PRE}/results\")\nMETAXCAN=glue::glue(\"{PRE}/repos/MetaXcan-master/software\")\nFASTENLOC=glue::glue(\"{PRE}/repos/fastenloc-master\")\n\n# This is a reference table we'll use a lot throughout the lab. It contains information about the genes.\ngencode_df = load_gencode_df()\n\n\ncheck the values of the variables you just defined in R\n\n\nMODEL\n\nDATA\n\n\ndefine some variables to access the data more easily in the terminal. Remember we are running R code in the R console and command line code in the terminal.\n\n\nexport PRE=\"/cloud/project/QGT-Columbia-HKI-repo/box_files\"\nexport LAB=\"/cloud/project/QGT-Columbia-HKI-repo/\"\nexport CODE=$LAB/code\nexport DATA=$PRE/data\nexport MODEL=$PRE/models\nexport RESULTS=$PRE/results\nexport METAXCAN=$PRE/repos/MetaXcan-master/software\n\n\ncheck the values of the variables you just defined\n\n\n\necho $CODE\necho $RESULTS"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#questionnaire-02",
    "href": "posts/transcriptome_qgt_lab/index.html#questionnaire-02",
    "title": "Transcriptome QGT lab",
    "section": "Questionnaire 02",
    "text": "Questionnaire 02\n\nOpen and start filling questionnaire 02 Prediction https://forms.gle/T6kAHvFTxYfcQguW7"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#optional-assess-actual-prediction-performance",
    "href": "posts/transcriptome_qgt_lab/index.html#optional-assess-actual-prediction-performance",
    "title": "Transcriptome QGT lab",
    "section": "(Optional) Assess Actual Prediction Performance",
    "text": "(Optional) Assess Actual Prediction Performance\n\n## download and read observed expression data from GEUVADIS \n## from https://uchicago.box.com/s/4y7xle5l0pnq9d1fwmthe2ewhogrnlrv\n\nobs_exp&lt;- read_csv(glue::glue(\"{DATA}/predixcan/GEUVADIS.observed_df.csv.gz\"))\n\n## Note that the version of the ensemble id of the gene was removed\nhead(predicted_expression)\n\n## Q: how many genes were predicted?\nlength(unique(predicted_expression$gene_id))\n\n## inner join predicted expression with observed expression data (by IID and gene)\n## common errors occur when ensemble id's have versions in one set and not the other set\nfullset=inner_join(predicted_expression, obs_exp, by = c(\"gene_id\",\"IID\"))\n\n## calculate spearman correlation for all genes\ngenelist = unique(predicted_expression$gene_id)\ncorvec = rep(NA,length(genelist))\nnames(corvec) = genelist\nfor(gg in 1:length(genelist))\n{\n  ind = fullset$gene_id==genelist[gg]\n  corvec[gg] = cor(fullset$predicted_expression[ind], fullset$observed_expression[ind])\n}\n\n## what's the best performing gene?\n\n## plot the histogram of the prediction performance\nhist(corvec)\n\n## list the top 10 best performing genes\nhead(sort(corvec,decreasing = TRUE),2)\ntail(sort(corvec,decreasing = TRUE),2)\n\n## plot the correlation of the top 2 best performing genes bottom 2\n\ngeneid = \"ENSG00000100376\"\ngenename = gencode_df %&gt;% filter(gene_id==geneid) %&gt;% .[[\"gene_name\"]]\nfullset %&gt;% filter(gene_id==geneid) %&gt;% ggplot(aes(observed_expression, predicted_expression))+geom_point()+ggtitle(paste(genename, \"-\",geneid))\n\ngeneid = \"ENSG00000075234\"\ngenename = gencode_df %&gt;% filter(gene_id==geneid) %&gt;% .[[\"gene_name\"]]\nfullset %&gt;% filter(gene_id==geneid) %&gt;% ggplot(aes(observed_expression, predicted_expression))+geom_point()+ggtitle(paste(genename, \"-\", geneid))\n\ngeneid = \"ENSG00000070371\"\ngenename = gencode_df %&gt;% filter(gene_id==geneid) %&gt;% .[[\"gene_name\"]]\nfullset %&gt;% filter(gene_id==geneid) %&gt;% ggplot(aes(observed_expression, predicted_expression))+geom_point()+ggtitle(paste(genename, \"-\", geneid))\n\ngeneid = \"ENSG00000184164\"\ngenename = gencode_df %&gt;% filter(gene_id==geneid) %&gt;% .[[\"gene_name\"]]\nfullset %&gt;% filter(gene_id==geneid) %&gt;% ggplot(aes(observed_expression, predicted_expression))+geom_point()+ggtitle(paste(genename, \"-\", geneid))"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#questionnaire-03",
    "href": "posts/transcriptome_qgt_lab/index.html#questionnaire-03",
    "title": "Transcriptome QGT lab",
    "section": "Questionnaire 03",
    "text": "Questionnaire 03\n\nOpen and start filling questionnaire 03 PrediXcan https://forms.gle/3H319knWbLgnynNs9\n\nWe are going to use a simulated phenotype for which only UPK3A has an effect on the phenotype (\\(\\beta=-0.9887378\\))\n\\(Y = \\sum_k T_k \\beta_k + \\epsilon\\)\nwith random effects \\(\\beta_k \\sim (1-\\pi)\\cdot \\delta_0 + \\pi\\cdot N(0,1)\\)\nHere we are running associations between phenotype and predicted expression.\n\n\nexport PHENO=\"sim.spike_n_slab_0.01_pve0.1\"\n\nprintf \"association\\n\\n\"\npython3 $METAXCAN/PrediXcanAssociation.py \\\n--expression_file $RESULTS/predixcan/Whole_Blood__predict.txt \\\n--input_phenos_file $DATA/predixcan/phenotype/$PHENO.txt \\\n--input_phenos_column pheno \\\n--output $RESULTS/predixcan/$PHENO/Whole_Blood__association.txt \\\n--verbosity 9 \\\n--throw\n\nMore predicted phenotypes can be found in $DATA/predixcan/phenotype/. The naming of the phenotypes provides information about the genetic architecture: the number after pve is the proportion of variance of Y explained by the genetic component of expression. The number after spike_n_slab represents the probability that a gene is causal π (i.e. prob β≠0)"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#looking-at-association-results",
    "href": "posts/transcriptome_qgt_lab/index.html#looking-at-association-results",
    "title": "Transcriptome QGT lab",
    "section": "Looking at Association Results",
    "text": "Looking at Association Results\n\n## read association results\nPHENO=\"sim.spike_n_slab_0.01_pve0.1\"\n\npredixcan_association = load_predixcan_association(glue::glue(\"{RESULTS}/predixcan/{PHENO}/Whole_Blood__association.txt\"), gencode_df)\n\n## take a look at the results\ndim(predixcan_association)\npredixcan_association %&gt;% arrange(pvalue) %&gt;% select(gene_name,effect,se,pvalue,gene) %&gt;% head\npredixcan_association %&gt;% arrange(pvalue) %&gt;% ggplot(aes(pvalue)) + geom_histogram(bins=10)\n## compare distribution against the null (uniform)\ngg_qqplot(predixcan_association$pvalue, max_yval = 40)\n\nQ3.6) How many genes are significantly associated with the simulated phenotype? (count the genes with p-value&lt;0.05/total number of tests)\n\nlength(unique(predixcan_association$gene[which(predixcan_association$pvalue&lt;0.05/230)]))\n#7"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#comparing-the-estimated-effect-size-with-true-effect-size",
    "href": "posts/transcriptome_qgt_lab/index.html#comparing-the-estimated-effect-size-with-true-effect-size",
    "title": "Transcriptome QGT lab",
    "section": "Comparing the Estimated Effect Size with True Effect Size",
    "text": "Comparing the Estimated Effect Size with True Effect Size\n\ntruebetas = load_truebetas(glue::glue(\"{DATA}/predixcan/phenotype/gene-effects/{PHENO}.txt\"), gencode_df)\nbetas = (predixcan_association %&gt;% \n               inner_join(truebetas,by=c(\"gene\"=\"gene_id\")) %&gt;%\n               select(c('estimated_beta'='effect', \n                        'true_beta'='effect_size',\n                        'pvalue', \n                        'gene_id'='gene', \n                        'gene_name'='gene_name.x', \n                        'region_id'='region_id.x')))\nbetas %&gt;% arrange(pvalue) %&gt;% select(gene_name,estimated_beta,true_beta,pvalue) %&gt;% head\n## do you see examples of potential LD contamination?\nbetas %&gt;% mutate(causal= true_beta!=0) %&gt;% ggplot(aes(estimated_beta, true_beta,col=causal))+geom_point(alpha=0.6,size=5)+geom_abline()+theme_bw()\n\nThere are lots of large values that are just noise (looking at estimated and true values).\nQ3.7) How many genes have non-zero true effect sizes?\n\nsum(betas$true_beta!=0)\n#1\n\nQ3.8) How many estimated effect sizes are different from 0?\n\nsum(betas$estimated_beta!=0)\n#230\n\nQ3.9) Which gene is the most likely causal gene?\n\nbetas$gene_name[which(betas$pvalue==min(betas$pvalue))]\n#UPK3A\n\nQ3.10) How many genes have effects that are significantly (Bonferroni corrected) different from zero?\n\nlength(unique(betas$gene_id[which(betas$estimated_beta!=0&betas$pvalue&lt; (0.05/230))]))\n#7\n\n^After this, can further reduce # suspected effects using colocalization.\n\nUPK3A is the causal gene and has the most significant pvalue. RIBC2 is also significantly associated but has no causal role (we know because we simulated the phenotype that way). Why?\n\nHint: correlation between the genes"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#questionnaire-04",
    "href": "posts/transcriptome_qgt_lab/index.html#questionnaire-04",
    "title": "Transcriptome QGT lab",
    "section": "Questionnaire 04",
    "text": "Questionnaire 04\n\nOpen and start filling questionnaire 04 S-PrediXcan https://forms.gle/xJs2U66cnrqdb5cj6"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#running-s-predixcan",
    "href": "posts/transcriptome_qgt_lab/index.html#running-s-predixcan",
    "title": "Transcriptome QGT lab",
    "section": "Running S-PrediXcan",
    "text": "Running S-PrediXcan\n\n\npython $METAXCAN/SPrediXcan.py \\\n--gwas_file  $DATA/spredixcan/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz \\\n--snp_column panel_variant_id \\\n--effect_allele_column effect_allele \\\n--non_effect_allele_column non_effect_allele \\\n--zscore_column zscore \\\n--model_db_path $MODEL/gtex_v8_mashr/mashr_Whole_Blood.db \\\n--covariance $MODEL/gtex_v8_mashr/mashr_Whole_Blood.txt.gz \\\n--keep_non_rsid \\\n--additional_output \\\n--model_db_snp_key varID \\\n--throw \\\n--output_file $RESULTS/spredixcan/eqtl/CARDIoGRAM_C4D_CAD_ADDITIVE__PM__Whole_Blood.csv\n\nQ4.2) What prediction model did you use?\n\nWe can run the full genome because the summary statistics based PrediXcan is much faster than individual level one."
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#plot-and-interpret-results",
    "href": "posts/transcriptome_qgt_lab/index.html#plot-and-interpret-results",
    "title": "Transcriptome QGT lab",
    "section": "Plot and Interpret Results",
    "text": "Plot and Interpret Results\n\nspredixcan_association = load_spredixcan_association(glue::glue(\"{RESULTS}/spredixcan/eqtl/CARDIoGRAM_C4D_CAD_ADDITIVE__PM__Whole_Blood.csv\"), gencode_df)\ndim(spredixcan_association)\nspredixcan_association %&gt;% arrange(pvalue) %&gt;% head\nspredixcan_association %&gt;% arrange(pvalue) %&gt;% ggplot(aes(pvalue)) + geom_histogram(bins=20)\n\ngg_qqplot(spredixcan_association$pvalue)\n\nQ4.3) Which gene has the smallest p-value?\n\nspredixcan_association$gene_name[which(spredixcan_association$pvalue==min(spredixcan_association$pvalue, na.rm=TRUE))]\n\n\nQuestion: SORT1, considered to be a causal gene for LDL cholesterol and as a consequence of coronary artery disease, is not found here. Why?\n\nQ4.5) Is SORT1 (ENSG00000134243) significantly associated with CAD?\n\nsum(spredixcan_association$gene==\"ENSG00000134243\")\nsum(spredixcan_association$gene_name==\"SORT1\")\n\n\ncheck whether SORT1 is expressed in whole blood GTEx portal\ncheck whether SORT1 has eQTL in whole blood GTEx portal"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#run-s-predixcan-using-gene-expression-predicted-in-liver",
    "href": "posts/transcriptome_qgt_lab/index.html#run-s-predixcan-using-gene-expression-predicted-in-liver",
    "title": "Transcriptome QGT lab",
    "section": "Run S-PrediXcan using gene expression predicted in liver",
    "text": "Run S-PrediXcan using gene expression predicted in liver\n-[] Run s-predixcan with liver model, do you find SORT1? Is it significant?\n\n#loction Liver models \n#/cloud/project/QGT-Columbia-HKI-repo/box_files/models/gtex_v8_mashr/\npython $METAXCAN/SPrediXcan.py \\\n--gwas_file  $DATA/spredixcan/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz \\\n--snp_column panel_variant_id \\\n--effect_allele_column effect_allele \\\n--non_effect_allele_column non_effect_allele \\\n--zscore_column zscore \\\n--model_db_path $MODEL/gtex_v8_mashr/mashr_Liver.db \\\n--covariance $MODEL/gtex_v8_mashr/mashr_Liver.txt.gz \\\n--keep_non_rsid \\\n--additional_output \\\n--model_db_snp_key varID \\\n--throw \\\n--output_file $RESULTS/spredixcan/eqtl/CARDIoGRAM_C4D_CAD_ADDITIVE__PM__Liver.csv\n\n\nspredixcan_association_L= load_spredixcan_association(glue::glue(\"{RESULTS}/spredixcan/eqtl/CARDIoGRAM_C4D_CAD_ADDITIVE__PM__Liver.csv\"), gencode_df)\ndim(spredixcan_association_L)\nspredixcan_association_L %&gt;% arrange(pvalue) %&gt;% head\nspredixcan_association_L %&gt;% arrange(pvalue) %&gt;% ggplot(aes(pvalue)) + geom_histogram(bins=20)\n\ngg_qqplot(spredixcan_association_L$pvalue)\ncol_order= c(\"gene_name\",\"gene\",\"zscore\",\"effect_size\",\"pvalue\",\"var_g\",\"pred_perf_r2\", \"pred_perf_pval\",\"pred_perf_qval\", \"n_snps_used\", \"n_snps_in_cov\", \"n_snps_in_model\",\"best_gwas_p\",\"largest_weight\")\nspredixcan_association_L &lt;- spredixcan_association_L[, col_order]\nfilter(spredixcan_association_L, gene_name==\"SORT1\")\n\nQ4.5) Is SORT1 (ENSG00000134243) significantly associated with CAD?\n\nsum(spredixcan_association_L$gene==\"ENSG00000134243\")\nspredixcan_association_L$pvalue[which(spredixcan_association_L$gene_name==\"SORT1\")]\n#p value = 4.629983e-23\n\n4.629983e-23 &lt; 0.05/length(unique(spredixcan_association_L$gene_name))"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#optional-compare-zscores-in-liver-and-whole-blood.",
    "href": "posts/transcriptome_qgt_lab/index.html#optional-compare-zscores-in-liver-and-whole-blood.",
    "title": "Transcriptome QGT lab",
    "section": "(Optional) Compare zscores in liver and whole blood.",
    "text": "(Optional) Compare zscores in liver and whole blood.\n\nRecall that zscore is the effect size divided by the standard error\n\n\nspredixcan_association_L=rename(spredixcan_association_L, zscore_liver = \"zscore\")\nhead(spredixcan_association_L)\ntest=left_join(spredixcan_association, spredixcan_association_L, by=\"gene_name\")\ntest=select(test,\"gene_name\",\"zscore\",\"zscore_liver\")\ntest %&gt;% arrange(zscore_liver) %&gt;% head\n\ntest %&gt;% mutate(zscore_WB=zscore) %&gt;% ggplot(aes(zscore_WB,zscore_liver)) + geom_point(size=3,alpha=.6) + geom_abline()\n\n## S-PrediXcan association in liver and whole blood are significantly correlated\ncor.test(test$zscore,test$zscore_liver)\n\nQ4.6) In which region of the qqplot are Bonferroni significant genes located?\nAbove the horizontal line\n\ngg_qqplot(spredixcan_association_L$pvalue[which(spredixcan_association_L$pvalue&lt;0.05/230)]) #graphing only significant genes\n\n\ngg_qqplot(spredixcan_association_L$pvalue)"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#optional-run-multixcan",
    "href": "posts/transcriptome_qgt_lab/index.html#optional-run-multixcan",
    "title": "Transcriptome QGT lab",
    "section": "(Optional) Run MultiXcan",
    "text": "(Optional) Run MultiXcan\n\nmultixcan aggregates information across multiple tissues to boost the power to detect association. It was developed movivated by the fact that eQTLs are shared across multiple tissues, i.e. many genetic variants that regulate expression are common across tissues.\nbefore you run multixcan ensure you have run s-predixcan for all the tissues you want to multixcan. In this tutorial we have two tissues (liver and whole blood), ensure you have run s-predixcan with the two tissues before running multixcan.\nOne thing to note is to ensure similar naming pattern for the output files. This is to ensure the files are captured correctly when running multixcan’s filter.\n\n\n\npython $METAXCAN/SMulTiXcan.py \\\n--models_folder $MODEL/gtex_v8_mashr \\\n--models_name_pattern \"mashr_(.*).db\" \\\n--snp_covariance $MODEL/gtex_v8_expression_mashr_snp_smultixcan_covariance.txt.gz \\\n--metaxcan_folder $RESULTS/spredixcan/eqtl/ \\\n--metaxcan_filter \"CARDIoGRAM_C4D_CAD_ADDITIVE__PM__(.*).csv\" \\\n--metaxcan_file_name_parse_pattern \"(.*)__PM__(.*).csv\" \\\n--gwas_file $DATA/spredixcan/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz \\\n--snp_column panel_variant_id --effect_allele_column effect_allele --non_effect_allele_column non_effect_allele --zscore_column zscore --keep_non_rsid --model_db_snp_key varID \\\n--cutoff_condition_number 30 \\\n--verbosity 9 \\\n--throw \\\n--output $RESULTS/smultixcan/eqtl/CARDIoGRAM_C4D_CAD_ADDITIVE_smultixcan.txt"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#questionnaire-5",
    "href": "posts/transcriptome_qgt_lab/index.html#questionnaire-5",
    "title": "Transcriptome QGT lab",
    "section": "Questionnaire 5",
    "text": "Questionnaire 5\n\nOpen and start filling questionnaire 05 Colocalization https://forms.gle/NfH2MSdy4UyJzGAp7"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#run-colocalization",
    "href": "posts/transcriptome_qgt_lab/index.html#run-colocalization",
    "title": "Transcriptome QGT lab",
    "section": "Run colocalization",
    "text": "Run colocalization\nWhen you use coloc on your own data, you may want to check out coloc’s documentation, with good advice and tips on avoiding common mistakes https://cran.r-project.org/web/packages/coloc/vignettes\nDue to time constraints, we will run one region and one gene only\n\nFinemap GWAS of CAD\nFinemap eQTL of SORT1\n\nFor finemapping, we need the summary statistics (effect size, standard errors, etc) and the correlation between SNPs (LD matrix)"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#load-the-genotype-to-calculate-the-ld-matrix",
    "href": "posts/transcriptome_qgt_lab/index.html#load-the-genotype-to-calculate-the-ld-matrix",
    "title": "Transcriptome QGT lab",
    "section": "load the genotype to calculate the ld matrix",
    "text": "load the genotype to calculate the ld matrix\n\n# load the genotype to calculate the ld matrix\nX_mat &lt;- BEDMatrix(glue::glue(\"{DATA}/colocalization/geuvadis_chr1\"))\ncolnames(X_mat) &lt;- gsub(\"\\\\_.*\", \"\",colnames(X_mat))\ncolnames(X_mat) &lt;- str_replace_all(colnames(X_mat),\":\",\"_\")\nsnp_info &lt;- fread(glue::glue(\"{DATA}/colocalization/geuvadis_chr1.bim\")) %&gt;% \n  setnames(., colnames(.), c(\"chr\", \"snp\", \"cm\", \"pos\", \"alt\", \"ref\")) \n\nsnp_info$snp &lt;- str_replace_all(snp_info$snp,\":\",\"_\")"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#load-the-eqtl-and-gwas-effect-size-files",
    "href": "posts/transcriptome_qgt_lab/index.html#load-the-eqtl-and-gwas-effect-size-files",
    "title": "Transcriptome QGT lab",
    "section": "Load the eqtl and gwas effect size files",
    "text": "Load the eqtl and gwas effect size files\n\n# load the eqtl effect sizes\ngene_ss &lt;- fread(glue::glue(\"{DATA}/colocalization/Liver_chr1.txt\"))\n\n# load gwas effect sizes\ngwas &lt;- data.table::fread(glue::glue(\"{DATA}/spredixcan/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz\"))\n\n# filter to select genome wide significant snps at 5 × 10−8\nfiltered_regions &lt;- gwas %&gt;% dplyr::filter(pvalue &lt; 5e-8)\n\n# load the ld block - attempt to split genome into independent blocks\nldblocks &lt;-read_tsv(glue::glue(\"{DATA}/spredixcan/eur_ld.hg38.txt.gz\"))"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#find-regions-with-the-strongest-signal-in-the-gwas",
    "href": "posts/transcriptome_qgt_lab/index.html#find-regions-with-the-strongest-signal-in-the-gwas",
    "title": "Transcriptome QGT lab",
    "section": "Find regions with the strongest signal in the gwas",
    "text": "Find regions with the strongest signal in the gwas\n\n# get the loci where the significant snps are located\nfor (n in 1:nrow(filtered_regions)){\n  # extract genename, start and end\n  variant_id &lt;- as.character(filtered_regions[n,\"variant_id\"])\n  variant_chr &lt;- as.character(filtered_regions[n,\"chromosome\"])\n  variant_pos &lt;- as.numeric(filtered_regions[n,\"position\"])\n  #gene_end &lt;- as.numeric(genes[n,\"end\"])\n\n  locus &lt;- ldblocks %&gt;%\n    dplyr::filter(chr == variant_chr) %&gt;%\n    filter(variant_pos &gt;= start & variant_pos &lt; stop) %&gt;%\n    mutate(locus_name = paste0(chr,\"_\",start,\"_\",stop)) %&gt;%\n    dplyr::rename(locus_start=start,locus_end=stop) %&gt;%\n    mutate(variant_id = variant_id, position=variant_pos)\n\n  # create a data frame with info\n  if (exists('all_loci') && is.data.frame(get('all_loci'))) {\n    all_loci &lt;- rbind(all_loci,locus)\n  } else {\n    all_loci &lt;- locus\n  }\n}\n\n# select uniq loci\nd_loci &lt;- all_loci %&gt;%\n  dplyr::select(locus_name,chr,locus_start,locus_end) %&gt;%\n  dplyr::distinct()"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#select-a-region-to-run-coloc",
    "href": "posts/transcriptome_qgt_lab/index.html#select-a-region-to-run-coloc",
    "title": "Transcriptome QGT lab",
    "section": "select a region to run coloc",
    "text": "select a region to run coloc\n\n# select regions to fine map. we are going to use regions in chromosome 1\nuniq_loci &lt;- d_loci %&gt;% dplyr::filter(\"chr1\" == chr)\nn = 3 # chr1_107867043_109761309 region\n\n# extract information\nl_chr = as.numeric(str_remove(uniq_loci[n,\"chr\"],\"chr\"))\ns_chr = uniq_loci[n,]$chr\nl_start = uniq_loci[n,]$locus_start\nl_stop = uniq_loci[n,]$locus_end\nl_name = uniq_loci[n,]$locus_name"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#prepare-gwas-data-for-coloc",
    "href": "posts/transcriptome_qgt_lab/index.html#prepare-gwas-data-for-coloc",
    "title": "Transcriptome QGT lab",
    "section": "Prepare gwas data for coloc",
    "text": "Prepare gwas data for coloc\n\n# select snps for the region from the summary stats\nss &lt;- gwas %&gt;% \n  dplyr::filter(chromosome == s_chr) %&gt;%\n  dplyr::filter(position &gt;= l_start & position &lt;= l_stop) %&gt;% \n  dplyr::filter(! is.na(effect_size))\n\n# find the snps in the genotype to calculate the correlation\ng.snps &lt;- ss %&gt;% inner_join(snp_info %&gt;% mutate(chr = glue::glue(\"chr{chr}\")), \n                            by=c(\"chromosome\" = \"chr\",\"panel_variant_id\" = \"snp\"))\n\n\n# select genotype to calculate correlation\n#f_mat &lt;- X_mat[,g.snps$snp]\nf_mat &lt;- X_mat[,g.snps$panel_variant_id]\n\n# calculate corr\nR = cora(f_mat) # the package is for speed\n\n## clean up\nrm(f_mat)\n\nff &lt;- g.snps %&gt;% dplyr::filter(! is.na(effect_size)) %&gt;% #select(-snp) %&gt;% \n  dplyr::rename(snp=panel_variant_id,beta=effect_size) %&gt;% \n  mutate(varbeta = standard_error^2) %&gt;% \n  dplyr::select(beta,varbeta,snp,position) %&gt;% as.list()\n\nff$type &lt;- \"cc\"\nff$sdY &lt;- 1\n\nff$LD = R\nff$N = 184305\n\n## check the data (NULL means it's fine)\ncheck_dataset(ff)"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#prepare-eqtl-data-for-coloc",
    "href": "posts/transcriptome_qgt_lab/index.html#prepare-eqtl-data-for-coloc",
    "title": "Transcriptome QGT lab",
    "section": "Prepare eqtl data for coloc",
    "text": "Prepare eqtl data for coloc\n\n#Using SORT1 gene and liver tissue\ngene &lt;- gene_ss %&gt;% dplyr::filter(gene_id == \"ENSG00000134243.11\") %&gt;% \n  dplyr::rename(snp = variant_id,beta = slope, MAF = maf,\n                pvalue = pval_nominal) %&gt;% \n  mutate(varbeta = slope_se^2, name = snp) %&gt;% \n  filter(! is.na(varbeta)) %&gt;% \n  separate(name, into = c(\"chr\", \"position\",\"ref\",\"alt\",\"build\"),sep = \"_\")\n\n## calculate the ld matrix\n### get the snps\ngene.snps &lt;- gene %&gt;% mutate(position = as.integer(position)) %&gt;% \n  inner_join(snp_info %&gt;% mutate(chr = glue::glue(\"chr{chr}\")), \n                              by=c(\"chr\" = \"chr\",\"snp\" = \"snp\"))\n\n# select genotype to calculate correlation\ng_mat &lt;- X_mat[,gene.snps$snp]\n# calculate corr\ng.R = cora(g_mat)\n\n# clean up\nrm(g_mat)\n\n# format data for coloc\ngg &lt;- gene %&gt;% dplyr::filter(snp %in% gene.snps$snp) %&gt;%\n  mutate(position = as.integer(position)) %&gt;% \n  dplyr::select(beta,varbeta,snp,position,MAF, pvalue) %&gt;% as.list()\n\ngg$type &lt;- \"quant\"\ngg$LD = g.R\ngg$N = 208 # 670 for blood\n\n## check the data\ncheck_dataset(gg)"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#run-older-version-of-coloc-which-assumes-single-causal-variant",
    "href": "posts/transcriptome_qgt_lab/index.html#run-older-version-of-coloc-which-assumes-single-causal-variant",
    "title": "Transcriptome QGT lab",
    "section": "run older version of coloc which assumes single causal variant",
    "text": "run older version of coloc which assumes single causal variant\ncoloc.abf makes the simplifying assumption that each trait has at most one causal variant in the region under consideration\n\nmy.res &lt;- coloc.abf(dataset1=ff, dataset2=gg)\nsensitivity(my.res,\"H4 &gt; 0.9\")"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#run-coloc-allowing-multiple-causal-variants",
    "href": "posts/transcriptome_qgt_lab/index.html#run-coloc-allowing-multiple-causal-variants",
    "title": "Transcriptome QGT lab",
    "section": "run coloc allowing multiple causal variants",
    "text": "run coloc allowing multiple causal variants\nMultiple causal variants, using SuSiE to separate the signals\n\n# Run susie fine mapping\nS3 = runsusie(ff)\nS4 = runsusie(gg)\n#summary(S3)\n\n# Run coloc\nsusie.res=coloc.susie(S3,S4)\nprint(susie.res$summary)\n\nSuSiE can take a while to run on larger datasets, so it is best to run once per dataset with the =runsusie= function, store the results and feed those into subsequent analyses.\nplot the coloc result with the sensitivity function because weird effects are much easier to understand visually\n\nsensitivity(susie.res,\"H4 &gt; 0.9\",row=1,dataset1=ff,dataset2=gg,)\n\nQ5.1) What is fine-mapping of GWAS loci? An estimation of the probability that a variant is causal.\nQ5.2) What does PIP stand for? Posterior inclusion probability.\nQ5.3) What is the interpretation of PIP? The posterior probability that a variant is causal.\nQ5.4) What is the interpretation PIP? The posterior probability that a variant is causal.\nQ5.5) Relationship between p-value and PIP Typically, a causal variant will have a small p-value and a large PIP, so they will be somewhat correlated, but many variants with small p-values may have zero PIP.\nQ5.6) How do colocalization methods work? Colocalization tries to estimate the probability that the causal variants of the expression trait is the same as the causal variant of the complex trait.\nQ5.7) How many SNPs are used for finemapping the gwas?\n\nlength(ff$snp)\n#3853\n\nQ5.8) How many SNPs are used for finemapping the eqtl?\n\nlength(gg$snp)\n#5628\n\nQ5.9) How many SNPs were used to run coloc? Hint: overlap between gwas and eqtl SNPs\n\nlength(intersect(gg$snp,ff$snp))\n#2523"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#questionnaire-06",
    "href": "posts/transcriptome_qgt_lab/index.html#questionnaire-06",
    "title": "Transcriptome QGT lab",
    "section": "Questionnaire 06",
    "text": "Questionnaire 06\n\nOpen and start filling out questionnaire 06 cTWAS https://forms.gle/A4evWkbhR7cXLy36A\n\n\n#install.packages(\"R.utils\")\n\n#install.packages(\"remotes\")\n#remotes::install_github(\"simingz/ctwas\", ref = \"develop\")\n\nlibrary(ctwas)\n\n#get positions for region of interest (SORT1/PSRC1 locus)\nregion &lt;- unlist(strsplit(spredixcan_association$region_id[spredixcan_association$gene_name==\"PSRC1\"], \"_\"))\nchr &lt;- region[2]\nstart &lt;- as.numeric(region[3])\nend &lt;- as.numeric(region[4])\n\n#format summary statistics (and subset to variants in region to save memory)\nz_snp &lt;- data.table::fread(glue::glue(\"{DATA}/spredixcan/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz\"), select=c(\"chromosome\", \"position\", \"variant_id\", \"effect_allele\", \"non_effect_allele\", \"zscore\", \"sample_size\"))\nz_snp &lt;- z_snp[z_snp$chromosome==chr & z_snp$position &gt;= start & z_snp$position &lt;= end,]\nz_snp &lt;- z_snp[!is.na(z_snp$variant_id),-(1:2)]\ncolnames(z_snp) &lt;- c(\"id\", \"A1\", \"A2\", \"z\", \"ss\")\n\n#specify directories for LD matrices and weights\nld_R_dir &lt;- glue::glue(\"{DATA}/cTWAS/LD_matrices\")\nweight &lt;-  glue::glue(\"{MODEL}/gtex_v8_mashr/mashr_Liver.db\")\n\n#specify output locations and names for cTWAS\noutputdir &lt;- glue::glue(\"{DATA}/cTWAS/results/\")\noutname.e &lt;- \"CARDIoGRAM_Liver_expr\"\noutname &lt;- \"CARDIoGRAM_Liver_ctwas\"\n\n#impute gene z scores using cTWAS and save the results\n##################################\n# NOTE: we are skipping this step and using the precomputed values \n## takes ~10 minutes\n##################################\n# ctwas_imputation &lt;- impute_expr_z(z_snp=z_snp, weight=weight, ld_R_dir=ld_R_dir, outputdir=outputdir, outname=outname.e, harmonize_z=F, harmonize_wgt=F)\n# save(ctwas_imputation, file = paste0(outputdir, outname.e, \"_output.Rd\"))\nload(paste0(outputdir, outname.e, \"_output.Rd\"))\n\nz_gene &lt;- ctwas_imputation$z_gene\nld_exprfs &lt;- ctwas_imputation$ld_exprfs\nz_snp &lt;- ctwas_imputation$z_snp\n\n\n#make custom region file for single region\nld_regions_custom &lt;- data.frame(\"chr\" = chr, \"start\" = start, \"stop\" = end)\n\nwrite.table(ld_regions_custom, \n            file= paste0(outputdir, \"ld_regions_custom.txt\"),\n            row.names=F, col.names=T, sep=\"\\t\", quote = F)\n    \nld_regions_custom &lt;- paste0(outputdir, \"ld_regions_custom.txt\")\n\n#run cTWAS with pre-specified prior parameters at a single locus\n#estimating prior requires genome-wide data, too slow for demonstration\n#prior is 1% inclusion for genes and is 100x more likely than SNPs\n\n#if SNPs and genes have similar evidence from the data, we prioritize the gene\n\n#prior assumes genes have larger effect size than SNPs, in reasonable range for data we've looked at\n\n\n#jointly fine mapping all the genes and all the SNPs in this region\n#ctwas_rss(z_gene=z_gene, z_snp=z_snp, ld_exprfs=ld_exprfs, ld_R_dir = ld_R_dir, ld_regions_custom=ld_regions_custom, outputdir = outputdir, outname = outname, thin = 0.01,\n          #estimate_group_prior = F,\n          #estimate_group_prior_var = F,\n          #group_prior=c(0.01, 0.0001),\n          #group_prior_var=c(50, 25))\n\n#load results\nctwas_results &lt;- data.table::fread(paste0(outputdir,outname,\".susieIrss.txt\"))\n\nQ6.1) What’s the posterior inclusion probability of SORT1?\n\nctwas_results$susie_pip[which(ctwas_results$genename==\"SORT1\")]\n#0.999728\n\nQ6.2) How many SNPs are used to run cTWAS?\n\nctwas_results$\n\n\n#merge gene names into the results\nsqlite &lt;- RSQLite::dbDriver(\"SQLite\")\ndb = RSQLite::dbConnect(sqlite, weight)\nquery &lt;- function(...) RSQLite::dbGetQuery(db, ...)\nextra_table &lt;- query(\"select * from extra\")\nRSQLite::dbDisconnect(db)\n\nctwas_results$genename &lt;- extra_table$genename[match(ctwas_results$id, extra_table$gene)]\n\n#show results with highest PIPs\ncol_order_2= c(\"genename\", \"chrom\", \"id\", \"pos\", \"type\", \"region_tag1\", \"region_tag2\", \"cs_index\", \"susie_pip\" , \"mu2\")\nctwas_results &lt;- ctwas_results[, ..col_order_2]\nhead(ctwas_results[order(-ctwas_results$susie_pip),])\n\nQ6.2) How many SNPs are used to run cTWAS?\n\nsum(ctwas_results$type==\"SNP\")\n\nQ6.3) How many genes/transcripts have non zero probability of being causal (pip)?\n\nsum(ctwas_results$type==\"gene\"&ctwas_results$susie_pip!=0)\n\nQ6.4) How many snps have non zero pip?\n\nsum(ctwas_results$type==\"SNP\"&ctwas_results$susie_pip!=0)\n\nQ6.5) What is the most likely causal gene?\n\nctwas_results$genename[which(ctwas_results$susie_pip==max(ctwas_results$susie_pip))]\n\nDifference between genes and SNPs: in the prior When you define prior, all SNPs are non-informative prior (all equally likely to be causal) \\(\\to\\) 1/# SNPs\nall SNPS of a region are equally likely (1/# genes)\nbased on biological knowledge we can give higher prior probability to genes rather than SNPs\n(Still kind of arbitrary though)"
  },
  {
    "objectID": "posts/llm_summary/index.html",
    "href": "posts/llm_summary/index.html",
    "title": "LLM in molecular biology summary",
    "section": "",
    "text": "LLM in molecular biology summary\narticle link\nLarge language models\n\nLLM: a type of neural network that acquires the ability to generate text mirroring human language by scrutinizing vast amounts of textual data\n\nself-supervised — model learns to predict subsequent word in a sentence based on preceding words\ncan identify patterns and try to predict (i.e. advanced form of autocomplete)\n\n\nPrimary types of language models and their unique features:\n\nword grams: predict next word in a sentence based on frequency of word pairs/word bags (sets of words) — disregard context or word order (generates text that bear little resemblance to human text)\nCNNs: analyze text data by considering relationships between adjacent words in a fixed window; good at identifying local patterns, but fall short in capturing long-range dependencies or comprehending complex sentence structures\nLSTMs (long short-term memory networks): variant of RNNs; store+process information from earlier parts of a text; outperform CNNs in understanding context/managing long-range dependencies, but falter w/ complex sentences+long text\nattention mechanisms: enable models to concentrate on pertinent parts of input when making predictions; number of attention “heads” allow model to focus on different parts of previous text when predicting the next word\n\nlike revisiting key points/details; model refers back to relevant parts of text+incorporates info into current context\nex. transformers are a class of language models that implement attention mechanisms\n\nLLMs: models such as GPT-3 are transformers that leverage attention mechanisms+are trained on vast amounts of data; considerable size facilitates the learning of intricate patterns/relationships/context within text; represent the most advanced language models presently available, capable of generating more accurate+coherent responses across a broad spectrum of topics\n\n2 LLMs that use transformer architecture: BERT+GPT series\nBERT (bidirectional encoder representations from transformers): a series of LLMs by Google+open sourced\n\ntrained using masked language modeling (hide/“mask” some percentage of input tokens at random, then predict those masked tokens)\n\nforces model to understand context from both left/right sides of input (bidirectional)\n\nalso uses next sentence prediction task\nduring training, model is given pairs of sentences+has to predict whether second sentence in pair is the next sentence in the original document\n\nGPT (generative pretrained transformer): series of LLMs introduced by OpenAI\n\ntrained using traditional language modeling task of autocomplete (predict next word in sentence)\nonly attends to left context (previous tokens) during training (unidirectional)\ngenerative model that is stronger in tasks involving text generation\n\nThe genetic dogma\n\nbiological trajectory of a human or any other organism is a complex interplay between genetics+environment (DNA and environment individual is exposed to) aka genotype-phenotype-environment\ncentral dogma of molec bio describes flow of genetic info within living organisms\n\nsource of genetic info: our DNA (exact replica of which is harbored in nucleus of every cell in our body)\n\neach individual possesses 2 nearly identical copies of human genome (one from mom, one from dad)\n\n\nhuman chromosome structure: chromatin tightly packed in hierarchical coil structures. from bottom, 146 nucleotide pairs wrapped around histone (like a bead), and histones are coiled and supercoiled to form compact chromosome that fits within the nucleus of a cell\nwithin genome: ~20,000 genes (DNA segments accountable for protein synthesis)\n~1% of genome codes for proteins while remainder comprises regions controlling gene expression, regions within genes that don’t code for proteins, regions contributing to DNA structure, and “junk” regions of selfish DNA that have “learned” to self-replicate\ncentral dogma of molec bio maps out molec info flow from genome → expression of genes+subsequent production of proteins (building blocks of life)\ngenes expressed within cells by transcription (copies genes into single-stranded molecule mRNA) and translation (mRNA → amino acid protein sequence); 4-letter nucleotide code of DNA segment translated into 20-amino acid code of protein sequence; protein folds in 3d to form functional protein structure\ntranscription → splicing → translation\nsplicing: excised segments aka introns; kept regions aka exons make up protein-coding part of mRNA\n\neach mature mRNA assembled from ~7 exons\nvital in higher organisms b/c a single gene can yield multiple different proteins by assembling different exon combinations during splicing\n\n20,000 genes → 70,000 known standard splice forms+larger # rare/aberrant splice forms\nafter transcription, mRNA transported to cell’s protein-synthesizing machinery (ribosome) where translation occurs\n\nmRNA sequence decoded by codons (each corresponds to 1/20 amino acids)\n\namino acids linked together in a chain to form protein sequence → folds into functional, 3d protein structure\ngene regulation: intricate processes that dictate when/where/in what quantity genes are expressed within cell, ensuring timely production of the right proteins in the right amounts\n\ngene regulation takes place at various levels (structuring of chromatin, chem modifications, through action of specific proteins known as transcription factors) that recruit RNA polymerase and/control when/where/what amount gene will be expressed (requires open chromatin for transcription)\n\nTranscription factors (TFs): proteins in gene regulation that bind to distinct DNA sequences near/within genes (transcription factor binding sites) and influence recruitment of RNA polymerase, the enzyme tasked w/ mRNA synthesis\n\ntranscription factors modulate expression of target genes, guaranteeing appropriate gene expression in response to diverse cellular signals/environmental conditions\nTFs themselves modulated by TFs, forming complex gene regulatory pathways\n\nPromoters+enhancers: DNA regions that play a role in gene expression control\n\npromoters: located adjacent to start of a gene (upstream/to left of gene start, in chemical direction of DNA)\nenhancers: more distant regulatory elements situated within introns or between genes\nboth harbor several TF binding sites\nwith assistance of TFs, a gene’s promoter+enhancers form 3-d structures that recruit and regulate RNA polymerase responsible for mRNA synthesis\n\nChromatin structure: an amalgamation of DNA+proteins (histones) that constitute chromosomes\n\nto fit within each cell’s nucleus, DNA is wound around proteins known as histones\nhistones: tetramers (structures formed by assembling 4 copies of histone protein)\neach histone wraps around 146 nucleotide pairs of DNA, creating a rosary structure that subsequently folds into a higher order helical structure (chromatin)\nchromatin organization determines which DNA regions are accessible for gene expression\nfor gene expression to occur, chromatin must be unfolded\ntightly packed chromatin prevents gene expression\n\nHistone modifications: chemical modifications (e.g. acetylation/methylation) that can affect the histone beads+influence chromatin structure+gene accessibility\n\nmodifications can either promote or inhibit gene expression depending on type+location of modification\npart of the histone code (sort of epigenetic code) i.e. additional layer of code superimposed on the genetic code inscribed in the DNA\n\nDNA methylation: chem modification where methyl group added to DNA molec usually at specific cytosine bases\n\ncan influence gene expression by affecting binding of transcription factors or changing chromatin structure, making it more compact/less accessible for transcription\nalso part of epigenetic code\ngene regulation is a dynamic process specific to each cell type; diff cells exhibit unique gene expression profiles → perform specialized functions\nthrough precise control of gene expression cells can respond to enviro stimuli, sustain homeostasis+execute complex processes essential for life\n\nBidirectional flow of info: some exceptions to unidirectional flow of info (e.g. central dogma: DNA → RNA → protein):\n\nreverse transcription: RNA converted back to DNA; facilitated by reverse transcriptase+common in retroviruses such as HIV\nDNA can also be transcribed into RNA molec besides mRNA — tRNA, rRNA, and other types of non-coding RNA, adding another level of complexity to the flow of genetic information\nrole of epigenetics by DNA methylation and histone modification\n\n\nVariation in our DNA\n\nevery individual biologically shaped by interplay between DNA+enviro influences\nDNA variants account for heritability of all our traits\norigins of DNA variants\n\nprimary mechanism: mutations between genomes of 2 parents+germline genomes that both parents contribute to offspring’s genome\ndrive genetic variation+account for differences from other species\nmost new variants are benign\nsmaller fraction can be deleterious esp if they damage a functional region\neven smaller fraction could be beneficial\n\nSelection: deleterious variants/harmful genetic alterations render an organism less “fit” — tend to be statistically eliminated from population\n\nrare variants generally more likely to be harmful\n\nCoalescence+DNA sequence conservation: effects of selection are highly informative — 2 regions of similarity between genes eventually coalesce — eventually there is an ancestor mammalian individual that had 2 kids that both inherited precise same DNA piece, each leading to each gene today\n\nmutations that took place in important parts of gene tended to make individuals less fit\nmore conserved parts of DNA region more likely to be functionally imporant\n\nData generation: short DNA segments w/ a specific property of interest such as binding a certain TF or being part of the open accessible chromatin are isolated in an experiment+sequenced\n\nother technologies like MS and affinity-based proteomics can measure the levels of all proteins in a biological sample\nX-ray crystallography provides 3-d protein structures\n\nLinking variation to function: want to correlate genetic variants across individuals’ genomes w/ specific phenotypes (e.g. presence vs absence of a particular disease) aka GWAS\n\nidentify statistically significant associations of certain genome locations (which could be genes or regulatory regions) with the phenotypes under study\nwhen measured phenotype isn’t binary but a quantifiable entity, regression can be performed between genomic variation and phenotype, with identified genetic loci termed quantitative trait loci\nbesides macroscopic phenotypes (disease status, height, hair color), genetic variation can be associated w/ molecular phenotypes such as gene expression levels (leading to expression quantitative trait loci — eQTLs), protein abundance (results in protein quantitative trait loci — pQTLs) and virtually every other molecular measurement\nlikely to be surpassed by application of LLMs\n\nLanguage models in molec bio\n\nmodeling molec bio doesn’t need artificial general intelligence (AGI); i.e. doesn’t require high-level planning, agency, or goals; limited need for combinatorics+algorithmic reasoning\n\nrequires what LLMs are good at: learning stat properties of intricate, noisy sequential data to best predict such data from lossy representations\n\n\n\nPredicting gene structure\n\nprimary function of DNA: encode genes that are transcribed+translated into proteins\nspecific segments of each gene translated into proteins determined by splicing mechanisms (segments are annotated)\nmutations can disrupt precise boundaries of splicing (splice sites)\nrare mutations can significantly impact resulting protein fn+ produce a completely different protein sequence\n\naccount for 10% rare genetic diseases\n\nFundamental computational task: predicting splice sites+deducing gene structure; implications for diagnosing genetic diseases\n\naccuracy is not high enough\n\nSpliceAI: employs earlier techniques for language model (not transformer tech or LLM), where language is DNA sequences\n\ndeep residual CNN\ndilated convolutions to efficiently expand the window size it can handle\naccepts 10k nucleotide windows of human genome as input → predict exact locations of intron-exon boundaries (donor/acceptor sites) — exon-intron and intron-exon borders\nprecise-recall AUC: 0.98\naccurate enough to perform mutational analysis in silico — artificially laters any position of DNA and determines whether this change introduces or eliminates a splice site within 10k nucleotides of alteration\ncan be utilized to aid genetic diagnosis\nachieved high accuracy by learning biomolec properties of DNA sequence that guide splicing machinery to splice sites (previously less known)\nnew question: how to extract biomolec rules that SpliceAI learned+gain insight into underlying biomolec mechanisms?\n\nPredicting protein structure\n\nprotein sequences directly translated from spliced mRNA sequences according to genetic code, then fold into 3d structures\nwant to predict protein structure from protein sequence — difficult\nnew open-source database (AlphaFold2) that provides high-accuracy structural predictions for various organisms\n\nAlphaFold2 methodology:\n\ncombines CNN operating on protein sequences w/ pairwise co-evolution feature\nidentifies pairs of sequence positions that co-vary across related protein seuqneces in diff species to predict 2D contact maps across protein seuqnece\n\ncontact map: score for every pair of positions in sequence — likelihood of 2 positions being in close proximity in 3D\n\nbuilds on these algorithsms and introduces some new improvements:\n\nbased on transformer LLM architecture — can better capture long-range interactions between AA in protein seq\nnovel energy-based score (Amber energy) introduced to directly optimize 3d protein structure → allows for end-to-end differentiable approach during structure optimization step\nimproved utilization of coevolutionary features by incorporating multiple sequence alignment (MSA) data boosts model’s ability to identify conserved structural features across homologous protein sequences\nrefine: fine-tune predicted protein structures using second model trained on output of first model → more accurate and consistent predictions\n\nnote: ensemble?\n\n\n\n\nPredicting impact of protein variants\n\n4 million positions in genomes of any 2 individuals vary\n\n20k such variants located within protein-coding regions\nsmall fraction of genetic diversity is deleterious → contributes to genetic diseases\n\nclue in determining if a variant is benign: compare human genetics to genetics of close relatives\n\nproteins conserved by evolution are even more similar on average\n\nsearch for mutations that confer serious genetic disease should start from mutations not on this list\nuse list to observe patterns within protein seq and structures that tend to tolerate variants+patterns that tend not to tolerate variants\n\ncan gain ability to annotate variants in proteins as likely benign+likely pathogenic\n\nPrimateAI-3D: transformer that learns to distinguish between benign+pathogenic variants in human proteins\n\naccomplished by learning patterns of protein positions where primate variants tend to be present vs protein positions where they tend to be absent\nuses both protein seq data as well as protein 3D models that are either experimentally reconstructed or computationally predicted by tools like AlphaFold+HHpred\ncan be applied to diagnosis of rare disease+prioritize variants that are likely deleterious, filter out benign variants\nanother application: discovery of genes associated w/ complex diseases\n\nlook for variants likely deleterious according to PrimateAI-3D, then look for abundance of such variants within specific gene across cohort\ngenes with genetic “burden” (signal of playing role in the disease)\n\nused primate AI-3D+developed improved rare variant polygenic risk score models (PRS) to identify individuals at high diseases risk\n\n\nModeling gene regulation\n\nmolecular components: DNA chromatin structure, chemical alterations within histones that DNA wraps around, attachment of TFs to promoters+enhancers, establishment of 3D DNA structure involving promoters, enhancers, bound transcription factors, and recruitment of RNA polymerase\n\nData generation informative of gene regulation\n\nexamples of info obtained, always related to a human cell line or tissue type:\n\nidentify precise locations across entire genome that have open chromatin vs tightly packed chromatin\n\n2 relevant assays: DNAse-seq and ATAC-seq\n\npinpoint all locations in genome where a specific transcriptions factor is bound\nidentify all location in genome where a specific histone chem modification has occurred\ndetermining level of mRNA available for a given gene i.e. expression level of particular gene\n\nlanguage models → culminate in transformer-based Enformer tool\n\naccept DNA sequence near a gene as input, output cell type-specific expression level of this gene for any gene in the genome\ntrained on task: given a genome region of 100k nucleotides and a specific cell type, predict available types of experimental data for this region, including status of open/packed chromatin, present histone modifications, specific bound TFs, and level of gene expression\n\nlanguage model ideal (rather than masked language modeling) for supervised training — predict all tracks simultaneously from DNA seq\n\nincorporates attention mechanisms+collates info from distant regions to predict status of given location\n\nEnformer performs well in predicting gene exp from sequence alone\n\nhowever doesn’t achieve reduction of collecting necessary experimental data (highly correlated replicates of same experiment) yet\ncan predict changes in gene exp caused by mutations present in diff individuals as well as by mutations artificially introduced through CRISPR experiments\nlimitations: perform poorly in predicting effects of distal enhancers+correctly determine direction of effect of personal variants in gene expression\n\nOrca model: language model based on convolutional encoder-decoder architecture that predicts 3D genome structure from proximity data provided by Hi-C experiments\n\nhierarchical multi-level convolutional encoder, multilevel decoder, predicts DNA structure at 9 levels of resolution for input DNA seq that are as long as the longest human chromosome\n\n\nFoundation models\n\nlarge DL architectures (such as transformer-based GPT models by OpenAI) that encode a vast amount of knowledge\n\ncan be fine-tuned for specific tasks\n\n\nscGPT: foundation model designed for single-cell transcriptomics, chromatin accessibility, and protein abundance\n\ntrained on single-cell data from 10 million human cells\neach cell contains expression values for a fraction of ~20k human genes\nmodel learns embeddings of this large cell x gene matrix → provides insights into underlying cellular states+active bio pathways\nconcept of “next gene” is unclear in single-cell data\n\nsolution: train model to generate data based on a gene prompt (collection of known gene values)+cell prompt\nstarting from known genes, model predicts remaining genes along w/ conf values\nfor K iterations, divides those into K bins, and the top 1/K most confident genes are fixed as known genes for next iteration\n\nonce trained, fine-tune for numerous downstream tasks: batch correction, cell annotation (ground truth: annotated collections of diff cell types), perturbation prediction (predict cell state after a given set of genes are experimentally perturbed), multiomics (each layer, transcriptome, chromatin, proteome, treated as a different language)\n\nNucleotide transformer\n\nfoundational model that focuses on raw DNA sequences\nsequences tokenized into words of 6 characters each (k-mers of length 6)+trained using BERT methodlogy\ntraining dtaa: ref human genome, 3200 additional diverse human genomes, genomes of 850 other species\nnucleotide transformer applied to 18 downstream tasks that encompass many of previously discussed ones (promoter pred, splice site donor/acceptor pred, histone modifications, etc.)\npredictions made either through probing (embeddings at different layers used as features for simple classifiers — e.g. logistic regression+perceptrons) or light, computationally inexpensive fine-tuning\n\nLooking forward\n\nAGI not required (understanding molec bio/link to human health doesn’t need to be an AI task)\nasking AI to learn complex stat properties of existing biological systems\nexpect it to learn one-step causality relationships (mutation → malfunction)\n\nif gene is underexpressed, other genes in cascade in/decrease\ntriangulate between correlations across modalities such as DNA variation, protein abundance, and phenotype (Mendelian randomization) and large-scale perturbation experiments → LLM can model cellular states\ngenome ↔︎ phenotype\n\nsignificant gatekeeper: data\n\nUK Biobank Project (UKB)\n\nlarge-scale biobank, biomedical database+research resources containing comprehensive genetic/health info from 1/2 million UK participants\nlots of other large-scale data initiatives"
  },
  {
    "objectID": "posts/pytorch_tensor/index.html",
    "href": "posts/pytorch_tensor/index.html",
    "title": "PyTorch tutorial: tensor demo",
    "section": "",
    "text": "original tutorial link\ntensor: n-dimensional array; specialized data structure similar to arrays+matrices\n\nin PyTorch, used to encode inputs/outputs of a model+model parameters\nsimilar to ndarrays but run on GPUs or other accelerated hardware for computing\n\nTensor initialization — many ways:\n\ndirectly from data\ndata = [[1,2,],[3,4]]\nx_data = torch.tensor(data)\n#data type inferred\nfrom NumPy array\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array)\nfrom another tensor\n#argument tensor:\ndata = [[1,2,],[3,4]]\nx_data = torch.tensor(data)\n\n#new tensor:\nx_ones = torch.ones_like(x_data) # retains the properties of x_data\n\n#new tensor overriding properties:\nx_rand = torch.rand_like(x_data, dtype=torch.float)\n\nnew tensor retains properties (shape+datatype) of argument tensor unless explicitly overridden\n\nwith random or constant values:\nshape = (2, 3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nhere shape is a tuple of tensor dimensions; determines dimensionality of output tensor\n\n\nTensor attributes: describe shape, datatype, and device on which they are stored\ntensor_name.shape\ntensor_name.dtype\ntensor_name.device\nTensor operations: over 100 including T, index, slicing, mathematical operations, linalg, random sampling, etc.\n\nTensor API very similar to NumPy API\nNumpy-like index+slice:\ntensor = torch.ones(4, 4)\ntensor[:,1] = 0\nJoining tensors: concatenate sequence of tensors along a given dimension using torch.cat\nt1 = torch.cat([tensor, tensor, tensor], dim=1)\nmultiplying tensors element-wise: use * operator or .mul() method\ntensor.mul(tensor)\n#or\ntensor*tensor\nmatrix multiplication: use @ operator with transposed matrix or .matmul(&lt;transpose&gt;) method\ntensor.matmul(tensor.T)\n#or\ntensor @ tensor.T\nin-place operations: any operations w/ a _ suffix\n#ex:\nx.copy_(y)\nx.t_()\n#change x in place\n\ntensor.add_(5) \n#change tensor in place\n\nhowever can cause issues when computing derivatives b/c of immediate loss of history\n\n\nBridge with NumPy: tensors on CPU+NumPy arrays can share underlying memory locations; changing one will change other\n\ntensor to np array\nt = torch.ones(5)\nn = t.numpy()\n\nchange in tensor reflects in np array\nt.add_(1)\n#n will change if t changes\n\nnp array to tensor\nn = np.ones(5)\nt = torch.from_numpy(n)\n\nchanges in np array reflects in tensor\nnp.add(n, 1, out=n)\n#t will change if n changes"
  },
  {
    "objectID": "posts/pytorch_nn/index.html",
    "href": "posts/pytorch_nn/index.html",
    "title": "PyTorch tutorial: NN demo",
    "section": "",
    "text": "original tutorial link\nNeural Networks\n\ntorch.nn package\nnn depends on autograd to define models+differentiate them\nnn.Module contains:\n\nlayers\nmethod forward(input) that returns output\n\n\nEx. MNIST\n\n\n\nhttps://pytorch.org/tutorials/_images/mnist.png\n\n\n\nsimple feed-forward network: takes input, feeds through several layers one after the other, then finally gives output\n\nTypical training procedure for NN:\n\ndefine NN that has some learnable parameters (or weights)\niterate over dataset of inputs\nprocess input through network\ncompute loss\npropagate gradients back into network’s parameters\nupdate weights of network, typically w/ simple update rule: weight = weight - learning_rate * gradient\n\nDefine the network\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        # kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square, you can specify with a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\nprint(net)\n\njust define forward fn\n\nbackward fn (where gradients are computed) automatically defined by autograd\ncan use any Tensor operations in forward fn\n\nlearnable parameters of a model returned by net.parameters()\n\nparams = list(net.parameters())\n#net_name.parameters()\n\nlen(params) #10\n\nparams[0].size()) #conv1's .weight\nTrying a random 32x32 input:\ninput = torch.randn(1,1,32,32)\nout=net(input)\nRandom gradients to zero gradient buffers of all parameters and backprops (by default, gradients are accumulated in buffers (i.e, not overwritten) whenever .backward() is called)\nnet.zero_grad()\nout.backward(torch.randn(1,10))\n\ntorch.nn only supports mini batches\nentire torch.nn package only supports inputs that are a mini-batch of samples and not a single sample\n\nex. nn.Conv2d takes in 4D tensor of nSamples x nChannels x Height x Width\n\nsingle sample → input.unsqueeze(0) to add fake batch dimension\n\nClass recap\n\ntorch.Tensor - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\nnn.Module: neural network module. Convenient way of encapsulating parameters, w/ helpers for moving them to GPU/exporting/loading, etc.\nnn.Parameter: a kind of tensor automatically registered as a parameter when assigned as an attribute to a Module\nautograd.Function: implements forward and backward definitions of autograd operation. Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history\n\nLoss fn\ntakes (output, target) pair of inputs → compute value that estimates how far away output is from target\nlist of loss fns under nn package\nEx. a simple loss is nn.MSELoss: computes mean-squared error between output+target\noutput = net(input)\ntarget = torch.randn(10) #dummy target\ntarget = target.view(1,-1) #make same shape as output\ncriterion = nn.MSELoss()\n\nloss = criterion(output, target\n\nfollow loss in backward direction using .grad_fn attribute → see graph of computations like this:\n\ninput -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d\n      -&gt; flatten -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear\n      -&gt; MSELoss\n      -&gt; loss\n\ncall loss.backward → whole graph is differentiated wrt NN parameters+all Tensors in graph w/ requires_grad=True will have their .grad Tensor accumulated w/ gradient\n\n#follow a few steps backward:\n\n#mseloss\nprint(loss.grad_fn)  \n\n#linear\nprint(loss.grad_fn.next_functions[0][0])  \n\n#relu\nprint(loss.grad_fn.next_functions[0][0].next_functions[0][0])  \nBackprop\nbackpropagate error w/ loss.backward()\n**need to clear existing gradients or they will be accumulated to existing gradients\nEx. call loss.backward() and look at conv1’s bias gradients before+after backward\n#zero grad buffers of all params\nnet.zero_grad()\n\n#conv1.bias.grad before backward\nprint(net.conv1.bias.grad)\n#output: None\n\nloss.backward()\n\n#after backward\nprint(net.conv1.bias.grad)\n#output: tensor([-0.0024, -0.0051, -0.0055, -0.0025,  0.0090,  0.0032])\nUpdate the weights\nsimplest update rule — stochastic gradient descent (SGD)\nweight = weight - learning_rate*gradient\nimplementation:\nlearning_rate = 0.01\nfor f in net.parameters():\n    f.data.sub_(f.grad.data * learning_rate)\n\nwill want to use different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n\nuse torch.optim package\n\n\nimport torch.optim as optim\n\n# create your optimizer\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\n# in your training loop:\noptimizer.zero_grad()   # zero the gradient buffers\noutput = net(input)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step()    # Does the update\n**Note: gradient buffers have to be manually set to zero using optimizer.zero_grad()"
  },
  {
    "objectID": "posts/population_structure/index.html",
    "href": "posts/population_structure/index.html",
    "title": "Population structure",
    "section": "",
    "text": "#libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(devtools)\n\nLoading required package: usethis\n\nlibrary(glue)\nlibrary(qqman)\n\n\nFor example usage please run: vignette('qqman')\n\nCitation appreciated but not required:\nTurner, (2018). qqman: an R package for visualizing GWAS results using Q-Q and manhattan plots. Journal of Open Source Software, 3(25), 731, https://doi.org/10.21105/joss.00731.\n\nsource_gist(\"38431b74c6c0bf90c12f\")\n\nℹ Sourcing gist \"38431b74c6c0bf90c12f\"\nℹ SHA-1 hash of file is \"cbeca7fd9bf1602dee41c4f1880cc3a5e8992303\"\nif(!file.exists(glue::glue(\"~/Downloads/analysis_population_structure.tgz\"))){\n  system(glue::glue(\"wget -O ~/Downloads/analysis_population_structure.tgz https://uchicago.box.com/shared/static/zv1jyevq01mt130ishx25sgb1agdu8lj.tgz\"))\n  ## tar -xf file_name.tar.gz --directory /target/directory\n  system(glue::glue(\"tar xvf ~/Downloads/analysis_population_structure.tgz --directory ~/Downloads/\")) \n}\n\nwork.dir =\"~/Downloads/analysis_population_structure/\"\nTest HWE w/ population structure\nPopulation composition\npopinfo = read_tsv(paste0(work.dir,\"relationships_w_pops_051208.txt\"))\n\nRows: 1301 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): FID, IID, dad, mom, population\ndbl (2): sex, pheno\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npopinfo %&gt;% count(population)\n\n# A tibble: 11 × 2\n   population     n\n   &lt;chr&gt;      &lt;int&gt;\n 1 ASW           90\n 2 CEU          180\n 3 CHB           90\n 4 CHD          100\n 5 GIH          100\n 6 JPT           91\n 7 LWK          100\n 8 MEX           90\n 9 MKK          180\n10 TSI          100\n11 YRI          180\n\nsamdata = read_tsv(paste0(work.dir,\"phase3_corrected.psam\"),guess_max = 2500) \n\nRows: 2504 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): #IID, PAT, MAT, SuperPop, Population\ndbl (1): SEX\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nCombine into superpopulation\nsuperpop = samdata %&gt;% select(SuperPop,Population) %&gt;% unique()\nsuperpop = rbind(superpop, data.frame(SuperPop=c(\"EAS\",\"HIS\",\"AFR\"),Population=c(\"CHD\",\"MEX\",\"MKK\")))\nEffect of population structure in HWE\nif(!file.exists(glue::glue(\"{work.dir}output/allhwe.hwe\")))\n  system(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --hardy --out {work.dir}output/allhwe\"))\nCalculate HWE in this mixed population:\nallhwe = read.table(glue::glue(\"{work.dir}output/allhwe.hwe\"),header=TRUE,as.is=TRUE)\nhist(allhwe$P)\n\n\n\nqqunif(allhwe$P,main='HWE HapMap3 All Pop')\n\nWarning in qqunif(allhwe$P, main = \"HWE HapMap3 All Pop\"): thresholding p to\n1e-30\nWhat if we calculate with single population?\npop = \"CHB\"\npop = \"CEU\"\npop = \"YRI\"\nfor(pop in c(\"CHB\",\"CEU\",\"YRI\"))\n{\n  ## what if we calculate with single population?\n  popinfo %&gt;% filter(population==pop) %&gt;%\n    write_tsv(path=glue::glue(\"{work.dir}{pop}.fam\") )\n  if(!file.exists(glue::glue(\"{work.dir}output/hwe-{pop}.hwe\")))\n    system(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --hardy --keep {work.dir}{pop}.fam --out {work.dir}output/hwe-{pop}\"))\n  pophwe = read.table(glue::glue(\"{work.dir}output/hwe-{pop}.hwe\"),header=TRUE,as.is=TRUE)\n  hist(pophwe$P,main=glue::glue(\"HWE {pop} and founders only\"))\n  qqunif(pophwe$P,main=glue::glue(\"HWE {pop} and founders only\"))\n}\n\nWarning: The `path` argument of `write_tsv()` is deprecated as of readr 1.4.0.\nℹ Please use the `file` argument instead.\nEffect of population stratification on GWAS\nGWAS on a growth phenotype in HapMap samples\nigrowth = read_tsv(\"https://raw.githubusercontent.com/hakyimlab/igrowth/master/rawgrowth.txt\")\n\nRows: 3726 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): IID, pop, serum\ndbl (4): sex, experim, meas.by, growth\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# add FID to igrowth file\nigrowth = popinfo %&gt;% select(-pheno) %&gt;% inner_join(igrowth %&gt;% select(IID,growth), by=c(\"IID\"=\"IID\"))\nwrite_tsv(igrowth,path=glue::glue(\"{work.dir}igrowth.pheno\"))\nigrowth %&gt;% ggplot(aes(population,growth)) + geom_violin(aes(fill=population)) + geom_boxplot(width=0.2,col='black',fill='gray',alpha=.8) + theme_bw(base_size = 15)\n\nWarning: Removed 130 rows containing non-finite values (`stat_ydensity()`).\n\n\nWarning: Removed 130 rows containing non-finite values (`stat_boxplot()`).\nsummary( lm(growth~population,data=igrowth) )\n\n\nCall:\nlm(formula = growth ~ population, data = igrowth)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-58821 -18093  -2242  15896  98760 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    73080.8      938.2  77.894  &lt; 2e-16 ***\npopulationCEU  -2190.1     1175.4  -1.863   0.0625 .  \npopulationCHB   9053.1     2043.9   4.429 9.73e-06 ***\npopulationJPT   3476.8     2034.8   1.709   0.0876 .  \npopulationYRI  -7985.2     1137.2  -7.022 2.61e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24160 on 3591 degrees of freedom\n  (130 observations deleted due to missingness)\nMultiple R-squared:  0.0345,    Adjusted R-squared:  0.03342 \nF-statistic: 32.08 on 4 and 3591 DF,  p-value: &lt; 2.2e-16\nif(!file.exists(glue::glue(\"{work.dir}output/igrowth.assoc.linear\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --linear --pheno {work.dir}igrowth.pheno --pheno-name growth --maf 0.05 --out {work.dir}output/igrowth\"))\nigrowth.assoc = read.table(glue::glue(\"{work.dir}output/igrowth.assoc.linear\"),header=TRUE, as.is=TRUE)\nhist(igrowth.assoc$P)\nqqunif(igrowth.assoc$P)\nmanhattan(igrowth.assoc, chr=\"CHR\", bp=\"BP\", snp=\"SNP\", p=\"P\" )"
  },
  {
    "objectID": "posts/population_structure/index.html#calculate-principal-components-using-plink",
    "href": "posts/population_structure/index.html#calculate-principal-components-using-plink",
    "title": "Population structure",
    "section": "6. Calculate principal components using plink",
    "text": "6. Calculate principal components using plink\n\n# generate PCs using plink\nif(!file.exists(glue::glue(\"{work.dir}output/pca.eigenvec\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --pca --out {work.dir}output/pca\"))\n\n\n# read plink calculated PCs\npcplink = read.table(glue::glue(\"{work.dir}output/pca.eigenvec\"),header=FALSE, as.is=TRUE)\nnames(pcplink) = c(\"FID\",\"IID\",paste0(\"PC\", c(1:(ncol(pcplink)-2))) )\npcplink = popinfo %&gt;% left_join(superpop,by=c(\"population\"=\"Population\")) %&gt;% inner_join(pcplink, by=c(\"FID\"=\"FID\", \"IID\"=\"IID\"))\n\n# plot PC1 vs PC2\npcplink %&gt;% ggplot(aes(PC1,PC2,col=population,shape=SuperPop)) + geom_point(size=3,alpha=.7) + theme_bw(base_size = 15)\n\n\n\n\nRunning igrowth GWAS using PCs\n\nif(!file.exists(glue::glue(\"{work.dir}output/igrowth-adjPC.assoc.linear\")))\nsystem(glue::glue(\"~/bin/plink --bfile   {work.dir}hapmapch22 --linear --pheno {work.dir}igrowth.pheno --pheno-name growth --covar {work.dir}output/pca.eigenvec --covar-number 1-4 --hide-covar --maf 0.05 --out {work.dir}output/igrowth-adjPC\"))\nigrowth.adjusted.assoc = read.table(glue::glue(\"{work.dir}output/igrowth-adjPC.assoc.linear\"),header=TRUE,as.is=TRUE)\n\n#indadd = igrowth.adjusted.assoc$TEST==\"ADD\"\ntitulo = \"igrowh association adjusted for PCs\"\n\nhist(igrowth.adjusted.assoc$P,main=titulo)\n\n\n\n\n\nqqunif(igrowth.adjusted.assoc$P, main=titulo)"
  },
  {
    "objectID": "posts/w1_thesis_updates/index.html",
    "href": "posts/w1_thesis_updates/index.html",
    "title": "W1 Thesis updates",
    "section": "",
    "text": "single cell analysis\n\nmatrix: rows - cells; columns: genes; entries: count\n1k1k - 1000 cells/individual; 1000 individuals → 1 million cells\nsingle cell vs bulk (bulk: less noisy)\n\nbefore: generate predictions of bulk data\nnow: generate predictions of single cell data (harder since noisier)\n\nusing ranks — don’t need to remove data (2 branches of analysis — one with and one without outliers)\n\ncomparing portability of enformer vs predixcan between populations (2 methods)\n\nenformer DL model - personalized predictions\nindividuals annotated by ancestral origin, bulk data\nrun personal genome through enformer → gene expression; split between ancestral origin\n\nissue: disparities in predictions based on training\n\nportability:\n\\[\n  P_g = \\frac{|r_Y|+\\epsilon}{|r_C|+\\epsilon}\n  \\]\n\ncorrelation between observed and predicted; split based on populations\nerror = .1\n\npredixcan: population level data - performs better currently for both EUR and AFR predictions\n\nissue: portability\nloss in performance when you from european to african individuals is worse in predixcan compared to enformer\n\n\nmetaboXcan\n\nuse genotype to predict metabolite levels → phenotype\n\ngenotype → gene prediction → phenotype\nmetaboXcan: “shortcut” for total mRNA → metabolite level\n\n\npredict transcriptome factor binding\n\ngoal: to do that for individuals w/ phenotypes w/ or w/o disease (potentially compare binding in regions)\nTFPred — method on top of enformer (given genome sequence, apply TFPred scores to quantify androgen receptor bindings for regions)\ntrying to find what genes are regulated by androgen receptor bindings\ncorrelating binding affinity for androgen receptor with expression levels of gene (more or less of mRNA)\nsetup (each row: individual):\n\n\n\nbinding score\ngen exp\n\n\n\n\n…\n…\n\n\n\ntesting “cis effect” - testing transcriptome factor binding on the same chromosome\nwant to test “trans effects” / associations\n\nto make sure not due to proximity\n\n#genes in cis\npairwise associations in trans\ntrans effects — be careful about mappability (artifact from wrong alignment)"
  },
  {
    "objectID": "posts/z2h_micrograd_questions/index.html",
    "href": "posts/z2h_micrograd_questions/index.html",
    "title": "z2h micrograd questions",
    "section": "",
    "text": "What does the code (f(x + h) - f(x))/h calculate?\n\nIt approximates the derivative of the function f at the point x, with h as 0.01 or 0.001 or some really small number. This is identical to the actual definition of the derivative, but here we are just approximating with h as 0.001 in order to numerically verify the results we are storing in .grad attributes of our value object nodes.\n\nWhat is the Value class designed to represent in this code?\n\nIt represents a node in a computational graph, storing data and gradient, and also defines how to compute gradient for our expression graph (chains the output gradients to input gradients).\n\nWhat does the backward method do in the Value class?\n\nThis initializes backprop. It calculates the gradients of all preceding nodes. It goes backwards through expression graph and recursively applies the chain rule.\n\nAfter calling f.backward(), what does b.grad represent?\n\nThe derivative of f wrt b.\n\nWhat does the function lol() do in the code?\n\nThis was a gating fn used to just make sure we didn’t mess up anything in our global scope. That way we can create new value objects there and just rewrite it to try different parameters. It calculates the gradient of L with respect to a single parameter, such as b or d or L itself (which is just 1).\n\nWhat are the operations performed in the __call__ method of the Neuron class?\n\nIt computes the weighted sum of its inputs and then passes that sum through the .tanh activation function. It does this by taking 2 iterators (\\(w_i\\) and \\(x_i\\))+creates new iterator that iterates over tuples of corresponding entries and multiplies them pairwise, then takes the sum (this is just dot product) and adds the bias to it. Then it squashes it through \\(\\tanh\\) activation fn in order to make it nonlinear.\n\nWhat is the purpose of the MLP class in the code?\n\nIt defines a simple MLP architecture with an arbitrary number of hidden layers. Specifically, this joins together the layer objects created from our Layer class so that we can call them sequentially (they feed into each other sequentially).\n\nWhat does ypred output?\n\nA forward pass — in the case of ypred = [n(x) for x in xs], the output of the network for each example in xs after 20 updates."
  },
  {
    "objectID": "posts/functional_annotation_gwas/index.html",
    "href": "posts/functional_annotation_gwas/index.html",
    "title": "Functional annotation of GWAS loci using transcriptome data",
    "section": "",
    "text": "Lecture notes from quant genomics training (day 2)\nPredixCan (TWAS)\nGTEx consortium\n\ncollect 1000 organ donors and tissue samples from different sites of the body; all RNA-sequenced (have genotype data)\n\neQTL\n\n→ that data used to calculate eQTL (expression quantitative trait loci) — SNPs associated/correlated w/ expression levels of a gene\n\\(T = \\gamma \\cdot X+\\epsilon\\)\n\nX = genotype; gamma = effect size being estimated\n\n\nwe have genotype data to train predictors\n\ndifferent layers of genotype matrices make up transcriptome\nfit linear model to predict weight that gives us best representation of genetic component of expression\ntake genotype matrix → transform/impute transcriptome\n\ntake genotype data → predict transcriptome\n\nHow well are we predicting?\n\ncompare predicted and observed expression\npretty high correlation (promising)\n\nblack curve: measure of heritability of the expression level of that gene\n\nAdvantages of gene level association\n\nreduced multiple testing burden (from a million to around 20k)\nfunction of genes are much better annotated than SNPs\nvalidation in other model systems is possible (like in mice or zebrafish)\nreverse causality issues is less of an issue (germline DNA doesn’t change w/ disease status)\nprovides direction of effects; i.e. whether up or down regulation of a gene increases risk of a disease\ncandidate causal gene is a good target for drug development\n\nS-PrediXcan (summary-PrediXcan)\nregular PrediXcan: take genotype matrix (hard to get access to) → predict transcriptome → run association for every gene → get gene-level results\n\nhard to do since it’s hard to get the data in genotype matrix\n\n\nsolution: S-prediXcan — take SNP-level results and get gene-level results\n\ncan derive formula that gives you the gene level results from SNP-level results\n\nLimitations of TWAS methods\n\n\nLD contamination\n\nin TWAS, taking a bunch of eQTLs and predict mRNA to some degree (correlating that to disease level)\neQTL is nearby a SNP that is causing the disease — so not actually from eQTL; just from proximity (artifact of LD)\nsolutions:\n\ncan use fine-mapped predictors (improve quality of list of genes associated with disease)\npost-filtering w/ colocalization methods\n\n\nCo-regulation: cannot figure out which gene is the closer one\n\nmight find 2 different genes that are associated with a disease but one of them is not actually (just a confounder)\nmore difficult to tackle\n\n\nTake home message\n\ngene-level association methods (TWAS methods) have many advantages but can lead to false positives\nsignificant genes are excellent hypothesis generators that need to be confirmed with independent lines of evidence (smaller list of genes → less hypothesis tests to run)\n\nColocalization\n\ngiven a GWAS study and eQTL study, can try to find which one is the causal gene\nattempts to determine whether causal variants altering gene expression levels are same as causal variants that alter disease\nkey: fine-mapping to find causal variants\n\ngiven a statistical method, tries to find causal variants (i.e. what’s the probability that this SNP is the causal variant)\n\nex. (note: normally we don’t know ground truth) true effects\n\n\nsimulate phenotype based on this model and run a GWAS\nbelow: GWAS p-values\n\nhigher up on the plot —&gt; more significant correlation\n\nnot all points at the bottom\n\n\nfine-mapping\n\n\nPIP (posterior inclusion probability) - probability that this given variant is the causal variant using SuSi\n\n\nColocalization: are causal variants = ?\n\ntries to tell you whether causal SNP for one thing is same as causal signal in something else\nex. what’s mediating trait in FTO muscle and BMI → if we find something, can manipulate levels in one thing to see if it does anything to the other thing\neven if location is the same the SNP could be different\nnot colocalized if signal seem to be in the same region but if you do fine mapping that the causal variant for one thing is distinct from the causal variant for the other thing\n\nLimits of colocalization\n\nfine-mapping can be sensitive to LD reference\n\ncorrelation matrix that we use between SNPs\n\n\n\nOther TWAS methods\n\nTWAS/FUSION\nUTMOST\ncTWAS\n\nuses both fine mapping and association to calculate probability of gene being causal accounting for LD\n\n\nColocalization Methods\n\nCOLOC\nENLOC\nfastENLOC\neCAVIAR\n\nMendelian randomization\nRandomized trial vs Mendelian randomization\n\n\nMendelian randomization: uses genetic variant as instruments to try to find causal exposures\nrandomized trial\n\nex. in biomedical studies if you want to know the effect of a drug on the severity of a disease, need to run randomized trial (control+treatment) in order to determine causality\n\nMendelian\n\nrandomization occurs by genetic variant (risk allele present or absent) aka nature’s randomized trial\n\n\n\n\nlevel of evidence for causation is much higher in Mendelian randomization\n\nMendelian randomization question\n\ngoal: test association between a modifiable exposure+disease (ex. smoking, HDL cholesterol levels, etc.)\nproblem: if you just look at exposure and disease there could be confounders\nsolution: IV (SNPs)\n\n3 IV assumptions:\n\nindependence assumption between SNPs and confounders\nrelevance assumption between SNPs and exposure (reliable association)\nexclusion restriction assumption between SNPs and disease (no direct effect)\n\n\nTWAS can be thought of as Mendelian randomization\n\n“exposure” — baseline expression of gene\nwant to see if up or down regulation of gene is causing a disease\nif we use SNPs to predict expression level of gene as instrument, can investigate any associations\n\nSMR (summary data-based Mendelian randomization)\n\nsame as previous way but with eQTL as IV\n\nmultiple eQTL and multiple genes\n\nNew methods\n\nMetaboXcan: predict metabolite levels\n\n\n\nadded link between mRNA levels and metabolites (don’t just want list of genes and list of metabolites)\n\nDeep learning-based predictors\n\n\n\n\nTFXcan\nsingle cell PrediXcan\nEpigenomeXcan"
  },
  {
    "objectID": "posts/enformer_summary/index.html",
    "href": "posts/enformer_summary/index.html",
    "title": "Enformer paper summary",
    "section": "",
    "text": "paper link\nobjective: improve gene expression prediction accuracy from DNA sequences using a type of DL architecture (Enformer) that can include info from long-range interactions in genome → higher accuracy for variant effect predictions on gene expression, learned to predict enhancer-promoter interactions directly from DNA sequence → future steps: more effective fine-mapping of human disease associations\nMain\nPopulation-based association studies+models predicting gene expression+chromatin states from DNA sequences:\n\nthe former limited to common variants; hard to disentangle causality from assoc due to LD (linkage disequilibrium)\nalso hard to test all variants of interest in different relevant contexts in experimental validation\nsequence-based computational models have limited accuracy\n\nCurrently: deep CNNs to predict gene expr from DNA seqs for human+mouse genomes\n\ndrawback: can only consider seq elements &lt; 20 kb away from TSS (transcription start site) b/c locality of convolutions limits info flow in net between far apart elements\n\nmany regulatory elements can infleunce gene expr from far away so need to increase info flow between distal elements\n\n\nResults\nSolution: Enformer — a new model architecture to predict gene expr+chromatin states in humans+mice from DNA seqs\n\ntransformers: a class of DL models for NLP; recently applied to model short DNA seqs\n\nconsist of attention layers that transform each position in input sequence by computing weighted sum across representations of all other positions in sequence\nattention weight between any 2 positions depends on embeddings of current representation vectors+distance between\nmodel can refine prediction at a TSS by getting info from relevant regions (e.g. relevant enhancers)\nbetter info flow between distal elements since each position directly attends to all other positions in sequence\n\nvs convolutional layers — need many many layers to get to far away elements due to their local receptive field\ntransformer layers → increase receptive field → reach distal regulatory elements up to 100 kb away\n\nexpands # relevant enhancers seen by model from 47% to 84% (&lt;100 kb)\n\nEnformer better at predicting RNA expr at TsS of human protein-coding genes, consistent across all 4 types of genome wide tracks (CAGE — transcriptional activity, histone modifications, TF binding, DNA accessibility)\n\nesp CAGE — prob b/c tissue-specific gene expr depends a lot on distal elements\n\ngreater predictive accuracy than ExPecto (model trained to predict gene expr levels measured by RNA seq) for across-genes+across-tissues\n\nDemonstrating benefit of attention layers\n\n(compared to dilated convolutions in previous model)\nreplace attention layers w/ dilated convolutions+tuned learning rate for optimal performance → attention layers outperformed dilated convolutions across all model sizes, # layers, # training data pts\nobserved performance drop when restricting receptive field of Enformer to that of the previous model by replacing global attention layers w/ local\nincrease # params → improve model performance (consistent w/ recent advances in NLP)\nuse custom relative positional basis fns in transformer layers (distinguish between proximal and distal regulatory elements more easily)\n\nnoticeable performance improvement compared to typically used relative basis fns+absolute positional encodings in NLP lit\n\n\nEnformer attends to cell-type-specific enhancers\n\nwhat seq elements Enformer uses to make predictions? → compute 2 diff gene expr contribution scores:\n\ninput gradients (gradient \\(\\times\\) input)\nattention weights\n\nfor several genes w/ CRISPRi-validated enhancers\n\ncontribution scores — highlight most predictive input seq for particular gene’s expression\n\ninput gradients are tissue- or cell-type-specific (b/c computed wrt particular output CAGE sample)\nvs attention weights — internal to model+shared among all tissue+cell-type predictions\nobserved that contribution scores of several genes correlated w/ histone H3 acetylated at K27 (H3K27ac) and highlighted not just local promoter regions but also distal enhancers &gt;20 kb away\n\nsuggests that Enformer is looking at biologically relevant regions when making predictions; gene expr contribution scores could be used to prioritize relevant enhancers\n\nunsolved problem: how to link candidate enhancers id’ed vai biochemical annotations to target genes\n\nissue: computational models have historically low accuracy b/c of noisy labels+class imbalance\n\nevaluate ability of contribution scores to pinpoint relevant enhancers for a specific gene: compare several contribution scores across all tested enhancer-gene pairs in 2 large-scale CRISPRi studies performed on K562 cell line\nCRISPRi used to suppress activity of &gt; 10,000 candidate enhancers+measure effect on gene expr\ncontribution scores prioritize validated en-gene pairs w/ higher accuracy despite only using DNA seq as input — never trained to explicity locate enhancers\n\ncan be used for arbitrary seq variations lacking experimental data\n\nmodel uses diff enhancer seqs in diff cell types as expected → can prioritize candidate enhancers in cell types used for model training\n\nhas model learned about insulator elements (separate 2 topologically associating domains, i.e. TADs, minimize enhancer-promoter crosstalk between the two)\n\nused attention matrices (more efficient than input gradients to compute b/c of many output targets) of seqs centered at TAD boundaries+compared to attention from seqs w/ no particular alignment\nEnformer paid more attention to TAD boundaries than random positions; less attention to regions on opposite side of boundary — consistent w/ reduced inter-TAD interactions in biology\nsuggest that model has not only learned about role of tissue-specific enhancers+promoters, but also about insulator elements (and their role in inhibiting info flow between genomic compartments)\n\nEnformer improves variant effect prediction on eQTL data\n\ncentral goal: predict influence of genetic variants on cell-type-specific gene expr\n\n→ inform fine-mapping of many noncoding assoc w/ phenotypes of interest from GWAS\n\ncomputational models that predict regulatory activity from DNA seqs can process distinct alleles+compare preds to score genetic variants\nsuccessful model can produce eQTL results w/o having to measure lots of individual gene expr profiles\nchallenge: LD in profiled pop → transfers causal eQTL’s effect to nearby co-occurring variants’ measurements\nSLDP (signed linkage disequilibrium profile) regression: measures genome-wide statistical concordance between signed variant annotations (e.g. model preds) and GWAS summary stats (e.g. GTEx eQTLs) while accounting for LD\nEnformer preds for noncoding-variant activity seem to improve for samples w/ similar cell-type composition mostly\n\nhow to assess use of preds for identifying causal variants → classification task per tissue → discriminate likely causal variants from likely spurious eQTLs\n\neach variant represented by its pred diff vector (reference-alternative allele, summed across seq) for all 5313 human datasets\ntrained random forest classifiers\npreds enabled more accurate classifier for most GTEx tissues\n\nEnformer improves MPRA mutation effect prediction\n\neval Enformer’s performance on second, independent variant effect pred tas w/ dataset where MPRAs (massively parallel reporter assays) directly measured fnal effect of genetic variatns through sutration mutagenesis of several enhancers+promoters in variety of cell types\nfor each variant, eval effect as pred diff between ref+alt allele → retrieve 5313 features\ncompare 2 approaches:\n\ntrain lasso reg model on provided training set for each gene\npreselect subset of features corresponding to cell-type-matched/agnostic preds of changes in CAGE+DNase → generate summary stat of features\n\n\nLasso regression w/ Enformer preds as features had best average correlation across all loci\nFuture steps\n\nsystematically apply Enformer to fine-map existing GWAS studies\nprioritize rare or de novo variants for rare disorders\nimpute regulatory activity across species to study cis-regulatory evolution\n\nMethods\nModel architecture\ndiagram of typical Transformer architecture:\n\n\n\nhttps://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png\n\n\nEnformer architecture has 3 parts\n\n\n\nhttps://www.biorxiv.org/content/biorxiv/early/2021/04/08/2021.04.07.438649/F5.medium.gif\n\n\n\n7 convolutional blacks w/ pooling\n11 transformer blocks\na cropping layer followed by final pointwise convolutions branching into 2 organism-specific network heads\n\ninput: one-hot-encoded DNA seq\n\nA = \\(\\left[1,0,0,0\\right]\\)\nC = \\(\\left[0,1,0,0\\right]\\)\nG = \\(\\left[0,0,1,0\\right]\\)\nT = \\(\\left[0,0,0,1\\right]\\)\nN = \\(\\left[0,0,0,0\\right]\\)\n\nof length 196,608 bp → predicts 5313 genomic tracks for human genome, 1643 tracks for mouse genome (each of length 896 corresponding to 114,688 bp aggregated into 128-bp bins)\n\nconvolutional blocks w/ pooling first reduce the spatial dimension from 196,608 bp to 1526 so each seq position vector represents 128 bp\ntransformer blocks capture long-range interactions across seq\ncropping layer trims 320 positions on each\nto avoid computing loss on far ends b/c these regions are disadvantage (observe regulatory elements only on one side, toward seq center)\n2 output heads predict organism-specific tracks\n\nchanges: use transformer blocks instead of dilated convolutions, attention pooling instead of max pooling, twice as many channels, 1.5x longer input seq\nattention pooling: summarizes contiguous chunk of input sequence \\(\\mathbf{x}_{k:k+L_p}^{full}=\\mathbf{x} \\in R^{L_p\\times C}\\) across \\(L_p\\) positions for each of the \\(C\\) chanels\n\noutput value \\(\\mathbf{h} \\in \\mathbf{\\mathit{R}}^C\\):\n\n\\[\nh_j = \\frac{\\sum_i exp(x_i \\cdot w_j)x_{ij}}{\\sum_i exp(x_i\\cdot w_j)}\n\\]\nwhere i indexes seq position in pooling window, which is weighted by exponentiated dot product \\(\\mathbf{x}_i\\cdot \\mathbf{w}_j\\) and \\(\\mathbf{\\mathit{w}} \\in R^{C\\times K}\\) is a matrix of learned weights\n\napply attention pooling to contiguous chunks of original input seq using window size \\(L_p = 2\\) and stride of 2\n\ninitialize \\(\\mathbf{w}\\) to \\(2\\times \\mathbf{1}\\)\n\n\\(\\mathbf{1}\\) = identity matrix\nprioritizes larger value → similar operation to max pooling\nslightly better performance than random initialization or initialization w/ zeros (average pooling)\n\nMHA (multi-head attention) layers used to capture long-range interactions across sequence\n\neach head has separate weights for queries, keys, values that transform input seq\n\nseparate set of weights \\(\\mathbf{w}^q \\in \\mathbf{R}^{C\\times K}\\), \\(\\mathbf{w}^k \\in \\mathbf{R}^{C\\times K}\\), \\(\\mathbf{w}^v \\in \\mathbf{R}^{C\\times K}\\)\n→ transform input sequence \\(\\mathbf{x}\\in \\mathbf{R}^{L\\times C}\\) into:\n\nqueries \\(\\mathbf{q}_i=\\mathbf{x}_i\\mathbf{w}^q\\)\nkeys \\(\\mathbf{k}_j=\\mathbf{x}_j\\mathbf{w}^k\\)\nvalues \\(\\mathbf{v}_j=\\mathbf{x}_j\\mathbf{w}^v\\)\n\n\nqueries = current info at each position\nkeys = info each position looks for to attend to\ndot product between queries+keys plus relative positional encodings \\(\\mathbf{R}_{ij}\\) → attention matrix\n\ncomputed as \\(\\mathbf{a}_{ij} = softmax(\\mathbf{q}_i\\mathbf{k}_j^T/\\sqrt{K}+\\mathbf{R}_{ij})\\)\nentry \\(a_{ij}\\) = amount of weight that query at position i puts on key at position j\n\nvalues = info propagated to positions attending to a specific position\neach single attention head computes its output as weighted sum across all input positions: \\(\\mathbf{av}\\)\n\nlets query position use info across whole seq\nmultiple heads compute w/ independent parameters\n\noutputs from each head concatenated to form final layer output followed by linear layer to combine\n\nEnhancer prioritization:\n\nGradient \\(\\times\\) input: compute abs value of gradient of CAGE targets at TSS w. regard to input reference seq nucleotide\n\nequivalent to computing gradient \\(\\times\\) input attirbutions (since one-hot-encoding)\nCAGE at TSS = summing abs grad values from 3 adjacent bins (including bin overlapping TSS, one flanking bin on each side)\nenhancer-gene scare obtained by summing abs grad\\(\\times\\)input scores in 2-kb window centered at enhancer\n\nattention: averaged transformer attention matrices across all heads+layers → extract row corresponding to query index positioned at TSS\n\nkeys correspond to diff spatial positions\nattention values = how much model attended to these positions when making preds for TSS\nen-gene score — sum attention scores in 2-kb window centered at enhancer\n\nISM: in silico mutagenesis en-gene score\n\ncomputed by comparing K562 CAGE preds at TSS from ref seq with preds from modif seq where 2-kb enhancer seq was replaced by a random sequence: \\(|f(modified)-f(reference)|\\)\n\n\nother details for methodology on model training+eval can be found in paper*"
  },
  {
    "objectID": "posts/w3_thesis_updates/index.html",
    "href": "posts/w3_thesis_updates/index.html",
    "title": "W3 thesis updates",
    "section": "",
    "text": "PRS-cs, Prsice, and Lassosum\n\n\n\n\nSNP\n\\(\\beta\\)\nse\n\n\n\n\nChr 1\n\n\n\n\n\n\n\nclumping: looks for loci where you have significant SNPs; keep top SNPs\nthresholding: do that but still want p-value to be there (choose what threshold gives you best preds)\nLassosum: combines Lasso reg technique w/ sum stats\n\nModel fine-tuning on Aracena et al dataset\n\nwhole genome sequencing for each individual; for each sample measure RNA seq, look at histone marks+DNA methylation\naim: use this as a fine-tuning dataset\n\nex. can start “foundation” part of the model after in the middle → add fine-tuning head using Aracena et al for training\n\n\nTFXcan\ngenome-wide scale on multiple genes on multiple chromosomes and effects on other chromosomes → trans effect\n\ny: target gene; x: trans-acting promoter\nwant to investigate trans biology; want to remove cis effects b/c they are very large\n\nMulti-task prediction model\n\npredicting expression in 5 different cell types\ncould try shrinkage\nhighly expressed vs lowly expressed clusters"
  },
  {
    "objectID": "posts/enformer_usage_hackathon/index.html",
    "href": "posts/enformer_usage_hackathon/index.html",
    "title": "Enformer usage hackathon demo",
    "section": "",
    "text": "Authors: Saideep Gona, Temidayo Adeluwa\nAcknowledgement: - Boxiang Liu - Festus Nyasimi (for providing us with Predixcan predictions)\nDate: Saturday April 2, 2022\nCopyright 2021 DeepMind Technologies Limited\nLicensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n https://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
  },
  {
    "objectID": "posts/enformer_usage_hackathon/index.html#genetic-medicine-deep-learning-hackathon-2022",
    "href": "posts/enformer_usage_hackathon/index.html#genetic-medicine-deep-learning-hackathon-2022",
    "title": "Enformer usage hackathon demo",
    "section": "",
    "text": "Authors: Saideep Gona, Temidayo Adeluwa\nAcknowledgement: - Boxiang Liu - Festus Nyasimi (for providing us with Predixcan predictions)\nDate: Saturday April 2, 2022\nCopyright 2021 DeepMind Technologies Limited\nLicensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n https://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
  },
  {
    "objectID": "posts/enformer_usage_hackathon/index.html#introduction",
    "href": "posts/enformer_usage_hackathon/index.html#introduction",
    "title": "Enformer usage hackathon demo",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, we explore how Enformer can be used to predict the expression of protein-coding genes. We utilized some code from the original Enformer usage colab notebook. Here, we showcase how the Enformer model can be used to predict gene expression on a GEUVADIS/1000 genomes dataset, and compare the predictions with true expression.\n“Effective gene expression prediction from sequence by integrating long-range interactions”\nŽiga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, David R. Kelley\n\nSteps\nThis notebook demonstrates how to - Prepare inputs for Enformer to make predictions - Make predictions with Enformer and produce figures - Compare predictions with true expression"
  },
  {
    "objectID": "posts/enformer_usage_hackathon/index.html#setup",
    "href": "posts/enformer_usage_hackathon/index.html#setup",
    "title": "Enformer usage hackathon demo",
    "section": "Setup",
    "text": "Setup\nGoogle Colab gives us some GPU access. This limited GPU is available to anyone with a Google account, who has signed up to use Colaboratory. We will begin by changing the runtime type to GPU. Follow the instruction below by clicking on “Runtime -&gt; Change runtime type -&gt; GPU” in the menu bar below the title of this notebook.\nStart the colab kernel with GPU: Runtime -&gt; Change runtime type -&gt; GPU\nBelow, we import tensorflow as tf, and check that the runtime has been changed to GPU.\n\nimport tensorflow as tf\n# Make sure the GPU is enabled\nassert tf.config.list_physical_devices('GPU'), 'Start the colab kernel with GPU: Runtime -&gt; Change runtime type -&gt; GPU'\n\n2023-07-06 02:32:57.968916: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nkipoiseq is a package that helps us to extract sequences from fasta files given some intervals. We will install the package.\n\n#%pip install kipoiseq==0.5.2 --quiet &gt; /dev/null\n# You can ignore the pyYAML error\n\nBiopython is a python package that helps us do many bioinfomatic analysis in python\n\n#%pip install Biopython\n\n\nSetting up our environments\nWe need to have some packages imported to help us do cool stuff.\n\nimport tensorflow_hub as hub # for interacting with saved models and tensorflow hub\nimport joblib\nimport gzip # for manipulating compressed files\nimport kipoiseq # for manipulating fasta files\nfrom kipoiseq import Interval # same as above, really\nimport pyfaidx # to index our reference genome file\nfrom pyfaidx import Fasta\nimport pandas as pd # for manipulating dataframeshttps://github.com/microsoft/pyright/blob/main/docs/configuration.md#reportMissingImports\nimport numpy as np # for numerical computations\nimport matplotlib.pyplot as plt # for plotting\nimport matplotlib as mpl # for plotting\nimport seaborn as sns # for plotting\nimport pickle # for saving large objects\nimport os, sys # functions for interacting with the operating system\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nNext,\nWe want to define some paths to save downloaded files for the duration of this notebook. These will be wiped off by Google as soon as we are done.\n\ntransform_path = 'gs://dm-enformer/models/enformer.finetuned.SAD.robustscaler-PCA500-robustscaler.transform.pkl'\nmodel_path = 'https://tfhub.dev/deepmind/enformer/1'\nfasta_file = '/grand/TFXcan/imlab/users/tiffanie/enformer/data/genome.fa'\n\nWe may inspect the tracks used to train the model. The CAGE prediction corresponding to B lymphoblastoid cell line is index 5110. We use B lymphoblastoid cell line predictions here because that is the cell line used to generate GEUVADIS gene expression data. You can copy the https link, paste in another tab in your browser and look through the large txt file for other tracks.\n\n# Download targets from Basenji2 dataset\n# Cite: Kelley et al Cross-species regulatory sequence activity prediction. PLoS Comput. Biol. 16, e1008050 (2020).\ntargets_txt = 'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_human.txt'\ndf_targets = pd.read_csv(targets_txt, sep='\\t')\ndf_targets[df_targets.index==5110]\n\n\n\n\n\n\n\n\nindex\ngenome\nidentifier\nfile\nclip\nscale\nsum_stat\ndescription\n\n\n\n\n5110\n5110\n0\nCNhs12333\n/home/drk/tillage/datasets/human/cage/fantom/C...\n384\n1\nsum\nCAGE:B lymphoblastoid cell line: GM12878 ENCOD...\n\n\n\n\n\n\n\n\n\nDownload files\nWe need to download some files. Give it a moment. We will download the following files: - The reference genome fasta file (we will also index this file in the process) - A text file for the transcription start sites for each chromosome - Per chromosome files that has annotation for the genes - A compressed file that contains the variant bed files for the genes and their locations.\nCredit to Genome Reference Consortium: https://www.ncbi.nlm.nih.gov/grc\nSchneider et al 2017 http://dx.doi.org/10.1101/gr.213611.116: Evaluation of GRCh38 and de novo haploid genome assemblies demonstrates the enduring quality of the reference assembly\nMake a data directory, and download the necessary bed files and chromosome annotation files\nNB: You may decide to download these files into your “/content/drive/MyDrive/Enformer_Hackathon_2022/” directory. You don’t need to do this. But if you want permanent access to the files we use in this notebook, you can change the path from “/grand/TFXcan/imlab/users/tiffanie/enformer/data/” to “/content/drive/MyDrive/Enformer_Hackathon_2022/”, and modify what you need accordingly.\nThe next line of code will download the reference genome fasta file and index this file.\n\n# reference genome and indexed\n!wget -O - https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz | gunzip -c &gt; {fasta_file}\npyfaidx.Faidx(fasta_file)\n\n--2023-07-06 02:33:16--  https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz\nResolving proxy.alcf.anl.gov (proxy.alcf.anl.gov)... 140.221.69.42\nConnecting to proxy.alcf.anl.gov (proxy.alcf.anl.gov)|140.221.69.42|:3128... connected.\n\n\nProxy request sent, awaiting response... 200 OK\nLength: 948731419 (905M) [application/x-gzip]\nSaving to: ‘STDOUT’\n\n-                   100%[===================&gt;] 904.78M  52.1MB/s    in 17s     \n\n2023-07-06 02:33:33 (54.0 MB/s) - written to stdout [948731419/948731419]\n\n\n\nFaidx(\"/grand/TFXcan/imlab/users/tiffanie/enformer/data/genome.fa\")\n\n\nThe next lines of code will download the variation bed files, and we have created links to help us download the variation bed files for each chromosome, for each gene.\n\nchrom_bed_downloads = pd.read_csv(\"https://uchicago.box.com/shared/static/du77wf31li38tciv8imivwu57svae03p.csv\")\nchrom_bed_downloads.index = chrom_bed_downloads[\"chroms\"]\n\nchrom_bed_downloads.head(5)\n\n\n\n\n\n\n\n\nchroms\nlink\n\n\nchroms\n\n\n\n\n\n\n1\n1\nhttps://uchicago.box.com/shared/static/9q9n4a0...\n\n\n2\n2\nhttps://uchicago.box.com/shared/static/1tk6a3f...\n\n\n3\n3\nhttps://uchicago.box.com/shared/static/77ldwqq...\n\n\n4\n4\nhttps://uchicago.box.com/shared/static/s0g48al...\n\n\n5\n5\nhttps://uchicago.box.com/shared/static/yafgxb1...\n\n\n\n\n\n\n\nWe will define a function to help us download bed variation files for a given gene or list of genes\n\ndef download_chrom_beds(chromosome, genes, downloads_table=chrom_bed_downloads):\n  '''\n  Downloads bed/variation files for a chromosome and list of genes\n  '''\n\n  link = downloads_table.loc[str(chromosome), \"link\"]\n  chr_which = 'chr' + chromosome\n  for gene in genes:\n    if os.path.exists('/grand/TFXcan/imlab/users/tiffanie/enformer/data/individual_beds/chr' + chromosome + '/chr' + chromosome + '_' + gene + '.bed'): # if the file is in the folder, no need to download again\n      continue\n    !curl -L {link} --output /grand/TFXcan/imlab/users/tiffanie/enformer/data/chr_{chromosome}_bed.tar.gz && cd /grand/TFXcan/imlab/users/tiffanie/enformer/data/ && tar -zxf /grand/TFXcan/imlab/users/tiffanie/enformer/data/chr_{chromosome}_bed.tar.gz ./individual_beds/{chr_which}/{chr_which}_{gene}.bed\n\n    # remove the download tar.gz file\n    !rm /grand/TFXcan/imlab/users/tiffanie/enformer/data/chr_{chromosome}_bed.tar.gz\n\nWe don’t need this function yet. But we can test out how it works.\nAssuming we want to download the variation files for ‘ERAP1’, which is located on chromosome 5…\nThis will download the bed file into /grand/TFXcan/imlab/users/tiffanie/enformer/data/individual_beds/chr5/\n\ndownload_chrom_beds(chromosome = '5', genes=['ERAP1', 'ERAP2'])\n\nAnd when you need the file, you can read it in like…\n\nerap1_variations = pd.read_table('/grand/TFXcan/imlab/users/tiffanie/enformer/data/individual_beds/chr5/chr5_ERAP1.bed', sep='\\t')\nerap1_variations.head(5)\n\n\n\n\n\n\n\n\n#CHROM\nPOS\nREF\nALT\nHG00096\nHG00097\nHG00099\nHG00100\nHG00101\nHG00102\n...\nNA20810\nNA20811\nNA20812\nNA20813\nNA20814\nNA20815\nNA20816\nNA20819\nNA20826\nNA20828\n\n\n\n\n0\n5\n95923584\nT\nC\n1|0\n0|1\n0|0\n0|0\n0|1\n0|1\n...\n0|0\n1|0\n1|0\n0|0\n0|1\n0|0\n0|0\n0|1\n0|0\n1|0\n\n\n1\n5\n95923823\nA\nG\n1|0\n0|1\n0|0\n0|0\n0|1\n0|1\n...\n0|0\n1|0\n1|0\n0|0\n0|1\n0|0\n0|0\n0|1\n0|0\n1|0\n\n\n2\n5\n95923836\nG\nA\n1|0\n0|1\n0|0\n0|0\n0|1\n0|1\n...\n0|0\n1|0\n1|0\n0|0\n0|1\n0|0\n0|0\n0|1\n0|0\n1|0\n\n\n3\n5\n95924552\nT\nC\n1|0\n0|1\n0|0\n0|0\n0|1\n0|1\n...\n0|0\n1|0\n1|0\n0|0\n0|1\n0|0\n0|0\n0|1\n0|0\n1|0\n\n\n4\n5\n95925045\nT\nA\n1|0\n0|1\n0|0\n0|0\n0|1\n0|1\n...\n0|0\n1|0\n1|0\n0|0\n0|1\n0|0\n0|0\n0|1\n0|0\n1|0\n\n\n\n\n5 rows × 459 columns\n\n\n\nYou can pass in a list of genes as long as they are all located on that chromosome.\nIn the next block of code, we download the TSS for each chromosome and the genes in that chromosome, as wells as the per chromosome gene annotations. We need this information to estimate predictions.\n\n!curl -L https://uchicago.box.com/shared/static/perc3uabzzd267cbp8zc0inwgrmur7pu.gz --output /grand/TFXcan/imlab/users/tiffanie/enformer/data/chr_tss.tar.xz && cd /grand/TFXcan/imlab/users/tiffanie/enformer/data/ && tar -zxf /grand/TFXcan/imlab/users/tiffanie/enformer/data/chr_tss.tar.xz\n\n!mkdir -p /grand/TFXcan/imlab/users/tiffanie/enformer/data/gene_chroms #creates a folder to hold our files\n!curl -L https://uchicago.box.com/shared/static/e2kiwrjlgqqio0pc37a2iz7l5bqbv57u.gz --output /grand/TFXcan/imlab/users/tiffanie/enformer/data/gene_chroms/gene_chroms.tar.gz && cd /grand/TFXcan/imlab/users/tiffanie/enformer/data/gene_chroms/ && tar -zxf /grand/TFXcan/imlab/users/tiffanie/enformer/data/gene_chroms/gene_chroms.tar.gz\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100     7    0     7    0     0      6      0 --:--:--  0:00:01 --:--:--  7000\n100 1783k  100 1783k    0     0   978k      0  0:00:01  0:00:01 --:--:-- 4078k\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100     6    0     6    0     0      6      0 --:--:-- --:--:-- --:--:--    11\n100  728k  100  728k    0     0   575k      0  0:00:01  0:00:01 --:--:--  575k\n\n\n\n\nHow do we want to go about using Enformer given all these files we just downloaded?\nAs we know, enformer’s input is a single strand genome sequence. Yet, we are interested in predicting on population level data which includes individual-specific variation. To get around this limitation, we will treat each individual as the sum of their haplotypes. Using the phased variant data around each gene (stored in the variant bed files) to modify the reference sequence, we can create two distinct haplotype sequences for each individual. The sum of both of Enformer’s haplotype predictions serves as an individual-specific, additive estimate which we can correlate with true predictions. Together, the files we downloaded give us all the information we need to build these haplotype sequences.\nAlthought enformer predicts a wide array of functional output, we will focus here on gene expression in lymphoblastoid cells allowing for correlation against ground truth Geuvadis gene expression data.\nThere are many functions that we have defined in the next code block. You can explore them later, but for now, simply run the block by clicking on the play button.\n\n\nCode\nNext, we have some functions that will help us along the way. Classes and methods defined in this code block can be found in the original Enformer usage colab notebook.\n\n# @title `Enformer`, `EnformerScoreVariantsNormalized`, `EnformerScoreVariantsPCANormalized`,\nSEQUENCE_LENGTH = 393216\n\nclass Enformer:\n\n  def __init__(self, tfhub_url):\n    self._model = hub.load(tfhub_url).model\n\n  def predict_on_batch(self, inputs):\n    predictions = self._model.predict_on_batch(inputs)\n    return {k: v.numpy() for k, v in predictions.items()}\n\n  @tf.function\n  def contribution_input_grad(self, input_sequence,\n                              target_mask, output_head='human'):\n    input_sequence = input_sequence[tf.newaxis]\n\n    target_mask_mass = tf.reduce_sum(target_mask)\n    with tf.GradientTape() as tape:\n      tape.watch(input_sequence)\n      prediction = tf.reduce_sum(\n          target_mask[tf.newaxis] *\n          self._model.predict_on_batch(input_sequence)[output_head]) / target_mask_mass\n\n    input_grad = tape.gradient(prediction, input_sequence) * input_sequence\n    input_grad = tf.squeeze(input_grad, axis=0)\n    return tf.reduce_sum(input_grad, axis=-1)\n\n\nclass EnformerScoreVariantsRaw:\n\n  def __init__(self, tfhub_url, organism='human'):\n    self._model = Enformer(tfhub_url)\n    self._organism = organism\n\n  def predict_on_batch(self, inputs):\n    ref_prediction = self._model.predict_on_batch(inputs['ref'])[self._organism]\n    alt_prediction = self._model.predict_on_batch(inputs['alt'])[self._organism]\n\n    return alt_prediction.mean(axis=1) - ref_prediction.mean(axis=1)\n\n\nclass EnformerScoreVariantsNormalized:\n\n  def __init__(self, tfhub_url, transform_pkl_path,\n               organism='human'):\n    assert organism == 'human', 'Transforms only compatible with organism=human'\n    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n      transform_pipeline = joblib.load(f)\n    self._transform = transform_pipeline.steps[0][1]  # StandardScaler.\n\n  def predict_on_batch(self, inputs):\n    scores = self._model.predict_on_batch(inputs)\n    return self._transform.transform(scores)\n\n\nclass EnformerScoreVariantsPCANormalized:\n\n  def __init__(self, tfhub_url, transform_pkl_path,\n               organism='human', num_top_features=500):\n    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n      self._transform = joblib.load(f)\n    self._num_top_features = num_top_features\n\n  def predict_on_batch(self, inputs):\n    scores = self._model.predict_on_batch(inputs)\n    return self._transform.transform(scores)[:, :self._num_top_features]\n\n\n# TODO(avsec): Add feature description: Either PCX, or full names.\n\n\n# @title `variant_centered_sequences`\n\nclass FastaStringExtractor:\n\n    def __init__(self, fasta_file):\n        self.fasta = pyfaidx.Fasta(fasta_file)\n        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n\n    def extract(self, interval: Interval, **kwargs) -&gt; str:\n        # Truncate interval if it extends beyond the chromosome lengths.\n        chromosome_length = self._chromosome_sizes[interval.chrom]\n        trimmed_interval = Interval(interval.chrom,\n                                    max(interval.start, 0),\n                                    min(interval.end, chromosome_length),\n                                    )\n        # pyfaidx wants a 1-based interval\n        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n                                          trimmed_interval.start + 1,\n                                          trimmed_interval.stop).seq).upper()\n        # Fill truncated values with N's.\n        pad_upstream = 'N' * max(-interval.start, 0)\n        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n        return pad_upstream + sequence + pad_downstream\n\n    def close(self):\n        return self.fasta.close()\n\n\ndef one_hot_encode(sequence):\n  return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n\n\n\n# @title `plot_tracks`\n\ndef plot_tracks(tracks, interval, height=1.5):\n  fig, axes = plt.subplots(len(tracks), 1, figsize=(20, height * len(tracks)), sharex=True)\n  for ax, (title, y) in zip(axes, tracks.items()):\n    ax.fill_between(np.linspace(interval.start, interval.end, num=len(y)), y)\n    ax.set_title(title)\n    sns.despine(top=True, right=True, bottom=True)\n  ax.set_xlabel(str(interval))\n  plt.tight_layout()\n\nHere, we define some utility functions for ourselves, to help us make predictions and analyse our predictions.\n\nimport Bio\n\nfrom Bio.Seq import Seq\ndef create_rev_complement(dna_string):\n    return(str(Seq(dna_string).reverse_complement()))\n\n\ndef prepare_for_quantify_prediction_per_TSS(predictions, gene, tss_df):\n\n  '''\n\n  Parameters:\n          predicitions (A numpy array): All predictions from the track\n          gene (a gene name, character): a gene\n          tss_df: a list of dataframe of genes and their transcription start sites\n  Returns:\n          A dictionary of cage experiment predictions and a list of transcription start sites\n\n  '''\n\n  output = dict()\n  for tdf in tss_df:\n    if gene not in tdf.genes.values:\n      continue\n    gene_tss_list = tdf[tdf.genes == gene].txStart_Sites.apply(str).values\n    gene_tss_list = [t.split(', ') for t in gene_tss_list]\n    gene_tss_list = [int(item) for nestedlist in gene_tss_list for item in nestedlist]\n    gene_tss_list = list(set(gene_tss_list))\n  output['cage_predictions'] = predictions[:, 5110] # a numpy array\n  output['gene_TSS'] = gene_tss_list # a list\n\n\n  return(output) # a dictionary\n\ndef quantify_prediction_per_TSS(low_range, TSS, cage_predictions):\n\n  '''\n  Parameters:\n          low_range (int): The lower interval\n          TSS (list of integers): A list of TSS for a gene\n          cage_predictions: A 1D numpy array or a vector of predictions from enformer corresponding to track 5110 or CAGE predictions\n  Returns:\n          A dictionary of gene expression predictions for each TSS for a gene\n    '''\n  tss_predictions = dict()\n  for tss in TSS:\n    bin_start = low_range + ((768 + 320) * 128)\n    count = -1\n    while bin_start &lt; tss:\n      bin_start = bin_start + 128\n      count += 1\n    if count &gt;= len(cage_predictions)-1:\n      continue\n    cage_preds = cage_predictions[count - 1] + cage_predictions[count] + cage_predictions[count + 1]\n    tss_predictions[tss] = cage_preds\n\n  return(tss_predictions)\n\ndef collect_intervals(chromosomes = [\"22\"], gene_list=None):\n\n  '''\n    Parameters :\n      chromosomes : a list of chromosome numbers; each element should be a string format\n      gene_list : a list of genes; the genes should be located on those chromosomes\n\n    Returns :\n      A dictionary of genes (from gene_list) and their intervals within their respective chromosomes\n  '''\n\n  gene_intervals = {} # Collect intervals for our genes of interest\n\n  for chrom in chromosomes:\n    with open(\"/grand/TFXcan/imlab/users/tiffanie/enformer/data/gene_chroms/gene_\"+ chrom + \".txt\", \"r\") as chrom_genes:\n      for line in chrom_genes:\n        split_line = line.strip().split(\"\\t\")\n        gene_intervals[split_line[2]] = [\n                                          split_line[0],\n                                          int(split_line[3]),\n                                          int(split_line[4])\n                                        ]\n\n  if isinstance(gene_list, list): # if the user has supplied a list of genes they are interested in\n    use_genes = dict((k, gene_intervals[k]) for k in gene_list if k in gene_intervals)\n    return(use_genes)\n  elif isinstance(gene_list, type(None)):\n    return(gene_intervals)\n\n\ndef run_predictions(gene_intervals, tss_dataframe, individuals_list=None):\n  '''\n  Parameters :\n    gene_intervals : the results from calling `collect_intervals`\n    tss_dataframe : a list of the TSSs dataframes i.e. the TSS for the genes in the chromosomes\n    individuals_list : a list of individuals on which we want to make predictions; defaults to None\n\n  Returns :\n    A list of predictions; the first element is the predictions around the TSS for each gene. The second is the prediction across CAGE tracks\n  '''\n\n  gene_output = dict()\n  gene_predictions = dict()\n\n  for gene in gene_intervals.keys():\n    gene_interval = gene_intervals[gene]\n    target_interval = kipoiseq.Interval(\"chr\" + gene_interval[0],\n                                        gene_interval[1],\n                                        gene_interval[2]) # creates an interval to select the right sequences\n    target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))  # extracts the fasta sequences, and resizes such that it is compatible with the sequence_length\n    window_coords = target_interval.resize(SEQUENCE_LENGTH) # we also need information about the start and end locations after resizing\n    try:\n      cur_gene_vars = pd.read_csv(\"/grand/TFXcan/imlab/users/tiffanie/enformer/data/individual_beds/chr\" + gene_interval[0] + \"/chr\" + gene_interval[0] + \"_\"+ gene + \".bed\", sep=\"\\t\", header=0) # read in the appropriate bed file for the gene\n    except:\n      continue\n    individual_results = dict()\n    individual_prediction = dict()\n\n    if isinstance(individuals_list, list) or isinstance(individuals_list, type(np.empty([1, 1]))):\n      use_individuals = individuals_list\n    elif isinstance(individuals_list, type(None)):\n      use_individuals = cur_gene_vars.columns[4:]\n\n    for individual in use_individuals:\n      print('Currently on gene {}, and predicting on individual {}...'.format(gene, individual))\n      # two haplotypes per individual\n      haplo_1 = list(target_fa[:])\n      haplo_2 = list(target_fa[:])\n\n      ref_mismatch_count = 0\n      for i,row in cur_gene_vars.iterrows():\n\n        geno = row[individual].split(\"|\")\n        if (row[\"POS\"]-window_coords.start-1) &gt;= len(haplo_2):\n          continue\n        if (row[\"POS\"]-window_coords.start-1) &lt; 0:\n          continue\n        if geno[0] == \"1\":\n          haplo_1[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n        if geno[1] == \"1\":\n          haplo_2[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n\n      # predict on the individual's two haplotypes\n      prediction_1 = model.predict_on_batch(one_hot_encode(\"\".join(haplo_1))[np.newaxis])['human'][0]\n      prediction_2 = model.predict_on_batch(one_hot_encode(\"\".join(haplo_2))[np.newaxis])['human'][0]\n\n      temp_predictions = [prediction_1[:, 5110], prediction_2[:, 5110]] # CAGE predictions we are interested in\n      individual_prediction[individual] = temp_predictions\n\n      # Calculate TSS CAGE expression which correspond to column 5110 of the predictions above\n      temp_list = list()\n\n      pred_prepared_1 = prepare_for_quantify_prediction_per_TSS(predictions=prediction_1, gene=gene, tss_df=tss_dataframe)\n      tss_predictions_1 = quantify_prediction_per_TSS(low_range = window_coords.start, TSS=pred_prepared_1['gene_TSS'], cage_predictions=pred_prepared_1['cage_predictions'])\n\n      pred_prepared_2 = prepare_for_quantify_prediction_per_TSS(predictions=prediction_2, gene=gene, tss_df=tss_dataframe)\n      tss_predictions_2 = quantify_prediction_per_TSS(low_range = window_coords.start, TSS=pred_prepared_2['gene_TSS'], cage_predictions=pred_prepared_2['cage_predictions'])\n\n      temp_list.append(tss_predictions_1)\n      temp_list.append(tss_predictions_2) # results here are a dictionary for each TSS for each haplotype\n\n      individual_results[individual] = temp_list # save for the individual\n\n    gene_output[gene] = individual_results\n    gene_predictions[gene] = individual_prediction\n\n  return([gene_output, gene_predictions])\n\n\ndef collect_target_intervals(gene_intervals):\n\n  '''\n  Returns a dictionary of Interval objects (from kipoiseq) for each gene corresponding to the locations of the gene\n  '''\n\n  target_intervals_dict = dict()\n\n  for gene in gene_intervals.keys():\n    gene_interval = gene_intervals[gene]\n    target_interval = kipoiseq.Interval(\"chr\" + gene_interval[0],\n                                        gene_interval[1],\n                                        gene_interval[2])\n    target_intervals_dict[gene] = target_interval\n\n  return(target_intervals_dict)\n\ndef prepare_for_plot_tracks(gene, individual, all_predictions, chromosome=['22']):\n\n  '''\n  This returns a dictionary of gene tracks and gene intervals, prepared for the function plot_tracks.\n\n  Parameters:\n    - gene\n    - individual\n    - all_predictions\n  '''\n\n  haplo_predictions = all_predictions[gene][individual]\n  gene_tracks = {gene + ' | ' + individual + ' | haplotype 1': np.log10(1 + haplo_predictions[0]),\n                gene + ' | ' + individual + ' | haplotype 2': np.log10(1 + haplo_predictions[1])}\n\n  gene_intervals = collect_intervals(chromosomes=chromosome, gene_list=[gene])\n  gene_intervals = collect_target_intervals(gene_intervals)\n\n  output = dict()\n  output['gene_tracks'] = gene_tracks\n  output['gene_intervals'] = gene_intervals[gene]\n\n  return(output)\n\ndef check_individuals(path_to_bed_file, list_of_individuals):\n\n  '''\n  Checks if an individual is missing in bed variation files.\n  These individuals should be removed prior to training\n  '''\n\n  myfile = open(path_to_bed_file, 'r')\n  myline = myfile.readline()\n  bed_names = myline.split('\\t')[4:]\n  myfile.close()\n\n  if set(list_of_individuals).issubset(set(bed_names)) == False:\n    missing = list(set(list_of_individuals).difference(bed_names))\n    print('This (or these) individual(s) is/are not present: {}'.format(missing))\n  else:\n    missing = []\n    print('All individuals are present in the bed file.')\n\n  return(missing)\n\n\ndef plot_predixcan_vs_geuvadis(interested_gene, interested_individuals, geuvadis_expression, predixcan_expression):\n\n  '''\n  Show a plot and return correlation coefficient\n  '''\n  # from predixcan expression\n  df_predixcan = predixcan_expression[predixcan_expression.gene_name == interested_gene].loc[:,interested_individuals]\n  # from enformer\n  df_geuvadis = geuvadis_expression[geuvadis_expression.gene_name == interested_gene].loc[:,interested_individuals]\n\n  # concatenate both\n  df_all = pd.concat([df_predixcan, df_geuvadis], axis=0)\n  df_all.index = ['Predixcan', 'GEUVADIS']\n\n  # plotting\n  sns.regplot(x=df_all.iloc[0,:], y=df_all.iloc[1,:], color='red').set(title='Predixcan vs. GEUVADIS predictions on {} individuals for gene {}'.format(len(df_all.columns), interested_gene))\n\n  # correlation coefficient\n  corr_coef = np.corrcoef(x=df_all.iloc[0,:], y=df_all.iloc[1,:])[0][1]\n\n  return([df_all, corr_coef])\n\ndef plot_enformer_vs_predixcan(prediction_results, interested_gene, interested_individuals, predixcan_expression, how='sum'):\n\n  '''\n  Show a plot and return correlation coefficient\n  '''\n\n  enformer_predictions = dict()\n\n  for gene, individuals in prediction_results[0].items():\n    temp_individual = dict()\n    for individual, haplo_predictions in individuals.items():\n      temp = list()\n      for i in range(0, len(haplo_predictions[0])):\n        temp.append(list(haplo_predictions[0].values())[i] + list(haplo_predictions[1].values())[i])\n      if how == 'sum':\n        temp_individual[individual] = np.sum(temp)\n      elif how == 'max':\n        temp_individual[individual] = np.max(temp)\n    enformer_predictions[gene] = temp_individual\n\n  # from predixcan expression\n  df_predixcan = predixcan_expression[predixcan_expression.gene_name == interested_gene].loc[:,interested_individuals]\n  # from enformer\n  df_enformer = pd.DataFrame(enformer_predictions[interested_gene], index=[0]).loc[:, df_predixcan.columns]\n\n  # concatenate both\n  df_all = pd.concat([df_enformer, df_predixcan], axis=0)\n  df_all.index = ['Enformer', 'Predixcan']\n\n  # plotting\n  sns.regplot(x=df_all.iloc[0,:], y=df_all.iloc[1,:], color='red').set(title='Predixcan vs. Enformer predictions on {} individuals for gene {}'.format(len(df_all.columns), interested_gene))\n\n  # correlation coefficient\n  corr_coef_predix = np.corrcoef(x=df_all.iloc[0,:], y=df_all.iloc[1,:])[0][1]\n\n  return([df_all, corr_coef_predix])\n\n\ndef plot_enformer_vs_geuvadis(prediction_results, interested_gene, interested_individuals, geuvadis_expression, how='sum'):\n\n  '''\n  Show a plot and return correlation coefficient\n  '''\n\n  enformer_predictions = dict()\n\n  for gene, individuals in prediction_results[0].items():\n    temp_individual = dict()\n    for individual, haplo_predictions in individuals.items():\n      temp = list()\n      for i in range(0, len(haplo_predictions[0])):\n        temp.append(list(haplo_predictions[0].values())[i] + list(haplo_predictions[1].values())[i])\n      if how == 'sum':\n        temp_individual[individual] = np.sum(temp)\n      elif how == 'max':\n        temp_individual[individual] = np.max(temp)\n    enformer_predictions[gene] = temp_individual\n\n  # from geuvadis expression\n  df_geuvadis = geuvadis_expression[geuvadis_expression.gene_name == interested_gene].loc[:,interested_individuals]\n  #df_enformer = np.transpose(pd.DataFrame(enformer_predictions)).loc[:, df_geuvadis.columns]\n  df_enformer = pd.DataFrame(enformer_predictions[interested_gene], index=[0]).loc[:, df_geuvadis.columns]\n\n  # concatenate both\n  df_all = pd.concat([df_enformer, df_geuvadis], axis=0)\n  df_all.index = ['Enformer', 'GEUVADIS']\n\n  # plotting\n  sns.regplot(x=df_all.iloc[0,:], y=df_all.iloc[1,:], color='blue').set(title='Enformer vs. Geuvadis predictions on {} individuals for gene {}'.format(len(df_all.columns), interested_gene))\n\n  # correlation coefficient\n  corr_coef_geu = np.corrcoef(x=df_all.iloc[0,:], y=df_all.iloc[1,:])[0][1]\n\n  return([df_all, corr_coef_geu])"
  },
  {
    "objectID": "posts/enformer_usage_hackathon/index.html#make-predictions-on-the-geuvadis-dataset.",
    "href": "posts/enformer_usage_hackathon/index.html#make-predictions-on-the-geuvadis-dataset.",
    "title": "Enformer usage hackathon demo",
    "section": "Make predictions on the GEUVADIS dataset.",
    "text": "Make predictions on the GEUVADIS dataset.\nHere, we will begin to make predictions. Excited?!\nWe still need the model itself. The model has been graciously hosted on Tensorflow Hub, which hosts many other models too. You can click on the link and explore. When you click the link, you can see that the model is about 892 Mb large. Quite big. We will use the url to the model to download and use it here.\nEarlier, we defined an Enformer class (see the codes section). We will load the model into this class. The model has been trained and the weights are freely available. All we need to do is to load this model and use it. Neat.\nWe also defined a class FastaStringExtractor, that can help us extract raw sequences from fasta files given the intervals we want. We will make use of this class too.\n\nmodel = Enformer(model_path) # here we load the model architecture.\nfasta_extractor = FastaStringExtractor(fasta_file) # we define a class called fasta_extractor to help us extra raw sequence data\n\n\nEXERCISE 1:\nFor evaluation, we need to sum the predictions around each unique TSS for a given gene. We will be using this a lot so it is important that we define what it means. Essentially, for a gene with one TSS, we take the sum of predicitions of the 128 bp output bin containing the TSS and its two immediate neighboring bins. We do this for each haplotype and each TSS to give TSS-level predictions.\nTo get individual-level estimates for a whole gene, we sum each haplotype TSS estimate to summarize TSS-level predictions per individual, and then take either the sum or max of TSS-level predictions to summarize at the gene level.\nThere are many genes and many individuals in our datasets. To make illustration simpler, we will use four genes, ERAP1, ERAP2, NUDT2, and PEX6, located on chromosome 5, 5, 9, and 6 respectively. We will use predictions for 10 randomly selected individuals located in the bed files.\n\ndownload_chrom_beds(chromosome = \"5\", genes = ['ERAP1', 'ERAP2'])\ndownload_chrom_beds(chromosome = \"9\", genes = ['NUDT2'])\ndownload_chrom_beds(chromosome = \"6\", genes = ['PEX6'])\n\nHere, we read into a dataframe the TSS (transcription start sites) per gene for the chromosomes we are interested in. The dataframe has three columns. The first contains the genes, and the second contains the TSS(s) for that gene, and the third contains the strand information. We are interested in genes located on chromosomes 5, 6 and 9.\n\nchr5_tss = pd.read_table('/grand/TFXcan/imlab/users/tiffanie/enformer/data/tss_by_chr/chr5_tss_by_gene.txt', sep='\\t')\nchr6_tss = pd.read_table('/grand/TFXcan/imlab/users/tiffanie/enformer/data/tss_by_chr/chr6_tss_by_gene.txt', sep='\\t')\nchr9_tss = pd.read_table('/grand/TFXcan/imlab/users/tiffanie/enformer/data/tss_by_chr/chr9_tss_by_gene.txt', sep='\\t')\n\nchr9_tss.head(10)\n\n\n\n\n\n\n\n\ngenes\ntxStart_Sites\nstrand\n\n\n\n\n0\nDDX11L5\n11987, 12134\n+\n\n\n1\nXXyac-YRM2039.2\n29739, 25007, 19145, 29259\n-\n\n\n2\nMIR1302-11\n27657\n+\n\n\n3\nMIR1302-9\n30144\n+\n\n\n4\nFAM138C\n35841, 35856\n-\n\n\n5\nRP11-143M1.7\n52680\n-\n\n\n6\nRP11-143M1.2\n72701, 72706, 72716\n+\n\n\n7\nRP11-143M1.3\n102850, 102941\n-\n\n\n8\nRP11-143M1.4\n113754\n-\n\n\n9\nFOXD4\n118417\n-\n\n\n\n\n\n\n\n\nPreparing inputs for Enformer\nNow that we have downloaded the genetic information that we need, we want to prepare the inputs for Enformer.\nWe need the following - The genes we want to predict for - The genomic interval for these genes - Information about the transcription start sites for these genes - The individuals we want to predict for\nWe have a utility function that helps to define the intervals of a gene, and resize this interval to make it acceptable for Enformer. Enformer needs a specific, defined sequence length. We use the collect_intervals function. The result is a dictionary that contains chromosome and interval information for each gene.\nFor example, let’s explore ERAP1…\n\nERAP1_intervals = collect_intervals(chromosomes=['5'], gene_list=['ERAP1'])\nERAP1_target_intervals = collect_target_intervals(ERAP1_intervals)\nERAP1_intervals, ERAP1_target_intervals\n\n({'ERAP1': ['5', 96096521, 96143803]},\n {'ERAP1': Interval(chrom='chr5', start=96096521, end=96143803, name='', strand='.', ...)})\n\n\nERAP1_target_intervals is an Interval object created using the kipoiseq package we installed earlier. It is used during predictions, and we don’t need to know the methods of this object for the purpose of the next questions.\nHowever, we have similar information in ERAP1_intervals, which is a python dictionary of lists. For the questions below, we will use the ERAP1_intervals object.\n\n\nQuestion 1a\nWhat is the size of this interval? Hint: Look at the ERAP1_intervals, and remember that Python is 0-based indexed. You need to access the key of this dictionary, which is the gene name, and for the value, which is a list, you can access the first element using 0, the second element using 1, and so on.\n\nERAP1_intervals[\"ERAP1\"][2]-ERAP1_intervals[\"ERAP1\"][1]\n\n47282\n\n\n\n\nNote\nYou can roughly confirm this interval by going to the UCSC genome browser or Ensemble genome browser. We have provided a link for UCSC genome browser’s interval length for ERAP1 here. Click on this link, the answer is right at the top of the browser.\nEnformer takes in a defined sequence length. When we provide a gene and collect its intervals, we need to resize this interval to be acceptable for Enformer. Here, we will use the Intervals object define earlier, ERAP1_target_intervals.\n\nERAP1_target_interval_resized = ERAP1_target_intervals['ERAP1'].resize(SEQUENCE_LENGTH)\nERAP1_target_interval_resized\n\nInterval(chrom='chr5', start=95923554, end=96316770, name='', strand='.', ...)\n\n\n\n\nQuestion 1b\nWhat is the length of this interval? Simply run the next line of code.\n\nERAP1_target_interval_resized.end - ERAP1_target_interval_resized.start\n\n393216\n\n\nEssentially, we resized the length of the gene and pad it with the native sequences to the left and to the right, such that the length of the input sequence is 393216, and we can imagine our gene right at the center of this wider interval. This is the same interval length used to train ENCODE data to build Enformer. Since this value is pre-define, we really cannot change it. This is information that Enformer uses to make very good predictions. Below, we confirm that this is true.\n\n(ERAP1_target_interval_resized.end - ERAP1_target_interval_resized.start) == SEQUENCE_LENGTH\n\nTrue\n\n\n\n\nMaking predictions with Enformer\nWe will select 10 individuals (we have provided 10 randomly sample individuals for ease), and use four genes, ERAP1, ERAP2, NUDT2, and PEX6, located on chromosome 5, 5, 9, and 6 respectively\nWe will collect the intervals that correspond to these genes, collect the sequences for that interval from the reference fasta file, loop through each individual’s variations in the bed files we provided, switch around the variations for each haplotype and predict expression.\nEventually, for each individual, we should have predictions corresponding to each haplotype. We expect that since the haplotypes are different, the predictions should vary too.\nAdditionally, we need the TSS for these genes. Remember that we read in the dataframe earlier.\n\nexercise_1_genes = ['ERAP1', 'NUDT2', 'ERAP2', 'PEX6'] # our gene of interest\n#exercise_1_gene = ['NUDT2', 'ERAP2'] # our gene of interest\n\nexercise_1_individuals = ['NA11992', 'NA19235', 'NA20770', 'HG00232', 'HG00342', 'NA20502', 'NA19189', 'HG00108', 'HG00380', 'NA12872'] # individuals we are interested in\n\nexercise_1_chromosomes = ['5', '9', '6'] # the gene is on chromosome 5\n\nexercise_1_tss_dfs = [chr5_tss, chr9_tss, chr6_tss] # we use the TSS information\n\n\n\nQUESTION 2\nWhat is the id of the 8th individual? Hint: Python used 0-based indexing\n\nprint('The 8th individual is {}'.format(exercise_1_individuals[7])) # your code goes into the ellipsis\n\nThe 8th individual is HG00108\n\n\nIt is possible to have individuals not present in our variation bed files for some reasons. So, we will do some sanity checks.\nUsing the check_individuals functions, we will check if all these individuals are present in the bed file for that gene.\n\nmissing_1 = check_individuals(\"/grand/TFXcan/imlab/users/tiffanie/enformer/data/individual_beds/chr9/chr9_NUDT2.bed\", list_of_individuals = exercise_1_individuals)\nmissing_2 = check_individuals(\"/grand/TFXcan/imlab/users/tiffanie/enformer/data/individual_beds/chr5/chr5_ERAP2.bed\", list_of_individuals = exercise_1_individuals)\nmissing_3 = check_individuals(\"/grand/TFXcan/imlab/users/tiffanie/enformer/data/individual_beds/chr5/chr5_ERAP1.bed\", list_of_individuals = exercise_1_individuals)\nmissing_4 = check_individuals(\"/grand/TFXcan/imlab/users/tiffanie/enformer/data/individual_beds/chr6/chr6_PEX6.bed\", list_of_individuals = exercise_1_individuals)\nmissing_1, missing_2, missing_3, missing_4\n\nAll individuals are present in the bed file.\nAll individuals are present in the bed file.\nAll individuals are present in the bed file.\nAll individuals are present in the bed file.\n\n\n([], [], [], [])\n\n\nIt looks like all the individuals are present. Very nice! We are good to go.\nTo make predictions, we first collect the intervals for the genes we want to predict for.\n\nexercise_1_interval = collect_intervals(chromosomes=exercise_1_chromosomes, gene_list=exercise_1_genes) # here, we collect the intervals for that gene\nexercise_1_interval\n\n{'ERAP1': ['5', 96096521, 96143803],\n 'NUDT2': ['9', 34329504, 34343709],\n 'ERAP2': ['5', 96211643, 96255420],\n 'PEX6': ['6', 42931608, 42946958]}\n\n\nNext, we use the run_predictions function\n\nexercise_1_predictions = run_predictions(gene_intervals=exercise_1_interval, tss_dataframe=exercise_1_tss_dfs, individuals_list=exercise_1_individuals) # here we make predictions and save it.\n\nCurrently on gene ERAP1, and predicting on individual NA11992...\n\n\nCurrently on gene ERAP1, and predicting on individual NA19235...\nCurrently on gene ERAP1, and predicting on individual NA20770...\nCurrently on gene ERAP1, and predicting on individual HG00232...\nCurrently on gene ERAP1, and predicting on individual HG00342...\nCurrently on gene ERAP1, and predicting on individual NA20502...\nCurrently on gene ERAP1, and predicting on individual NA19189...\nCurrently on gene ERAP1, and predicting on individual HG00108...\nCurrently on gene ERAP1, and predicting on individual HG00380...\nCurrently on gene ERAP1, and predicting on individual NA12872...\nCurrently on gene NUDT2, and predicting on individual NA11992...\nCurrently on gene NUDT2, and predicting on individual NA19235...\nCurrently on gene NUDT2, and predicting on individual NA20770...\nCurrently on gene NUDT2, and predicting on individual HG00232...\nCurrently on gene NUDT2, and predicting on individual HG00342...\nCurrently on gene NUDT2, and predicting on individual NA20502...\nCurrently on gene NUDT2, and predicting on individual NA19189...\nCurrently on gene NUDT2, and predicting on individual HG00108...\nCurrently on gene NUDT2, and predicting on individual HG00380...\nCurrently on gene NUDT2, and predicting on individual NA12872...\nCurrently on gene ERAP2, and predicting on individual NA11992...\nCurrently on gene ERAP2, and predicting on individual NA19235...\nCurrently on gene ERAP2, and predicting on individual NA20770...\nCurrently on gene ERAP2, and predicting on individual HG00232...\nCurrently on gene ERAP2, and predicting on individual HG00342...\nCurrently on gene ERAP2, and predicting on individual NA20502...\nCurrently on gene ERAP2, and predicting on individual NA19189...\nCurrently on gene ERAP2, and predicting on individual HG00108...\nCurrently on gene ERAP2, and predicting on individual HG00380...\nCurrently on gene ERAP2, and predicting on individual NA12872...\nCurrently on gene PEX6, and predicting on individual NA11992...\nCurrently on gene PEX6, and predicting on individual NA19235...\nCurrently on gene PEX6, and predicting on individual NA20770...\nCurrently on gene PEX6, and predicting on individual HG00232...\nCurrently on gene PEX6, and predicting on individual HG00342...\nCurrently on gene PEX6, and predicting on individual NA20502...\nCurrently on gene PEX6, and predicting on individual NA19189...\nCurrently on gene PEX6, and predicting on individual HG00108...\nCurrently on gene PEX6, and predicting on individual HG00380...\nCurrently on gene PEX6, and predicting on individual NA12872...\n\n\nNB: If you intend to make predictions across many individuals and genes, it will be faster if you have larger GPU access. For now, we are using limited GPU. So, we have to limit our predictions.\nQuite fast right? Very nice.\nOur prediction object, exercise_1_predictions is a list of length two. - The first item in the list corresponds to the sum of predictions around each unique TSS, for each haplotype, for each individual, for each gene.\n\nThe second item in the list corresponds to the CAGE:B lymphoblastoid cell line predictions across all 128bp bins for each haplotype, for each individual, for the genes. We will use the second item for plotting the tracks.\n\nLet us take a look at the object.\n\nprint(\"The exercise_1_predictions object is a {} of length {}.\".format(type(exercise_1_predictions).__name__, len(exercise_1_predictions)))\n\nThe exercise_1_predictions object is a list of length 2.\n\n\n\nexercise_1_predictions[0]\n\n{'ERAP1': {'NA11992': [{96122464: 1.5851965,\n    96143625: 489.72952,\n    96116885: 5.0453863,\n    96126329: 6.617508,\n    96143642: 489.72952,\n    96143803: 512.877,\n    96143612: 489.72952,\n    96129374: 4.2146845},\n   {96122464: 1.4974236,\n    96143625: 494.15118,\n    96116885: 4.8735304,\n    96126329: 6.5158434,\n    96143642: 494.15118,\n    96143803: 521.8283,\n    96143612: 494.15118,\n    96129374: 4.1429954}],\n  'NA19235': [{96122464: 1.4509864,\n    96143625: 495.32883,\n    96116885: 4.4289474,\n    96126329: 6.330592,\n    96143642: 495.32883,\n    96143803: 522.2405,\n    96143612: 495.32883,\n    96129374: 4.0906773},\n   {96122464: 1.4198803,\n    96143625: 484.1627,\n    96116885: 4.3863516,\n    96126329: 6.0631924,\n    96143642: 484.1627,\n    96143803: 511.7953,\n    96143612: 484.1627,\n    96129374: 4.03969}],\n  'NA20770': [{96122464: 1.4975338,\n    96143625: 494.1459,\n    96116885: 4.8770905,\n    96126329: 6.524975,\n    96143642: 494.1459,\n    96143803: 521.7979,\n    96143612: 494.1459,\n    96129374: 4.1486564},\n   {96122464: 1.5024135,\n    96143625: 486.95276,\n    96116885: 4.827089,\n    96126329: 6.5431585,\n    96143642: 486.95276,\n    96143803: 514.4656,\n    96143612: 486.95276,\n    96129374: 4.200532}],\n  'HG00232': [{96122464: 1.5801065,\n    96143625: 489.637,\n    96116885: 5.0469484,\n    96126329: 6.6298485,\n    96143642: 489.637,\n    96143803: 512.04626,\n    96143612: 489.637,\n    96129374: 4.226439},\n   {96122464: 1.4742824,\n    96143625: 496.64105,\n    96116885: 5.0108047,\n    96126329: 6.502072,\n    96143642: 496.64105,\n    96143803: 524.12646,\n    96143612: 496.64105,\n    96129374: 4.1736445}],\n  'HG00342': [{96122464: 1.4785506,\n    96143625: 489.152,\n    96116885: 4.458046,\n    96126329: 6.4399824,\n    96143642: 489.152,\n    96143803: 511.1334,\n    96143612: 489.152,\n    96129374: 4.1376834},\n   {96122464: 1.4740081,\n    96143625: 496.40088,\n    96116885: 5.0094404,\n    96126329: 6.502212,\n    96143642: 496.40088,\n    96143803: 523.8369,\n    96143612: 496.40088,\n    96129374: 4.17309}],\n  'NA20502': [{96122464: 1.4889812,\n    96143625: 489.89267,\n    96116885: 4.470429,\n    96126329: 6.4531817,\n    96143642: 489.89267,\n    96143803: 511.80463,\n    96143612: 489.89267,\n    96129374: 4.153894},\n   {96122464: 1.4899651,\n    96143625: 489.3021,\n    96116885: 4.458701,\n    96126329: 6.4426136,\n    96143642: 489.3021,\n    96143803: 511.09793,\n    96143612: 489.3021,\n    96129374: 4.1477146}],\n  'NA19189': [{96122464: 1.5069106,\n    96143625: 493.48376,\n    96116885: 4.877583,\n    96126329: 6.5253005,\n    96143642: 493.48376,\n    96143803: 521.14453,\n    96143612: 493.48376,\n    96129374: 4.157276},\n   {96122464: 1.5116152,\n    96143625: 494.3658,\n    96116885: 4.879621,\n    96126329: 6.52306,\n    96143642: 494.3658,\n    96143803: 522.06714,\n    96143612: 494.3658,\n    96129374: 4.159506}],\n  'HG00108': [{96122464: 1.5751556,\n    96143625: 489.00113,\n    96116885: 5.0051737,\n    96126329: 6.606428,\n    96143642: 489.00113,\n    96143803: 511.88898,\n    96143612: 489.00113,\n    96129374: 4.207954},\n   {96122464: 1.5207282,\n    96143625: 498.83698,\n    96116885: 4.6417713,\n    96126329: 6.477669,\n    96143642: 498.83698,\n    96143803: 527.29694,\n    96143612: 498.83698,\n    96129374: 4.149347}],\n  'HG00380': [{96122464: 1.4761904,\n    96143625: 496.42267,\n    96116885: 5.0203476,\n    96126329: 6.5033755,\n    96143642: 496.42267,\n    96143803: 523.8202,\n    96143612: 496.42267,\n    96129374: 4.1738315},\n   {96122464: 1.479633,\n    96143625: 495.5843,\n    96116885: 4.774443,\n    96126329: 6.435179,\n    96143642: 495.5843,\n    96143803: 522.6483,\n    96143612: 495.5843,\n    96129374: 4.099624}],\n  'NA12872': [{96122464: 1.4858556,\n    96143625: 489.6926,\n    96116885: 4.469023,\n    96126329: 6.4571686,\n    96143642: 489.6926,\n    96143803: 511.5365,\n    96143612: 489.6926,\n    96129374: 4.141449},\n   {96122464: 1.4740081,\n    96143625: 496.40088,\n    96116885: 5.0094404,\n    96126329: 6.502212,\n    96143642: 496.40088,\n    96143803: 523.8369,\n    96143612: 496.40088,\n    96129374: 4.17309}]},\n 'NUDT2': {'NA11992': [{34329504: 360.29645}, {34329504: 349.09186}],\n  'NA19235': [{34329504: 359.55475}, {34329504: 355.93185}],\n  'NA20770': [{34329504: 368.78345}, {34329504: 359.83127}],\n  'HG00232': [{34329504: 368.47693}, {34329504: 360.06628}],\n  'HG00342': [{34329504: 348.71606}, {34329504: 348.70926}],\n  'NA20502': [{34329504: 360.18698}, {34329504: 368.52585}],\n  'NA19189': [{34329504: 368.25357}, {34329504: 358.4064}],\n  'HG00108': [{34329504: 367.97302}, {34329504: 359.6162}],\n  'HG00380': [{34329504: 366.38867}, {34329504: 368.78345}],\n  'NA12872': [{34329504: 368.5999}, {34329504: 360.27188}]},\n 'ERAP2': {'NA11992': [{96215268: 1.9573662,\n    96211690: 3.5760527,\n    96232107: 6.8314896,\n    96212204: 101.06402,\n    96212210: 101.06402,\n    96249042: 1.8730129,\n    96232276: 18.42331,\n    96224921: 3.1579292,\n    96228122: 2.926152,\n    96211643: 2.8741279,\n    96216637: 3.287645},\n   {96215268: 1.9141405,\n    96211690: 3.825592,\n    96232107: 6.658587,\n    96212204: 99.12656,\n    96212210: 99.12656,\n    96249042: 1.8300577,\n    96232276: 17.99865,\n    96224921: 3.0357225,\n    96228122: 2.8358064,\n    96211643: 3.085919,\n    96216637: 2.9487746}],\n  'NA19235': [{96215268: 1.65781,\n    96211690: 3.4480612,\n    96232107: 6.0128064,\n    96212204: 90.94387,\n    96212210: 90.94387,\n    96249042: 1.7983971,\n    96232276: 14.72766,\n    96224921: 2.9637382,\n    96228122: 2.591193,\n    96211643: 2.6010232,\n    96216637: 2.363321},\n   {96215268: 1.65606,\n    96211690: 3.7396135,\n    96232107: 7.172323,\n    96212204: 99.45302,\n    96212210: 99.45302,\n    96249042: 1.8005201,\n    96232276: 19.94696,\n    96224921: 2.995028,\n    96228122: 2.8180008,\n    96211643: 3.0171106,\n    96216637: 2.8931894}],\n  'NA20770': [{96215268: 1.9114778,\n    96211690: 3.8670275,\n    96232107: 6.7972136,\n    96212204: 100.553764,\n    96212210: 100.553764,\n    96249042: 1.8425192,\n    96232276: 18.338827,\n    96224921: 3.0765967,\n    96228122: 2.8608246,\n    96211643: 3.1222825,\n    96216637: 2.9986215},\n   {96215268: 2.1246905,\n    96211690: 3.586944,\n    96232107: 6.779892,\n    96212204: 100.929825,\n    96212210: 100.929825,\n    96249042: 1.887657,\n    96232276: 18.227226,\n    96224921: 3.2118118,\n    96228122: 2.956795,\n    96211643: 2.8861322,\n    96216637: 3.0828393}],\n  'HG00232': [{96215268: 1.5992041,\n    96211690: 3.436069,\n    96232107: 5.8945107,\n    96212204: 88.888435,\n    96212210: 88.888435,\n    96249042: 1.6416681,\n    96232276: 14.656058,\n    96224921: 2.7000933,\n    96228122: 2.3659935,\n    96211643: 2.7696738,\n    96216637: 2.390921},\n   {96215268: 1.659333,\n    96211690: 3.4326713,\n    96232107: 5.9903665,\n    96212204: 90.37398,\n    96212210: 90.37398,\n    96249042: 1.7810837,\n    96232276: 14.734984,\n    96224921: 2.949909,\n    96228122: 2.5706675,\n    96211643: 2.5881612,\n    96216637: 2.3925076}],\n  'HG00342': [{96215268: 1.9035211,\n    96211690: 4.33154,\n    96232107: 6.936302,\n    96212204: 106.723305,\n    96212210: 106.723305,\n    96249042: 1.9025588,\n    96232276: 16.903862,\n    96224921: 3.336564,\n    96228122: 2.953258,\n    96211643: 3.5385842,\n    96216637: 3.9677246},\n   {96215268: 1.6591437,\n    96211690: 3.4410238,\n    96232107: 6.007612,\n    96212204: 90.39801,\n    96212210: 90.39801,\n    96249042: 1.7848752,\n    96232276: 14.763089,\n    96224921: 2.955677,\n    96228122: 2.575909,\n    96211643: 2.592025,\n    96216637: 2.394583}],\n  'NA20502': [{96215268: 1.6603775,\n    96211690: 3.440762,\n    96232107: 6.002086,\n    96212204: 90.66086,\n    96212210: 90.66086,\n    96249042: 1.8020586,\n    96232276: 14.683932,\n    96224921: 2.9849577,\n    96228122: 2.6114209,\n    96211643: 2.5948172,\n    96216637: 2.3549569},\n   {96215268: 1.6661326,\n    96211690: 3.5037177,\n    96232107: 6.7828074,\n    96212204: 97.268196,\n    96212210: 97.268196,\n    96249042: 1.7742944,\n    96232276: 18.379011,\n    96224921: 2.9369073,\n    96228122: 2.7641768,\n    96211643: 2.8144753,\n    96216637: 2.6909184}],\n  'NA19189': [{96215268: 1.912669,\n    96211690: 3.830634,\n    96232107: 6.7558894,\n    96212204: 99.85003,\n    96212210: 99.85003,\n    96249042: 1.8170478,\n    96232276: 18.263515,\n    96224921: 3.0230594,\n    96228122: 2.8030372,\n    96211643: 3.0923781,\n    96216637: 2.9977138},\n   {96215268: 1.8720021,\n    96211690: 4.188224,\n    96232107: 6.3600917,\n    96212204: 97.48567,\n    96212210: 97.48567,\n    96249042: 1.8381789,\n    96232276: 15.724998,\n    96224921: 3.1853266,\n    96228122: 2.631789,\n    96211643: 3.4172022,\n    96216637: 3.2543797}],\n  'HG00108': [{96215268: 1.9507817,\n    96211690: 3.4833183,\n    96232107: 6.6762815,\n    96212204: 100.01849,\n    96212210: 100.01849,\n    96249042: 1.846806,\n    96232276: 18.03885,\n    96224921: 3.0925941,\n    96228122: 2.8673196,\n    96211643: 2.7999053,\n    96216637: 3.1439543},\n   {96215268: 1.6853107,\n    96211690: 3.513917,\n    96232107: 6.0838914,\n    96212204: 90.43939,\n    96212210: 90.43939,\n    96249042: 1.8000969,\n    96232276: 14.899693,\n    96224921: 2.964983,\n    96228122: 2.5964456,\n    96211643: 2.6558022,\n    96216637: 2.4208496}],\n  'HG00380': [{96215268: 1.6373917,\n    96211690: 3.4473348,\n    96232107: 5.8897705,\n    96212204: 88.72897,\n    96212210: 88.72897,\n    96249042: 1.7486763,\n    96232276: 14.52218,\n    96224921: 2.910553,\n    96228122: 2.5147474,\n    96211643: 2.7709281,\n    96216637: 2.3488388},\n   {96215268: 1.7261969,\n    96211690: 3.5750756,\n    96232107: 7.0957203,\n    96212204: 101.91976,\n    96212210: 101.91976,\n    96249042: 1.8566126,\n    96232276: 19.044308,\n    96224921: 2.9651632,\n    96228122: 2.8200989,\n    96211643: 2.6818175,\n    96216637: 2.7964373}],\n  'NA12872': [{96215268: 1.6664355,\n    96211690: 3.4343095,\n    96232107: 6.074191,\n    96212204: 90.90537,\n    96212210: 90.90537,\n    96249042: 1.8024985,\n    96232276: 14.850572,\n    96224921: 2.9908426,\n    96228122: 2.622704,\n    96211643: 2.5870202,\n    96216637: 2.4059932},\n   {96215268: 1.6591437,\n    96211690: 3.4410238,\n    96232107: 6.007612,\n    96212204: 90.39801,\n    96212210: 90.39801,\n    96249042: 1.7848752,\n    96232276: 14.763089,\n    96224921: 2.955677,\n    96228122: 2.575909,\n    96211643: 2.592025,\n    96216637: 2.394583}]},\n 'PEX6': {'NA11992': [{42946888: 309.12695, 42946958: 309.12695},\n   {42946888: 309.77313, 42946958: 309.77313}],\n  'NA19235': [{42946888: 344.3359, 42946958: 344.3359},\n   {42946888: 344.27832, 42946958: 344.27832}],\n  'NA20770': [{42946888: 357.873, 42946958: 357.873},\n   {42946888: 309.77625, 42946958: 309.77625}],\n  'HG00232': [{42946888: 344.83777, 42946958: 344.83777},\n   {42946888: 309.80035, 42946958: 309.80035}],\n  'HG00342': [{42946888: 357.86163, 42946958: 357.86163},\n   {42946888: 311.33905, 42946958: 311.33905}],\n  'NA20502': [{42946888: 357.7039, 42946958: 357.7039},\n   {42946888: 309.78336, 42946958: 309.78336}],\n  'NA19189': [{42946888: 348.3653, 42946958: 348.3653},\n   {42946888: 358.78842, 42946958: 358.78842}],\n  'HG00108': [{42946888: 358.2019, 42946958: 358.2019},\n   {42946888: 344.84875, 42946958: 344.84875}],\n  'HG00380': [{42946888: 309.6954, 42946958: 309.6954},\n   {42946888: 358.15863, 42946958: 358.15863}],\n  'NA12872': [{42946888: 359.5108, 42946958: 359.5108},\n   {42946888: 358.18723, 42946958: 358.18723}]}}\n\n\n\n\nPlotting the CAGE:B lymphoblastoid cell line tracks\nNext, we will plot the tracks. We have already defined two helper functions, prepare_for_plot_tracks and plot_tracks to plot the expression along the TSS for a gene, for an individual and for each haplotype.\nFor NUDT2…\n\ntemp = prepare_for_plot_tracks(gene=exercise_1_genes[1], individual=exercise_1_individuals[0], all_predictions=exercise_1_predictions[1], chromosome=['9'])\nplot_tracks(tracks=temp['gene_tracks'], interval=temp['gene_intervals'])\n\n\n\n\nLooks nice!\nAlthough it looks like there is no variation in the predictions for the haplotypes, we can take a look at the actual prediction values across the TSS.\nThe columns are the transcription start sites, and the rows are the haplotypes for the individual. The entries are the sum of the predictions at the TSS, at TSS - 1, and at the TSS + 1.\nWe will look at the first individual, NA11992, for NUDT2…\n\npd.DataFrame(exercise_1_predictions[0][exercise_1_genes[1]][exercise_1_individuals[0]], index=['haplotype_1', 'haplotype_2'])\n\n\n\n\n\n\n\n\n34329504\n\n\n\n\nhaplotype_1\n360.296448\n\n\nhaplotype_2\n349.091858\n\n\n\n\n\n\n\nWe will look at the first individual, NA11992, for PEX6…\n\npd.DataFrame(exercise_1_predictions[0][exercise_1_genes[3]][exercise_1_individuals[0]], index=['haplotype_1', 'haplotype_2'])\n\n\n\n\n\n\n\n\n42946888\n42946958\n\n\n\n\nhaplotype_1\n309.126953\n309.126953\n\n\nhaplotype_2\n309.773132\n309.773132\n\n\n\n\n\n\n\nMerely looking at the values, it looks like there are variations in the predictions across the haplotypes and the TSS. We expected some variations because we are predicting expression for each haplotype, which tend to have variations in them. Very nice!"
  },
  {
    "objectID": "posts/enformer_usage_hackathon/index.html#comparing-with-true-expression-from-geuvadis-and-with-predixcan",
    "href": "posts/enformer_usage_hackathon/index.html#comparing-with-true-expression-from-geuvadis-and-with-predixcan",
    "title": "Enformer usage hackathon demo",
    "section": "Comparing with true expression from GEUVADIS and with Predixcan",
    "text": "Comparing with true expression from GEUVADIS and with Predixcan\nWe should read in the GEUVADIS and Predixcan predictions.\n\ngeuvadis_gene_expression = pd.read_table('https://uchicago.box.com/shared/static/5vwc7pjw9qmtv7298c4rc7bcuicoyemt.gz', sep='\\t',\n                                         dtype={'gene_id': str, 'gene_name':str, 'TargetID':str, 'Chr':str})\ngeuvadis_gene_expression.head(5)\n\n\n\n\n\n\n\n\ngene_id\ngene_name\nTargetID\nChr\nCoord\nHG00096\nHG00097\nHG00099\nHG00100\nHG00101\n...\nNA20810\nNA20811\nNA20812\nNA20813\nNA20814\nNA20815\nNA20816\nNA20819\nNA20826\nNA20828\n\n\n\n\n0\nENSG00000223972.4\nDDX11L1\nENSG00000223972.4\n1\n11869\n0.320818\n0.344202\n0.354225\n0.478064\n-0.102815\n...\n1.008605\n0.384489\n0.581284\n0.513981\n0.667449\n0.350890\n0.186103\n-0.037976\n0.405439\n0.199143\n\n\n1\nENSG00000227232.3\nWASH7P\nENSG00000227232.3\n1\n29806\n33.714457\n20.185174\n18.095407\n24.100871\n29.018719\n...\n30.980194\n34.086207\n39.678442\n29.643513\n27.120420\n29.121624\n31.117198\n32.047074\n22.798959\n23.563874\n\n\n2\nENSG00000243485.1\nMIR1302-11\nENSG00000243485.1\n1\n29554\n0.240408\n0.157456\n0.218806\n0.320878\n0.067833\n...\n0.065940\n0.228784\n0.140642\n0.283905\n0.273821\n0.286311\n0.324060\n0.049574\n0.255288\n0.157440\n\n\n3\nENSG00000238009.2\nRP11-34P13.7\nENSG00000238009.2\n1\n133566\n0.328272\n0.327932\n0.090064\n0.420443\n0.220269\n...\n0.274071\n0.384179\n0.533693\n0.307221\n0.307367\n0.400278\n0.612321\n0.666633\n0.281138\n1.346129\n\n\n4\nENSG00000239945.1\nRP11-34P13.8\nENSG00000239945.1\n1\n91105\n0.332171\n-0.032164\n0.017323\n0.424677\n0.214025\n...\n0.347323\n0.346744\n0.073580\n0.400396\n0.470517\n0.069749\n0.299353\n0.090019\n0.282554\n-0.157170\n\n\n\n\n5 rows × 467 columns\n\n\n\n\npredixcan_gene_expression = pd.read_table('https://uchicago.box.com/shared/static/4k68u7x7rxjpoljfdva6qipjxwzd3l0g.txt', sep=' ')\npredixcan_gene_expression.head(5)\n\n\n\n\n\n\n\n\ngene_names_proper\ngene_name\nHG00315\nHG00327\nHG00334\nHG00339\nHG00341\nHG00346\nHG00353\nHG00358\n...\nNA20760\nNA20765\nNA20772\nNA20796\nNA20804\nNA20809\nNA20811\nNA20816\nNA20828\nNA20506\n\n\n\n\n0\nENSG00000002016\nRAD52\n-0.250104\n0.025857\n-0.044889\n-0.126075\n-0.019377\n-0.305435\n-0.084487\n-0.083447\n...\n0.067738\n-0.032721\n-0.044850\n-0.136171\n0.030402\n-0.145576\n0.000571\n0.021220\n0.006563\n-0.012295\n\n\n1\nENSG00000002549\nLAP3\n-0.139077\n-0.324475\n-0.314899\n-0.316845\n-0.279671\n-0.142608\n-0.318413\n0.028499\n...\n-0.308563\n-0.150983\n0.017815\n-0.333274\n-0.130020\n-0.312917\n-0.289931\n-0.148905\n-0.324063\n-0.151374\n\n\n2\nENSG00000002726\nAOC1\n0.134822\n0.004770\n-0.182188\n-0.238028\n0.044468\n-0.250673\n-0.116671\n-0.243399\n...\n0.018929\n-0.021651\n0.010531\n0.364958\n0.131044\n-0.009168\n-0.219884\n-0.424765\n-0.404101\n0.006370\n\n\n3\nENSG00000002822\nMAD1L1\n-0.130900\n-0.138393\n-0.113105\n0.106536\n-0.026348\n-0.070047\n-0.062220\n-0.018478\n...\n-0.159111\n-0.127137\n-0.037308\n-0.032433\n-0.213594\n-0.156237\n-0.187337\n-0.115429\n-0.235621\n-0.059553\n\n\n4\nENSG00000003393\nALS2\n0.250872\n-0.202676\n0.271500\n0.282373\n0.199417\n-0.179208\n-0.598147\n-0.140201\n...\n0.238821\n0.250585\n0.185087\n0.042159\n0.244893\n-0.185666\n0.242587\n0.234396\n-0.294932\n0.154260\n\n\n\n\n5 rows × 460 columns\n\n\n\n\nQUESTION 3a\nWhat is the dimension/size/shape of the geuvadis_gene_expression dataframe? Hint: You can use the .shape method on a dataframe.\n\ngeuvadis_dimension = geuvadis_gene_expression.shape\nprint(\"The geuvadis_gene_expression dataframe has {} rows and {} columns\".format(*geuvadis_dimension))\n\nThe geuvadis_gene_expression dataframe has 23722 rows and 467 columns\n\n\n\n\nQUESTION 4b\nWhat is the dimension/size/shape of the predixcan_gene_expression dataframe? Hint: You can use the .shape method on a dataframe.\n\npredixcan_dimension = predixcan_gene_expression.shape\nprint(\"The predixcan_gene_expression dataframe has {} rows and {} columns\".format(*predixcan_dimension))\n\nThe predixcan_gene_expression dataframe has 4031 rows and 460 columns\n\n\nWe select the individuals and the gene from the geuvadis_gene_expression dataframe.\n\nerap1_geuvadis_expression = geuvadis_gene_expression[geuvadis_gene_expression.gene_name == exercise_1_genes[0]].loc[:,exercise_1_individuals]\nnudt2_geuvadis_expression = geuvadis_gene_expression[geuvadis_gene_expression.gene_name == exercise_1_genes[1]].loc[:,exercise_1_individuals]\nerap2_geuvadis_expression = geuvadis_gene_expression[geuvadis_gene_expression.gene_name == exercise_1_genes[2]].loc[:,exercise_1_individuals]\npex6_geuvadis_expression = geuvadis_gene_expression[geuvadis_gene_expression.gene_name == exercise_1_genes[3]].loc[:,exercise_1_individuals]\n\n\nnudt2_geuvadis_expression\n\n\n\n\n\n\n\n\nNA11992\nNA19235\nNA20770\nHG00232\nHG00342\nNA20502\nNA19189\nHG00108\nHG00380\nNA12872\n\n\n\n\n10804\n23.713984\n14.787901\n16.181407\n13.594301\n20.765908\n16.877474\n12.753234\n11.754371\n10.113347\n17.138522\n\n\n\n\n\n\n\nWe will sum the prediction for both haplotypes for each TSS, and take the sum of the resulting values. The function used here can also take the max instead of the sums.\nWe have 3 utility functions to help us - plot_enformer_vs_guevadis - plot_predixcan_vs_geuvadis - plot_enformer_vs_predixcan (if you think this is necessary)\n\nerap1_vs_geu = plot_enformer_vs_geuvadis(prediction_results=exercise_1_predictions, geuvadis_expression=geuvadis_gene_expression,\n                            interested_gene=exercise_1_genes[0], interested_individuals=exercise_1_individuals, how='sum')\nprint('Correlation coefficient: {}'.format(erap1_vs_geu[1]))\n\nCorrelation coefficient: 0.2248028760296875\n\n\n\n\n\n\npex6_vs_geu = plot_enformer_vs_geuvadis(prediction_results=exercise_1_predictions, geuvadis_expression=geuvadis_gene_expression,\n                            interested_gene=exercise_1_genes[3], interested_individuals=exercise_1_individuals, how='sum')\n\nprint('Correlation coefficient: {}'.format(pex6_vs_geu[1]))\n\nCorrelation coefficient: 0.8669689275345515\n\n\n\n\n\n\nnudt_vs_geu = plot_enformer_vs_geuvadis(prediction_results=exercise_1_predictions, geuvadis_expression=geuvadis_gene_expression,\n                            interested_gene=exercise_1_genes[1], interested_individuals=exercise_1_individuals, how='sum')\n\nprint('Correlation coefficient: {}'.format(nudt_vs_geu[1]))\n\nCorrelation coefficient: -0.7469431320173172\n\n\n\n\n\n\nerap2_vs_geu = plot_enformer_vs_geuvadis(prediction_results=exercise_1_predictions, geuvadis_expression=geuvadis_gene_expression,\n                            interested_gene=exercise_1_genes[2], interested_individuals=exercise_1_individuals, how='sum')\n\nprint('Correlation coefficient: {}'.format(erap2_vs_geu[1]))\n\nCorrelation coefficient: -0.46924360279488414\n\n\n\n\n\nNow, we can see how Predixcan performs on these individuals\n\nerap1_predix = plot_predixcan_vs_geuvadis(interested_gene=exercise_1_genes[0], interested_individuals=exercise_1_individuals, geuvadis_expression=geuvadis_gene_expression, predixcan_expression=predixcan_gene_expression)\nprint('The correlation coefficient: {}'.format(erap1_predix[1]))\n\nThe correlation coefficient: 0.8736403074571262\n\n\n\n\n\n\npex6_predix = plot_predixcan_vs_geuvadis(interested_gene=exercise_1_genes[3], interested_individuals=exercise_1_individuals, geuvadis_expression=geuvadis_gene_expression, predixcan_expression=predixcan_gene_expression)\nprint('The correlation coefficient: {}'.format(pex6_predix[1]))\n\nThe correlation coefficient: 0.9747939454280228\n\n\n\n\n\n\nerap2_predix = plot_predixcan_vs_geuvadis(interested_gene=exercise_1_genes[2], interested_individuals=exercise_1_individuals, geuvadis_expression=geuvadis_gene_expression, predixcan_expression=predixcan_gene_expression)\nprint('The correlation coefficient: {}'.format(erap2_predix[1]))\n\nThe correlation coefficient: 0.7541638507596199\n\n\n\n\n\n\nnudt2_predix = plot_predixcan_vs_geuvadis(interested_gene=exercise_1_genes[1], interested_individuals=exercise_1_individuals, geuvadis_expression=geuvadis_gene_expression, predixcan_expression=predixcan_gene_expression)\nprint('The correlation coefficient: {}'.format(nudt2_predix[1]))\n\nThe correlation coefficient: 0.7396287059598816\n\n\n\n\n\nQuite neat and impressive!"
  },
  {
    "objectID": "posts/enformer_usage_hackathon/index.html#exercise-2",
    "href": "posts/enformer_usage_hackathon/index.html#exercise-2",
    "title": "Enformer usage hackathon demo",
    "section": "EXERCISE 2",
    "text": "EXERCISE 2\nIn this exercise, you will get your hands dirty, and run Enformer on your gene(s) of interest.\n\nSelect your favorite gene(s). Note that the more genes you use, the longer it will take to run.\nRandomly select 10 individuals, just because we don’t have all the computational power.\nRun predictions\n\nWe only have data for a finite set of genes (sorry!). Here is a list of available genes you can use:\n\n!curl -L https://uchicago.box.com/shared/static/x8d7dx1ykefz49ep6sxot42v44sfvcv5.tsv --output /grand/TFXcan/imlab/users/tiffanie/enformer/data/all_genes.tsv\n\nwith open(\"/grand/TFXcan/imlab/users/tiffanie/enformer/data/all_genes.tsv\", \"r\") as ag:\n  all_genes = [line.strip() for line in ag]\nprint(len(all_genes))\nprint(\"First 5 genes all_genes:\", all_genes[0:5])\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100     5    0     5    0     0      4      0 --:--:--  0:00:01 --:--:--  5000\n100 20052  100 20052    0     0  15133      0  0:00:01  0:00:01 --:--:-- 15133\n3114\nFirst 5 genes all_genes: ['PEX10', 'TMEM69', 'ATAD3A', 'BPNT1', 'SNIP1']\n\n\n\nSelect your genes\n\n\nmy_genes = ['PEX10', 'TMEM69', 'ATAD3A', 'BPNT1', 'SNIP1']\nprint(\"My gene(s) is/are {}\".format(', '.join(my_genes)))\n\nMy gene(s) is/are PEX10, TMEM69, ATAD3A, BPNT1, SNIP1\n\n\n\nRead in the TSS txt files where those chromosome are located. If you have genes located on more than one chromosome, copy the pd.read_table line for each chromosome you have, and replace the chromosome number (ellipses) as appropriate.\n\n\nmy_chromosomes = [1] # put in the chromosomes where the genes are located. Just the numbers will do, or you can put them in as a string type\n\n\nmy_tss_list = []\nfor chr in my_chromosomes:\n  chr = str(chr)\n  bed_file = '/grand/TFXcan/imlab/users/tiffanie/enformer/data/tss_by_chr/chr{}_tss_by_gene.txt'.format(chr)\n  my_tss_list.append(pd.read_table(bed_file, sep='\\t')) # we read in the TSSs for each chromosome, and put them into a list\n\n\nRandomly select 10 individuals\n\n\n# let us set a seed to randomly select 10 individuals\nnp.random.seed(2023)  # replace ... with an integer you want\n\nnumber_of_individuals = 10\n\nmy_individuals = np.random.choice(a=geuvadis_gene_expression.columns[6:-1], size=number_of_individuals, replace=False) # individuals we are interested in\nmy_individuals\n\narray(['NA19144', 'NA18933', 'HG00327', 'NA19138', 'NA19096', 'HG00133',\n       'NA20757', 'NA20803', 'NA18923', 'HG00355'], dtype=object)\n\n\n\nWe want to make sure that we have complete variation information for all 10 individuals.\n\nFirst, we need to download the variation bed files for these individuals\n\ndownload_chrom_beds(chromosome='1', genes=my_genes) # remember that the genes should be on that chromosome, and you can use this code for each chromosome you have.\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100     8    0     8    0     0      7      0 --:--:--  0:00:01 --:--:--  8000\n100 18.6M  100 18.6M    0     0  8183k      0  0:00:02  0:00:02 --:--:-- 21.3M\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100     8    0     8    0     0      7      0 --:--:--  0:00:01 --:--:--  8000\n100 18.6M  100 18.6M    0     0  7030k      0  0:00:02  0:00:02 --:--:-- 15.2M\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100     8    0     8    0     0      7      0 --:--:--  0:00:01 --:--:--  8000\n100 18.6M  100 18.6M    0     0  8025k      0  0:00:02  0:00:02 --:--:-- 19.2M\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100     8    0     8    0     0      8      0 --:--:-- --:--:-- --:--:--    16\n100 18.6M  100 18.6M    0     0  5037k      0  0:00:03  0:00:03 --:--:-- 7656k\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100     8    0     8    0     0      8      0 --:--:-- --:--:-- --:--:--     8\n100 18.6M  100 18.6M    0     0  10.1M      0  0:00:01  0:00:01 --:--:-- 27.9M\n\n\nRead in the variation bed files\n\nimport os\n\nmy_missing_list = list()\nfor chr in my_chromosomes:\n  for gene in my_genes:\n    chr = str(chr)\n    file_path = '/grand/TFXcan/imlab/users/tiffanie/enformer/data/individual_beds/chr' + chr + '/chr' + chr + '_' + gene + '.bed'\n    if not os.path.exists(file_path):\n      continue\n    my_missing_list.append(check_individuals(file_path, my_individuals))\n\nAll individuals are present in the bed file.\nAll individuals are present in the bed file.\nAll individuals are present in the bed file.\nAll individuals are present in the bed file.\nAll individuals are present in the bed file.\n\n\n\nQUESTION 4\nAre there missing individuals? All answers, based on your results are correct. If there are missing individuals, can you remove them? You can add new code blocks as you like.\n\nmy_missing_list #no missing individuals\n\n[[], [], [], [], []]\n\n\nIt looks like we are almost set to make predictions.\n\nMake predictions. First, we will collect the intervals for the genes we want, check the object and make sure we are on the right track. Next, we will call our run_predictions function.\n\n\nchr5_tss = pd.read_table('/grand/TFXcan/imlab/users/tiffanie/enformer/data/tss_by_chr/chr5_tss_by_gene.txt', sep='\\t')\n\n\nmy_intervals = collect_intervals(chromosomes= [\"1\"], gene_list= my_genes) # here, we collect the intervals for that gene; replace ... with the right objects\nmy_intervals\n\n{'PEX10': ['1', 2336236, 2345236],\n 'TMEM69': ['1', 46152886, 46160115],\n 'ATAD3A': ['1', 1447531, 1470067],\n 'BPNT1': ['1', 220230824, 220263804],\n 'SNIP1': ['1', 38002142, 38019905]}\n\n\n\nmy_predictions = run_predictions(gene_intervals= my_intervals, tss_dataframe= my_tss_list, individuals_list=my_individuals)\n\nCurrently on gene PEX10, and predicting on individual NA19144...\nCurrently on gene PEX10, and predicting on individual NA18933...\nCurrently on gene PEX10, and predicting on individual HG00327...\nCurrently on gene PEX10, and predicting on individual NA19138...\nCurrently on gene PEX10, and predicting on individual NA19096...\nCurrently on gene PEX10, and predicting on individual HG00133...\nCurrently on gene PEX10, and predicting on individual NA20757...\nCurrently on gene PEX10, and predicting on individual NA20803...\nCurrently on gene PEX10, and predicting on individual NA18923...\nCurrently on gene PEX10, and predicting on individual HG00355...\nCurrently on gene TMEM69, and predicting on individual NA19144...\nCurrently on gene TMEM69, and predicting on individual NA18933...\nCurrently on gene TMEM69, and predicting on individual HG00327...\nCurrently on gene TMEM69, and predicting on individual NA19138...\nCurrently on gene TMEM69, and predicting on individual NA19096...\nCurrently on gene TMEM69, and predicting on individual HG00133...\nCurrently on gene TMEM69, and predicting on individual NA20757...\nCurrently on gene TMEM69, and predicting on individual NA20803...\nCurrently on gene TMEM69, and predicting on individual NA18923...\nCurrently on gene TMEM69, and predicting on individual HG00355...\nCurrently on gene ATAD3A, and predicting on individual NA19144...\nCurrently on gene ATAD3A, and predicting on individual NA18933...\nCurrently on gene ATAD3A, and predicting on individual HG00327...\nCurrently on gene ATAD3A, and predicting on individual NA19138...\nCurrently on gene ATAD3A, and predicting on individual NA19096...\nCurrently on gene ATAD3A, and predicting on individual HG00133...\nCurrently on gene ATAD3A, and predicting on individual NA20757...\nCurrently on gene ATAD3A, and predicting on individual NA20803...\nCurrently on gene ATAD3A, and predicting on individual NA18923...\nCurrently on gene ATAD3A, and predicting on individual HG00355...\nCurrently on gene BPNT1, and predicting on individual NA19144...\nCurrently on gene BPNT1, and predicting on individual NA18933...\nCurrently on gene BPNT1, and predicting on individual HG00327...\nCurrently on gene BPNT1, and predicting on individual NA19138...\nCurrently on gene BPNT1, and predicting on individual NA19096...\nCurrently on gene BPNT1, and predicting on individual HG00133...\nCurrently on gene BPNT1, and predicting on individual NA20757...\nCurrently on gene BPNT1, and predicting on individual NA20803...\nCurrently on gene BPNT1, and predicting on individual NA18923...\nCurrently on gene BPNT1, and predicting on individual HG00355...\nCurrently on gene SNIP1, and predicting on individual NA19144...\nCurrently on gene SNIP1, and predicting on individual NA18933...\nCurrently on gene SNIP1, and predicting on individual HG00327...\nCurrently on gene SNIP1, and predicting on individual NA19138...\nCurrently on gene SNIP1, and predicting on individual NA19096...\nCurrently on gene SNIP1, and predicting on individual HG00133...\nCurrently on gene SNIP1, and predicting on individual NA20757...\nCurrently on gene SNIP1, and predicting on individual NA20803...\nCurrently on gene SNIP1, and predicting on individual NA18923...\nCurrently on gene SNIP1, and predicting on individual HG00355...\n\n\nAt this point, we will leave you to make your own plots…\n\nlen(my_predictions)\n\n2\n\n\n\nmy_genes\n\n['PEX10', 'TMEM69', 'ATAD3A', 'BPNT1', 'SNIP1']\n\n\nex. plotting PEX10 (located on chr1) expression levels in the first individual (NA19144)\n\nprep_pex10_plt = prepare_for_plot_tracks(gene=my_genes[0], individual=my_individuals[0], all_predictions=my_predictions[1], chromosome=['1'])\nplot_tracks(tracks=prep_pex10_plt['gene_tracks'], interval=prep_pex10_plt['gene_intervals'])"
  },
  {
    "objectID": "posts/conduct_gwas/index.html",
    "href": "posts/conduct_gwas/index.html",
    "title": "Conducting GWAS studies summary",
    "section": "",
    "text": "paper link\ndemo github link\n\n\nGlossary\n\n\nClumping: This is a procedure in which only the most significant SNP (i.e., lowest p value) in each LD block is identified and selected for further analyses. This reduces the correlation between the remaining SNPs, while retaining SNPs with the strongest statistical evidence.\nCo‐heritability: This is a measure of the genetic relationship between disorders. The SNP‐based co‐heritability is the proportion of covariance between disorder pairs (e.g., schizophrenia and bipolar disorder) that is explained by SNPs.\nGene: This is a sequence of nucleotides in the DNA that codes for a molecule (e.g., a protein)\nHeterozygosity: This is the carrying of two different alleles of a specific SNP. The heterozygosity rate of an individual is the proportion of heterozygous genotypes. High levels of heterozygosity within an individual might be an indication of low sample quality whereas low levels of heterozygosity may be due to inbreeding.\nIndividual‐level missingness: This is the number of SNPs that is missing for a specific individual. High levels of missingness can be an indication of poor DNA quality or technical problems.\nLinkage disequilibrium (LD): This is a measure of non‐random association between alleles at different loci at the same chromosome in a given population. SNPs are in LD when the frequency of association of their alleles is higher than expected under random assortment. LD concerns patterns of correlations between SNPs.\nMinor allele frequency (MAF): This is the frequency of the least often occurring allele at a specific location. Most studies are underpowered to detect associations with SNPs with a low MAF and therefore exclude these SNPs.\nPopulation stratification: This is the presence of multiple subpopulations (e.g., individuals with different ethnic background) in a study. Because allele frequencies can differ between subpopulations, population stratification can lead to false positive associations and/or mask true associations. An excellent example of this is the chopstick gene, where a SNP, due to population stratification, accounted for nearly half of the variance in the capacity to eat with chopsticks (Hamer & Sirota, 2000).\nPruning: This is a method to select a subset of markers that are in approximate linkage equilibrium. In PLINK, this method uses the strength of LD between SNPs within a specific window (region) of the chromosome and selects only SNPs that are approximately uncorrelated, based on a user‐specified threshold of LD. In contrast to clumping, pruning does not take the p value of a SNP into account.\nRelatedness: This indicates how strongly a pair of individuals is genetically related. A conventional GWAS assumes that all subjects are unrelated (i.e., no pair of individuals is more closely related than second‐degree relatives). Without appropriate correction, the inclusion of relatives could lead to biased estimations of standard errors of SNP effect sizes. Note that specific tools for analysing family data have been developed.\nSex discrepancy: This is the difference between the assigned sex and the sex determined based on the genotype. A discrepancy likely points to sample mix‐ups in the lab. Note, this test can only be conducted when SNPs on the sex chromosomes (X and Y) have been assessed.\nSingle nucleotide polymorphism (SNP): This is a variation in a single nucleotide (i.e., A, C, G, or T) that occurs at a specific position in the genome. A SNP usually exists as two different forms (e.g., A vs. T). These different forms are called alleles. A SNP with two alleles has three different genotypes (e.g., AA, AT, and TT).\nSNP‐heritability: This is the fraction of phenotypic variance of a trait explained by all SNPs in the analysis.\nSNP‐level missingness: This is the number of individuals in the sample for whom information on a specific SNP is missing. SNPs with a high level of missingness can potentially lead to bias.\nSummary statistics: These are the results obtained after conducting a GWAS, including information on chromosome number, position of the SNP, SNP(rs)‐identifier, MAF, effect size (odds ratio/beta), standard error, and p-value. Summary statistics of GWAS are often freely accessible or shared between researchers.\nThe Hardy–Weinberg (dis)equilibrium (HWE) law: This concerns the relation between the allele and genotype frequencies. It assumes an indefinitely large population, with no selection, mutation, or migration. The law states that the genotype and the allele frequencies are constant over generations. Violation of the HWE law indicates that genotype frequencies are significantly different from expectations (e.g., if the frequency of allele A = 0.20 and the frequency of allele T = 0.80; the expected frequency of genotype AT is 2 * 0.2 * 0.8 = 0.32) and the observed frequency should not be significantly different. In GWAS, it is generally assumed that deviations from HWE are the result of genotyping errors. The HWE thresholds in cases are often less stringent than those in controls, as the violation of the HWE law in cases can be indicative of true genetic association with disease risk.\n\n\nIntro\nAim of genome-wide association studies (GWAS): to identify single nucleotide polymorphisms (SNPs — variation in single nucleotide at specific position in genome; exists as 2 forms aka alleles) of which allele frequencies vary systematically as a fn of phenotypic trait values, since identifying trait-associated SNPs may reveal new insights into biological mechanisms behind phenotypes. SNP w/ 2 alleles → 3 different genotypes.\nHistorically studies suggest that psychiatric traits are influenced by SNPs — each having small individual effect sizes. GWAS relies strongly on in-depth knowledge of genetic architecture of the human genome, provided by the International HapMap Project (patterns of common SNPs within human DNA seq)+1000 Genomes project (map of both common and rare SNPs).\nGWAS results showed effect sizes of individual SNPs are small → want to find way to aggregate effect of SNPs → focus on PRS (polygenic risk score) analysis, since it can be applied to target samples w/ more modest sample sizes.\nPRS — combines effect sizes of multiple SNPs into an aggregated score (individual level, based on # risk variants carried, weighted by SNP effect sizes from an independent large-scaled discovery GWAS) used to predict disease risk.\n\ni.e. indication of total genetic risk for an individual for a specific trait (disease status); can be used in clinical pred/screening\nsignificantly associaed w/ case-control status for psychiatric traits; however discriminative accuracy still insufficient\nalso used to investigate if genetic effect sizes from a GWAS of a specific phenotype of interest can predict risk of another phenotype\npaper outline: quality control (QC) procedures before GWAS, commonly used tests of association, conduct analysis\n\nSoftware\nPLINK v1.08 will be used for QC procedures+stat analyses and R for visualization.\nPLINK: reads text-format or binary files, but binary is faster and recommended\n\ntext PLINK data: 2 files of different contents\n\ninfo on individuals+genotype (*.ped)\ninfo on genetic markers (*.map)\n\nbinary PLINK data: 3 files\n\nbinary file w/ IDs+genotypes for individuals (*.bed)\n\ne.g. individual IDs and genotypes\n\ntext file w/ info on individuals (*.fam)\n\ne.g. subject-related data like family relationship w/ other participants, sex, clinical diagnosis\n\ntext file w/ info on genetic markers (*.bim)\n\ne.g. info on phys position of SNPs\n\n\n\n\n\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001694/bin/MPR-27-e1608-g001.jpg\n\n\nPLINK — command line program → requires active shell waiting for commands\n\nafter prompt: indicate use of PLINK w/ plink (will need path to directory if not in current directory)\nafter plink other options, beginning w/ 2 dashes -- :\n\nprovide format+name of data files\n\ntext file: --file {your_file}\nbinary: --bfile {yourfile}\n\nafter: other options can be added; e.g. --assoc option for association analysis w/ \\(\\chi^2\\) test for each SNP to phenotype of interest\n\nmultiple options can be combined in a single command line, but default order is implemented\n--out {outfile} provides name to output file\n\nQC of genetic data\nAppropriate QC needed to generate reliable results since sources of errors include poor quality of DNA samples, poor DNA hybridization to array, poorly performing genotype probes, and sample mix-ups/contamination.\nData simulation w/ HapMap data:\n\nillustration of analysis steps w/ realistic genetic data using simulated dataset (\\(N=207\\)) w/ binary outcome measure\nrelatively small sample size+ethnically homogeneous dataset (*larger sample sizes will be needed to detect genetic risk factors of complex traits)\ndata from demo 1 in this md file (link)\n\n7 QC steps (key terms from beginning bolded) — hands-on lab in demos 1+2\n\nindividual+SNP missingness\ninconsistencies in assigned+genetic sex of subjects (see sex discrepancy)\nminor allele freq (MAF)\ndeviations from HWE (Hardy-Weinberg equilibrium)\nheterozygosity rate\nrelatedness\nethnic outliers (see population stratification)\n\nStep - command - fn - threshold table\n\n\n\n\n\n\n\n\n\nStep\nCommand\nFn\nThreshold\n\n\n\n\nmissingness of SNPs+individuals\n--geno, --mind\nexcludes SNPs missing in large proportion of subjects, remove SNPs w/ low genotype calls*; excludes individuals w/ high rates of genotype missingness, remove individual with low genotype calls\nrecommended: first filter SNPs+individuals based on relaxed threshold (0.2; &gt;20%) to filter out ones w/ very high levels of missingness; then apply filter w/ more stringent threshold (0.02);\n\n\nSNP filtering before individual filtering\n\n\n\n\n\nsex discrepancy\n--check-sex\nchecks for sex discrepancies between individuals recorded in dataset+sex based on X chromosome heterozygosity/homozygosity rates\ncan indicate sample mix-ups; want to see males X chromosome homozygosity estimate of &gt;0.8 and females of &lt;0.2\n\n\nMAF (minor allele freq)\n--maf\ninclude only SNPs above set MAF threshold\nSNPs w/ low MAF are rare → power lacking for detecting SNP-phenotype associations; SNPs also more prone to genotyping errors; MAF threshold depends on sample size — larger samples can use lower MAF thresholds; for large (\\(N = 100.000\\)) vs. moderate samples (\\(N = 10000\\)), 0.01 and 0.05 are commonly used as MAF threshold\n\n\nHWE\n--hwe\nexclude markers deviating from HWE\ncommon indicator of genotyping error; may also indicate evolutionary selection. binary traits — suggest excluding HWE where \\(p&lt;1\\times10^{-10}\\) in cases, \\(p&lt;1\\times10^{-6}\\) in controls; less strict case threshold avoids discarding disease-associated SNPs under selection; quantitative traits — recommend HWE \\(p &lt; 1\\times10^{-6}\\)\n\n\nheterozygosity\nscript from demo link\nexclude individuals w/ high or low heterozygosity rates\ndeviations can indicate sample contamination, inbreeding; suggest removing individuals deviating \\(\\pm 3s_x\\) from the samples’ heterozygosity rate mean.\n\n\nrelatedness\n--genome, --min\ncalculate identity by descent (IBD)** of all sample pairs, creates list of individuals w/ relatedness above set threshold**\nuse independent SNPs (pruning) for this analysis+limit to autosomal chromosomes only; cryptic relatedness can interfere with the association analysis — if you have a family-based sample (e.g., parent-offspring), don’t need to remove related pairs but should take family relatedness into account. for a population based sample suggested threshold is \\(\\hat{\\pi}=0.2\\)\n\n\npopulation stratification\n--genome, --cluster --mds-plot k\ncalculates IBD** of all sample pairs; produces \\(k\\)-dimensional representation of any substructure in data based on IBS\n\\(K\\)= # dimensions (to be defined; usually 10)\n\n\n\n\n*genotype calls: estimation of one unique SNP or genotype\n**identity by descent: matching segment of DNA shared by two or more people that has been inherited from a common ancestor without any intervening recombination\n\nControlling for population stratification\npopulation stratification: presence of multiple subpopulations (e.g., individuals with different ethnic background) in a study\n\ncan lead to false (+) associations and/or mask true associations\ne.g. chopstick gene (where a SNP due to population stratification accounted for nearly half of the variance in capacity to eat w. chopsticks)\nmay exist even within a single ethnic population\n\nOne method to correct: multidimensional scaling (MDS) approach — calculate genome-wide average proportion of alleles shared between any pair of individuals within sample to generate quantitative indices (components) of genetic variation for each individual. Individual component scores can be plotted to explore whether there are groups of individuals that are genetically more similar to each other than expected.\n\nplot scores of sample under investigation+population of known ethnic structure (such as HapMap/1KG data) — anchoring\nenables researcher to obtain ethnic info on their sample+determine possible ethnic outliers\noutliers can be removed; conduct new MDS analysis\n\nmain components used as covariates in association tests in order to correct for any remaining population stratification in population\n\n\n\n\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001694/bin/MPR-27-e1608-g003.jpg\n\n\nStatistical tests of association\nAfter QC+calculation of MDS components, select appropriate statistical test:\n\n1 df allelic test (trait value or log-odds of a binary trait increases linearly as a function of number of risk alleles — minor allele a vs major allele A)\n\neach allele becomes the statistical unit instead of an individual (main problem with this approach is that we would be ignoring the induced correlation in the data by the fact that we are, in some sense, “double counting” the observations)\n\nnon-additive tests*\n\ngenotypic association test (2 df — aa vs Aa vs AA)\ndominant gene action test (1 df — [aa & Aa] vs AA)\nrecessive gene action test (1 df — aa vs [Aa & AA])\n*not widely applied b/c power to detect non-additivity is low\n\n\nBinary outcome measure\n\nassociation between SNPs and a binary outcome (1=unaffected; 2=affected; 0/-9=missing) tested with --assoc (\\(\\chi^2\\) test of association — cannot include covariates) or --logistic (logistic regression analysis — allows covariates but more computational time)\n\nQuantitative outcome measure\n\nassociation between SNPs and quantitative outcome measures can be tested w/ assoc (automatically treats as numerical by performing asymptotic version of student’s \\(t\\) test to compare 2 means) and --linear (linear regression analysis w/ each individual SNP as a predictor; enables use of covariates but slower)\n\nCorrection for multiple testing\n\nmultiple testing burden since genotyping arrays can genotype up to 4 million markers concurrently, generating large # of tests\n3 widely applied alternatives for determining genome-wide significance:\n\nBonferroni: good for controlling Type I error rate; adjusted p w/ formula \\(\\frac{\\alpha}{n}\\) (where \\(\\alpha\\) is original p-value; here it is 0.05 and \\(n\\) is # SNPs tested)\n\nmany SNPs correlated due to linkage disequilibrium (LD) and thus not independent — too harsh of a correction and could lead to increase in proportion of false negative findings\n\nBenjamini-Hochberg: controls expected proportion of FP among all signals with FDR value below fixed threshold\n\nFDR method assumes SNPs are independent\nlimitation: SNPs and thus p values are not independent\n\n\n\n--adjust outputs unadjusted p value along w/ p values corrected w/ various multiple testing correction methods\n\n\npermutation methods: outcome measure labels randomly permuted mutiple times to remove any true association between outcome measure and genotype\n\nstatistical tests performed for all permuted datasets; compare empirical distribution of test statistic and p values under null to original test statistic\n--assoc and --mperm combined to generate 2 p-values: EMP1 (empirical p value — uncorrected) and EMP2 (empirical p value corrected for multiple testing)\n\n\n\nPRS analysis\nComputing a PRS\n\nsingle variant association analysis is the primary methdo in GWAS but needs large sample size to detect SNPs for many complex traits\nPRS aggregates genetic risk across genome in single individual polygenic score for a trait of interest\nlarge discovery sample required to reliably dtermine how much each SNP is expected to contribute to polygenic score (”weights”) of a specific trait\nin an independent target sample (doesn’t need to be as big) polygenic scores calculated based on genetic DNA profiles+these weights — around 200 subjects provides sufficient power to detect significan proportion of variance explained\ndiscovery+target samples should have same # subjects until the target sample includes 2,000 subjects\nmore available samples → additional subjects should be included in discovery sample to maximize accuracy of estimation of effect sizes\nhas been successfully used to show significant associations both within+across traits\n\ne.g. a significant association with disease risk was found despite fact that the available sample sizes were too small to detect genome-wide significant SNPs\n\n\n\n\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001694/bin/MPR-27-e1608-g004.jpg\n\n\n\nto conduct PRS analysis, trait specific weights (\\(\\beta\\)s for continuous traits+log of odds ratio for binary traits) are obtained from a discovery GWAS\n\nin target sample, PRS is calculated for each individual based on weighted sum of # risk alleles that he or she carries multiplied by trait-specific weights (SNP effect sizes publicly available for many complex traits)\n\nall common SNPs could be used in a PRS analysis, but it is customary to clump first (only the most significant SNP (i.e., lowest p* value) in each LD block is identified and selected for further analyses) the GWAS results, then compute risk scores\np value thresholds typically used to remove SNPs that show little or no statistical evidence for association\nmultiple PRS analyses will be performed with varying thresholds for p values\n\nConducting polygenic risk prediction analyses\n\nonce PRS has been calculated for all subjects in target sample, use scores in logistic reg analysis → predict trait expected to show genetic overlap w/ trait of interest\n\nprediction accuracy expressed w/ \\(R^2\\) measure of regression analysis (include at least a few MDS components as covariates in regression analysis to control for population stratification)\n\nto estimate how much variation is explained by PRS, \\(R^2\\) of a model that includes covariates+PRS will be compared\n\nincrease in \\(R^2\\) due to PRS indicates increase in prediction accuracy explained by genetic risk factors\n\nprediction accuracy of PRS depends mostly on (co-)heritability of analyzed traits, the number of SNPs, and the size of discovery sample\n\nsize of target sample only affects reliability of \\(R^2\\)"
  },
  {
    "objectID": "posts/nn_3b1b/index.html",
    "href": "posts/nn_3b1b/index.html",
    "title": "Neural networks (3b1b)",
    "section": "",
    "text": "original tutorial link\nNeuron: thing that holds a number between 0 and 1** (see revised definition at the end)\n\nex. MNIST dataset — each neuron is one of the 28x28 pixels, holding a number between 0 and 1 representing grayscale value — activation\n\nwhen activation is a high number, neuron is “lit up”\n\noutput layer: each neuron has activation between 0 and 1 representing how likely it is to be each digit (between 0-9)\nhidden layers in between: ? — don’t know what’s going on there yet\n\nActivations in one layer determine activations in the next layer, loosely analogous to how some group of neurons (biological) firing causes some other group of neurons to fire.\n\nEx. if you feed in an image lighting up all 784 neurons of the input layer according to the brightness of each pixel of the image, that pattern of acitvations causes some very specific pattern in next layer → next layer → some pattern in output layer → brightest neuron of output layer is prediction for the digit\n\nWhy use layers?\n\nnot all connections are equal — some might be stronger than others\ngoal/hope:\n\nhumans piece together various components like loops+lines when recognizing digits\nex. w/ NN, feed in an image w/ a loop up top → there is some specific neuron whose activation will be close to 1.0\n\nhow to recognize subcomponents? recognize various edges that make it up:\n\nhope is that each neuron in second layer of network corresponds to some little edge; when image comes in, lights up neurons associated w/ all specific little edges inside that image → light up neurons in third layer associated w/ larger scale patterns like loops+long lines → cause some neuron from final layer to fire which corresponds to the appropriate digit\n\n\nLayers break problems into small pieces\n\nhow would acitvations in one layer determine activations in the next?\ngoal: have some mehcanism that coudl combine pixels into edges → edges into patterns → patterns into digits\nex. want neuron in second layer to pick up on whether or not the image has an edge in a particular region\n→ Q: what parameters should exist?\n\nassign weight to each one of connections between our neuron+neurons from first layer\n\nex. w1, w2, w3…\n\nweights are just numbers → take all activations from first layer+compute weighted sum according to these weights\n\nw1a1+w2a2+w3a3+….wnan\n\nthink of weights as organized into a grid of their own (blue=positive, red=negative weights); brightness of pixel=loose depiction of weight value\n\nmake weights associated w/ almost all of the pixels 0 except for some positive weights associated w/ these pixels in region where we want to detect an edge\n\n→ take weighted sum = adding up values of pixels in our region of interest\n\nthis pattern of weights will also pick up on big blobs of activated pixels (not just edges); make sure to pick up on whether or not this is an edge by having some negative weights associated w/ surrounding pixels\n\n\n→ sum will be largest when these pixels are bright and surrounding pixels are dark\n\n\n\nSigmoid squishification\n\nresult of weighted sum like this can be any number; this example (categorical) we want activations to be values between 0 and 1\nsolution: pump weighted sum into some fn that squishes real number line into range between 0 and 1\ncommon fn example: sigmoid fn (aka logistic curve)\n\\[\n  \\sigma(x)=\\frac{1}{1+e^{-x}}\n  \\]\n\nvery neg inputs end up close to 0; very pos inputs close to 1\nsteadily increases around 0\n\nback to examaple: activation of the neuron here is measure of how positive the relevant weighted sum is\nmaybe don’t want neuron to light up when sum is greater than 0; what if you only want it to be active when the sum is &gt; 10?\n\n→ you want some bias for it to be active\nsolution: add in bias — some other number (such as -10) to the weighted sum before plugging it through the sigmoid fn\n\nbias tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active\n\n\n\\[\n  \\sigma(w_1a_1+w_2a_2+w_3a_3+\\cdots+w_na_n\\color{green}-10\\color{black})\n  \\]\nbetween just 2 layers, each neuron will have its own weights and its own bias\n\nin this example:\n\n\\(784\\times 16+16\\times 16+16\\times 10\\;\\text{weights}\\)\n\\(16+16+10\\; \\text{biases}\\)\n\ntotal: 13,002 weights and biases\n\n\nNotation review\n\nexample: actual function for first neuron of second layer:\n\\[\n  a_0^{(1)}=\\sigma(w_{0,0}a_0^{(0)}+w_{0,1}a_1^{(0)}+\\cdots+w_{0,n}a_n^{(0)}+b_0)\n  \\]\n\n\norganize all activations from one layer into a column as a vector\n\n\\[\n\\begin{pmatrix}\na_0^{(0)}\\\\\na_1^{(0)}\\\\\n\\vdots\\\\\na_n^{(0)}\n\\end{pmatrix}\n\\]\n\norganize all weights as a matrix — each row vector: weights for connections between one layer and a single neuron in next layer\n\n\nex. the first row in matrix below represents connections between first layer and first neuron of second layer\n\n\\[\n\\color{purple}\n\\begin{pmatrix}\nw_{0,0}&w_{0,1}&\\dots&w_{0,n}\\\\\nw_{1,0}&w_{1,1}&\\dots&w_{1,n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nw_{k,0}&w_{k,1}&\\dots&w_{k,n}\n\\end{pmatrix}\n\\]\n\neach component of matrix-vector product is weighted sum of one of the sets of weights\n\n\nex. taking weighted sum of activations in first layer according to weights for the first neuron of second layer corresponds to one of the terms in matrix vector product (the first one), and so on\n\n\\[\n\\color{purple}\\begin{pmatrix}\nw_{0,0}&w_{0,1}&\\dots&w_{0,n}\\\\\nw_{1,0}&w_{1,1}&\\dots&w_{1,n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nw_{k,0}&w_{k,1}&\\dots&w_{k,n}\n\\end{pmatrix}\n\\color{black}\n\\begin{pmatrix}\na_0^{(0)}\\\\\na_1^{(0)}\\\\\n\\vdots\\\\\na_n^{(0)}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n?\\\\\n?\\\\\n\\vdots\\\\\n?\n\\end{pmatrix}\n\\]\n\ninstead of adding bias to each value independently, organize biases into vector and add that whole vector to previous matrix-vector product\n\n\\[\n\\color{purple}\\begin{pmatrix}\nw_{0,0}&w_{0,1}&\\dots&w_{0,n}\\\\\nw_{1,0}&w_{1,1}&\\dots&w_{1,n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nw_{k,0}&w_{k,1}&\\dots&w_{k,n}\n\\end{pmatrix}\\color{black}\n\\begin{pmatrix}\na_0^{(0)}\\\\\na_1^{(0)}\\\\\n\\vdots\\\\\na_n^{(0)}\n\\end{pmatrix}\n+\n\\color{green}\n\\begin{pmatrix}\nb_0\\\\\nb_1\\\\\n\\vdots\\\\\nb_n\n\\end{pmatrix}\n\\]\n\napply sigmoid fn to each specific component of the resulting vector inside\n\n\\[\n\\color{teal}\\sigma\\left[\\color{purple}\\begin{pmatrix}\nw_{0,0}&w_{0,1}&\\dots&w_{0,n}\\\\\nw_{1,0}&w_{1,1}&\\dots&w_{1,n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nw_{k,0}&w_{k,1}&\\dots&w_{k,n}\n\\end{pmatrix}\\color{black}\n\\begin{pmatrix}\na_0^{(0)}\\\\\na_1^{(0)}\\\\\n\\vdots\\\\\na_n^{(0)}\n\\end{pmatrix}\n+\n\\color{green}\n\\begin{pmatrix}\nb_0\\\\\nb_1\\\\\n\\vdots\\\\\nb_n\n\\end{pmatrix}\\color{teal}\\right]\n\\]\n\\[\n\\color{teal}\\sigma(\\mathbf{\\color{purple}W\\color{black}a^{(0)}}\\color{black}+\\color{green}\\mathbf{b}\\color{teal})\n\\]\ncommunicates full transition of activations from one layer to the next\nUpshot\nnew definition of neuron: a function — one takes in outputs of all the neurons in the previous layer and spits out a number between 0 and 1 (in this example)\nThe entire net is a (very complicated) fn — takes in 784 numbers as input and spits out 10 numbers as output.\n\\[\nf(a_0,\\dots,a_{783})=\\begin{pmatrix}\ny_0\\\\\n\\vdots\\\\\ny_9\n\\end{pmatrix}\n\\]\nnetwork → fn"
  },
  {
    "objectID": "posts/pytorch_train_classifier/index.html",
    "href": "posts/pytorch_train_classifier/index.html",
    "title": "PyTorch tutorial: training classifier demo",
    "section": "",
    "text": "original tutorial link\nWhat about data?\ncan use standard python packages that load data into np array → convert array into torch.*Tensor\nUseful packages:\n\nimages: Pillow, OpenCV\naudio: scipy, librosa\ntext: raw Python or Cython based loading, or NLTK, SpaCy\n\nFor vision: torchvision package has data loader for common datasets like ImageNet, CIFAR10, MNIST, etc. and data transformers for images, viz., torchvision.datasets and torch.utils.data.DataLoader\nDemo — ex: CIFAR10 dataset\n\n\nClasses: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\nimages are of size 3x32x32 (3 channel color images of 32x32 pixels)\n\nTraining an image classifier\nSteps:\n\nLoad+normalize CIFAR10 training+test datasets using torchvision\nDefine CNN\nDefine loss fn\nTrain net on training data\nTest net on test data\n\n(1) Load+normalize CIFAR10\nimport torchvision\nimport torchvision.transforms as transforms\noutput of torchvision datasets: PILImage images of range [0, 1] → transform to Tensors of normalized range [-1, 1]\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 4\n#check memory \n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n(2) Define CNN\nsame as NN code from NN demo — modify to take 3-channel images rather than 1 channel\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n#previously 1 channel:\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        # kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square, you can specify with a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n#now 3 channel:\nclass Net(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # 3 input image channels, 6 output channels, 5x5 square convolution\n        # kernel\n        self.conv1 = nn.Conv2d(3, 6, 5)\n                # Max pooling over a (2, 2) window\n                self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        # If the size is a square, you can specify with a single number\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\n(3) Define loss fn+optimizer\nex. classification cross-entropy loss+SGD w/ momentum\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n(4) Train net\nloop over data iterator+feed inputs to network/optimize\nbelow: 2 passes\nfor epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n            running_loss = 0.0\n\nprint('Finished Training')\ncan save trained model:\nPATH = './cifar_net.pth'\ntorch.save(net.state_dict(), PATH)\ninfo on serialization\n\nsaving+loading NN modules:\nfrequently serialized using a “state dict” — conains all of its parameters+persistent buffers\n\nbn = torch.nn.BatchNorm1d(3, track_running_stats=True)\nlist(bn.named_parameters())\n\n#output:\n#[('weight', Parameter containing: tensor([1., 1., 1.], requires_grad=True)),\n# ('bias', Parameter containing: tensor([0., 0., 0.], requires_grad=True))]\n\nlist(bn.named_buffers())\n\n#output:\n#[('running_mean', tensor([0., 0., 0.])),\n# ('running_var', tensor([1., 1., 1.])),\n# ('num_batches_tracked', tensor(0))]\n\n#state dict serialization:\nbn.state_dict()\n\n#output:\n#OrderedDict([('weight', tensor([1., 1., 1.])),\n#             ('bias', tensor([0., 0., 0.])),\n#             ('running_mean', tensor([0., 0., 0.])),\n#             ('running_var', tensor([1., 1., 1.])),\n#             ('num_batches_tracked', tensor(0))])\n\nrecommended to save only state dict — fn load_state_dict() can restore states from a state dict\n\ntorch.save(bn.state_dict(), 'bn.pt')\n#torch.load to load state dict from file\nbn_state_dict = torch.load('bn.pt') \nnew_bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n#restore w/ load_state_dict()\nnew_bn.load_state_dict(bn_state_dict)\n(5) Test net\ncheck if net has learnt anything — predict class label that net outputs+check against ground-truth\n\nif prediction is correct → add sample to list of correct predictions\n\n(if loading is necessary)\nnet = Net()\nnet.load_state_dict(torch.load(PATH))\noutputs: energies for 10 classes — higher energy = net thinks that the image is of the particular class more\noutputs = net(images)\nex. get index of highest energy:\n_, predicted = torch.max(outputs, 1)\n\nprint('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n                              for j in range(4)))\nnow look at whole dataset:\ncorrect = 0\ntotal = 0\n# since we're not training, don't need to calculate gradients for outputs\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = net(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n#accuracy of net on 10000 test images:\n100 * correct // total\n#compare to chance\n\n#chance: 10% accuracy\ncheck classes that performed well vs not well:\n#prepare to count preds for each class\ncorrect_pred = {classname: 0 for classname in classes}\ntotal_pred = {classname: 0 for classname in classes}\n\n#no grads needed\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predictions = torch.max(outputs, 1)\n        # collect the correct predictions for each class\n        for label, prediction in zip(labels, predictions):\n            if label == prediction:\n                correct_pred[classes[label]] += 1\n            total_pred[classes[label]] += 1\n\n#print accuracy for each class\nfor classname, correct_count in correct_pred.items():\n        accuracy = 100*float(correct_count) / total_pred[classname]\n        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\nTraining on GPU\ncan transfer NN onto GPU\n\ndefine device as first visible cuda device if we have CUDA available\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n# Assuming that we are on a CUDA machine, this should print a CUDA device:\nprint(device)\nrecursively go over all modules+conver params/buffers to CUDA tensors:\nnet.to(device)\n\n#send inputs+targets at every step to GPU too:\ninputs, labels = data[0].to(device), data[1].to(device)\nExercise: Try increasing the width of your network (argument 2 of the first nn.Conv2d, and argument 1 of the second nn.Conv2d – they need to be the same number), see what kind of speedup you get.\npreviously: width of 6\n\n\n\nnn width of 6\n\n\nnow: width of 20\n\n\n\nnn width of 20\n\n\nwider network → faster training time"
  },
  {
    "objectID": "posts/z2h_micrograd/index.html",
    "href": "posts/z2h_micrograd/index.html",
    "title": "z2h 1. Intro to NN+backprop: building micrograd",
    "section": "",
    "text": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom graphviz import Digraph\n\nimport torch\nfrom torch import Tensor\n\nimport random\n\n#from micrograd.engine import Value\n\nmicrograd: small autograd engine – implements backprop+allows you to efficiently evaluate grad of loss fn wrt weights of a NN\n\ncan iteratively tune weights of NN to minimize loss fn+improve accuracy of net\nallows you to build expressions (by wrapping values into value objects)\n\nthen can tranform the values and perform math operations\nbuilding out expression graph with inputs \\(\\rightarrow\\) create output\nany results of value objects in operations \\(\\rightarrow\\) also a value object\n\nex. add value objects \\(a\\) and \\(b\\) to get result \\(c\\) (see below)\n\nchild nodes of \\(c\\) will be \\(a\\) and \\(b\\)\ngo through all these operations to output of our forward pass (\\(g\\)) – access value using .data attribute\nthen take \\(g\\) and call .backward() to initizalize backprop at the node \\(g\\)\n\ngoes backwards through expression graph and recursively apply chain rule\ncan evaluate derivative of g wrt all internal nodes (such as \\(c\\), \\(d\\), \\(e\\)) and also inputs (\\(a\\) and \\(b\\))\nthen we can query derivative of \\(g\\) wrt \\(a\\) (a.grad), \\(g\\) wrt \\(b\\)\ntells us how \\(a\\) and \\(b\\) are affecting \\(g\\) through mathematical expression\n\nhow \\(g\\) will respond if \\(a\\) and \\(b\\) get tweaked a tiny amount in positive direction\n\n\n\n\n\n\n\n#example usage\na = Value(-4.0)\nb = Value(2.0)\nc = a + b\nd = a*b + b**3\nc += c + 1\nc += 1 + c + (-a)\nd += d*2 + (b+a).relu()\nd += 3*d + (b-a).relu()\ne = c - d\nf = e**2\ng = f/2.0\ng += 10.0/f\nprint(f'{g.data:.4f}') #outcome of forward pass\ng.backward()\nprint(f'{a.grad:.4f}') #num value of dg/da\nprint(f'{b.grad:.4f}') #dg/db\n\n24.7041\n138.8338\n645.5773\n\n\nNN: just a mathematical expression\n\ninput data as an input\nweights of NN as an input\noutput: predictions of NN or loss fn\nbackprop not specific to NN\n\nmicrograd: scalar valued autograd engine; works on levels of individual scalars (normally don’t write out every mathematical expression; this is just a demonstration without using tensors)\n\nnormally use tensors; run parallel (faster)\n\nBreaking down code for micrograd (very simple):\n\nengine.py: actual backprop engine (not to do w/ NN)\nnn.py: entire NN library; only need to define:\n\nwhat a neuron is\nlayer of neurons\nmulti-layer perceptron (MLP); just a sequence of layers of neurons\n\n\n\n#define scalar-valued fn\ndef f(x):\n    \"\"\"quadratic fn that takes single scalar x, returns single scalar y\"\"\"\n    return 3*x**2 - 4*x + 5\n\n\nf(3.0)\n\n20.0\n\n\n\nxs = np.arange(-5,5,0.25)\nys = f(xs)\nplt.plot(xs,ys)\n\n\n\n\nWhat is the derivative of this fn at any single input \\(x\\)?\n\non paper – write it out by hand and find derivative\nhowever NNs too complex for that; just understand what derivative is telling you about this fn\ndef derivative: at some point \\(a\\) if you slightly increase it by a small number \\(h\\), how does the fn respond? (w/ what sensitivity; up or down)\n\nEvaluate derivative numerically:\n\nh = 0.00001\nx = 3.0\nf(x+h) #expected to be slightly greater than 20 aka f(x)\n\n20.0001400003\n\n\n\n#slightly greater...by how much? \nf(x+h) - f(x) \n#how much fn responded in the positive direction \n\n0.0001400003000000538\n\n\n\n#normalize by run to get slope (rise/run)\n(f(x+h) - f(x))/h\n\n14.00003000000538\n\n\ncan make \\(h\\) smaller and smaller to converge to the exact amount, but at some point we’ll get incorrect answer (0) b/c representation of these numbers in computer memory is finite\n\n#slope at x=-3\nx = -3.0\n(f(x+h) - f(x))/h\n\n-21.999970000052823\n\n\nif we take the slope at \\(x=-3\\), expect something negative\nat some point, slope will be 0 (for this fn, at \\(x=2/3\\))\n\n#slope at x=2/3\nx = 2/3\n(f(x+h) - f(x))/h\n#nudge in positive direction -&gt; fn doesn't respond\n\n3.0000002482211127e-05\n\n\nNow a more complex example:\n\na = 2.0\nb = -3.0\nc = 10.0\nd = a*b + c\nprint(d)\n\n4.0\n\n\nslope below will be \\(d_2-d_1\\) – how much fn increased when we bump the specific input we’re interested in by a tiny amount; then normalized by \\(h\\)\n\n#look at derivatives of d wrt a, b, c\nh = 0.0001\n\n#fix inputs\na = 2.0\nb = -3.0\nc = 10.0\n\nd1 = a*b + c\n\n#derivative of d wrt a\na += h #bump a by h -- a is slightly more positive\nd2 = a*b + c #b is negative --&gt; expect value of fn to go down\n\nprint('d1',d1)\nprint('d2',d2)\nprint('slope',(d2-d1)/h)\n\nd1 4.0\nd2 3.999699999999999\nslope -3.000000000010772\n\n\nwe know that the slope is -3 mathematically as well; taking derivative of \\(ab+c\\) will give us \\(b\\), which is -3\n\n#now wrt b\nh = 0.0001\n\n#fix inputs\na = 2.0\nb = -3.0\nc = 10.0\n\nd1 = a*b + c\n\n#derivative of d wrt b\nb += h #bump b by h -- b is slightly more positive\nd2 = a*b + c #a is positive --&gt; expect value of fn to go up\n\nprint('d1',d1)\nprint('d2',d2)\nprint('slope',(d2-d1)/h)\n\nd1 4.0\nd2 4.0002\nslope 2.0000000000042206\n\n\nabove, slope is 2 b/c derivative wrt \\(b\\) is \\(a\\), which is 2\n\n#now wrt c\nh = 0.0001\n\n#fix inputs\na = 2.0\nb = -3.0\nc = 10.0\n\nd1 = a*b + c\n\n#derivative of d wrt c\nc += h #bump c by h -- value will go up by the exact smae amount we added to c\nd2 = a*b + c #d will increase by scale of 1 when this happens\n\nprint('d1',d1)\nprint('d2',d2)\nprint('slope',(d2-d1)/h)\n\nd1 4.0\nd2 4.0001\nslope 0.9999999999976694\n\n\nmoving to NN – massive expressions, so we need data structures to maintain expressions\nbuilt out value object\n\n#simple value object skeleton\nclass Value:\n    \"\"\"wraps+tracks a single scalar value\"\"\"\n    def __init__(self, data):\n        self.data = data\n    \n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\nCreate Value objects:\n\na = Value(2.0)\nb = Value(-3.0)\na + b\n\nTypeError: unsupported operand type(s) for +: 'Value' and 'Value'\n\n\nReturns TypeError b/c Python doesn’t know how to add 2 value objects\nTell it how using double underscores to define operators:\n\n#defining some operators\nclass Value:\n    \"\"\"wraps+tracks a single scalar value\"\"\"\n    def __init__(self, data):\n        self.data = data\n    \n    def __repr__(self):\n        \"\"\"print out nicer looking expression; actual value of the object\"\"\"\n        return f\"Value(data={self.data})\"\n    \n    #returns a new object that represents the sum of two objects\n    def __add__(self, other):\n        \"\"\"returns new value obj wrapping sum of the data float points\"\"\"\n        out = Value(self.data + other.data)\n        return out\n    \n    def __mul__(self, other):\n        out = Value(self.data*other.data)\n        return out\n\n\na = Value(2.0)\nb = Value(-3.0)\nc = Value(10.0)\na*b + c\n\n#equivalent to (manually):\n#(a.__mul__(b)).__add__(c)\n\nValue(data=4.0)\n\n\nStill missing “connective tissue” of this expression; want pointers to show what values produce other values (want to keep expression graphs)\nWhen creating a value obj through addition or multiplication, feed in children of this value object (self and other)\n\nclass Value:\n    #adding children (by default an empty tuple)\n    \"\"\"wraps+tracks a single scalar value; \"\"\"\n    def __init__(self, data, _children=()):\n        self.data = data\n        self._prev = set(_children) #empty set if unspecified\n    \n    def __repr__(self):\n        \"\"\"print out nicer looking expression; actual value of the object\"\"\"\n        return f\"Value(data={self.data})\"\n    \n    #children of value obj created through addition should be self and other\n    def __add__(self, other):\n        \"\"\"returns new value obj wrapping sum of the data float points\"\"\"\n        out = Value(self.data + other.data, (self, other))\n        return out\n    \n    def __mul__(self, other):\n        out = Value(self.data*other.data, (self, other))\n        return out\n\n\na = Value(2.0)\nb = Value(-3.0)\nc = Value(10.0)\nd = a*b + c\nd\n\nValue(data=4.0)\n\n\n\nd._prev\n#outputs value -6 (value resulting from a*b), value 10 (resulting from c)\n\n{Value(data=-6.0), Value(data=10.0)}\n\n\nWe now know children that created this value object, but not the operation\nnew parameter _op\n\nclass Value:\n    #want to track operation, not just children that create the value object\n    \"\"\"wraps+tracks a single scalar value; \"\"\"\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self._prev = set(_children) #empty set if unspecified\n        self._op = _op\n    \n    def __repr__(self):\n        \"\"\"print out nicer looking expression; actual value of the object\"\"\"\n        return f\"Value(data={self.data})\"\n    \n    #children of value obj created through addition should be self and other\n    def __add__(self, other):\n        \"\"\"returns new value obj wrapping sum of the data float points\"\"\"\n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n    \n    def __mul__(self, other):\n        out = Value(self.data*other.data, (self, other), '*')\n        return out\n\n\na = Value(2.0)\nb = Value(-3.0)\nc = Value(10.0)\nd = a*b + c\nd\n\nValue(data=4.0)\n\n\nVisualizing this expression graph\n\ndef trace(root):\n    \"\"\"enumerates vertices/edges; build set of all vertices+edges in graph\"\"\"\n    vertices, edges = set(), set()\n    def build(vertex):\n        if vertex not in vertices:\n            vertices.add(vertex)\n            for child in vertex._prev:\n                edges.add((child, vertex))\n                build(child)\n    build(root)\n    return vertices, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir':'LR'}) #LR = left to right\n\n    vertices, edges = trace(root)\n    for vertex in vertices:\n        vertex_id = str(id(vertex))\n        #creating a record node for any value in graph\n        dot.node(name=vertex_id, label=\"{ data %.4f }\" % (vertex.data, ), shape='record')\n        #if it's a result of some operation, create an op node for it\n        if vertex._op:\n             dot.node(name = vertex_id + vertex._op, label = vertex._op)\n             #connect to this vertex\n             #create edge\n             dot.edge(vertex_id + vertex._op, vertex_id)\n    \n    for vertex_1, vertex_2 in edges:\n        #connect vertex 1 to op node of vertex 2\n        dot.edge(str(id(vertex_1)), str(id(vertex_2)) + vertex_2._op)\n\n    return dot\n\ncan call draw_dot on some root vertex (such as \\(d\\))\nfrom left to right: - we get \\(a*b\\) creating an integrated value (-6) - plus \\(c\\) gives us output \\(d\\) (4) - rectangles are actual objects (connect op nodes – not actual nodes)\n\ndraw_dot(d)\n\n\n\n\n\nclass Value:\n    #add labels to graph\n    \"\"\"wraps+tracks a single scalar value; \"\"\"\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self._prev = set(_children) #empty set if unspecified\n        self._op = _op\n        self.label = label\n    \n    def __repr__(self):\n        \"\"\"print out nicer looking expression; actual value of the object\"\"\"\n        return f\"Value(data={self.data})\"\n    \n    #children of value obj created through addition should be self and other\n    def __add__(self, other):\n        \"\"\"returns new value obj wrapping sum of the data float points\"\"\"\n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n    \n    def __mul__(self, other):\n        out = Value(self.data*other.data, (self, other), '*')\n        return out\n\n\na = Value(2.0, label=\"a\")\nb = Value(-3.0, label=\"b\")\nc = Value(10.0, label=\"c\")\ne = a*b; e.label=\"e\"\nd = e + c; d.label=\"d\"\nd\n\nValue(data=4.0)\n\n\n\n#modify to include labels\ndef trace(root):\n    \"\"\"enumerates vertices/edges; build set of all vertices+edges in graph\"\"\"\n    vertices, edges = set(), set()\n    def build(vertex):\n        if vertex not in vertices:\n            vertices.add(vertex)\n            for child in vertex._prev:\n                edges.add((child, vertex))\n                build(child)\n    build(root)\n    return vertices, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir':'LR'}) #LR = left to right\n\n    vertices, edges = trace(root)\n    for vertex in vertices:\n        vertex_id = str(id(vertex))\n        #creating a record node for any value in graph\n        dot.node(name=vertex_id, label=\"{ %s | data %.4f }\" % (vertex.label, vertex.data), shape='record')\n        #if it's a result of some operation, create an op node for it\n        if vertex._op:\n             dot.node(name = vertex_id + vertex._op, label = vertex._op)\n             #connect to this vertex\n             #create edge\n             dot.edge(vertex_id + vertex._op, vertex_id)\n    \n    for vertex_1, vertex_2 in edges:\n        #connect vertex 1 to op node of vertex 2\n        dot.edge(str(id(vertex_1)), str(id(vertex_2)) + vertex_2._op)\n\n    return dot\n\n\ndraw_dot(d)\n\n\n\n\nMaking expression 1 layer deeper\n\na = Value(2.0, label=\"a\")\nb = Value(-3.0, label=\"b\")\nc = Value(10.0, label=\"c\")\n\ne = a*b; e.label=\"e\"\n#d no longer output\nd = e + c; d.label=\"d\"\nf = Value(-2.0, label=\"f\")\nL = d*f; L.label=\"L\" #output\nL\n\nValue(data=-8.0)\n\n\n\ndraw_dot(L)\n\n\n\n\nSo far, build out mathematical expressions (scalar valued) using addition and multiplication that produces a single output\n\nmultiple inputs: \\(a\\), \\(b\\), \\(c\\), \\(f\\)\n\nabove: visualizing output of the forward pass (-8); want to also run backprop\n\nstart at the end (at \\(L\\)) and reverse/calculate gradient along all intermediate values\ncompute derivative of \\(L\\) wrt each of those nodes\n\nstart at \\(L\\); derivative wrt itself is 1 (\\(\\frac{dL}{dL} = 1\\))\n\nthen wrt \\(f\\), \\(d\\), \\(c\\), etc\n\nIn NN setting: interested in finding derivative of a loss fn \\(L\\) wrt weights of NN (leaf nodes)\n\nother leaf nodes: the data itself\n\ndata is fixed, so don’t want derivative wrt data (whereas weights are iterative)\n\n\nMake a variable grad that is initially 0 (no effect)\n\nassume that this variable does not impact loss fn\n\n\nclass Value:\n    #adding a new variable: grad\n    \"\"\"wraps+tracks a single scalar value; \"\"\"\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._prev = set(_children) #empty set if unspecified\n        self._op = _op\n        self.label = label\n    \n    def __repr__(self):\n        \"\"\"print out nicer looking expression; actual value of the object\"\"\"\n        return f\"Value(data={self.data})\"\n    \n    #children of value obj created through addition should be self and other\n    def __add__(self, other):\n        \"\"\"returns new value obj wrapping sum of the data float points\"\"\"\n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n    \n    def __mul__(self, other):\n        out = Value(self.data*other.data, (self, other), '*')\n        return out\n\n\n#modify to include grad\ndef trace(root):\n    \"\"\"enumerates vertices/edges; build set of all vertices+edges in graph\"\"\"\n    vertices, edges = set(), set()\n    def build(vertex):\n        if vertex not in vertices:\n            vertices.add(vertex)\n            for child in vertex._prev:\n                edges.add((child, vertex))\n                build(child)\n    build(root)\n    return vertices, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir':'LR'}) #LR = left to right\n\n    vertices, edges = trace(root)\n    for vertex in vertices:\n        vertex_id = str(id(vertex))\n        #creating a record node for any value in graph\n        dot.node(name=vertex_id, label=\"{ %s | data %.4f | grad %.4f }\" % (vertex.label, vertex.data, vertex.grad), shape='record')\n        #if it's a result of some operation, create an op node for it\n        if vertex._op:\n             dot.node(name = vertex_id + vertex._op, label = vertex._op)\n             #connect to this vertex\n             #create edge\n             dot.edge(vertex_id + vertex._op, vertex_id)\n    \n    for vertex_1, vertex_2 in edges:\n        #connect vertex 1 to op node of vertex 2\n        dot.edge(str(id(vertex_1)), str(id(vertex_2)) + vertex_2._op)\n\n    return dot\n\n\na = Value(2.0, label=\"a\")\nb = Value(-3.0, label=\"b\")\nc = Value(10.0, label=\"c\")\ne = a*b; e.label=\"e\"\nd = e + c; d.label=\"d\"\nf = Value(-2.0, label=\"f\")\nL = d*f; L.label=\"L\" #output\n\ndraw_dot(L)\n\n\n\n\nAbove: grad label shows derivative of \\(L\\) wrt that specific vertex (e.g. wrt \\(c\\))\nNow fill in gradients – starting from the root:\n\nderivative of \\(L\\) wrt \\(L\\): if we change \\(L\\) by a little bit in the positive direction (\\(h\\)), how much will \\(L\\) change?\n\nproportional – changes by \\(h\\)\nderivative is 1\n\n\nsome examples:\n\n#gating fn -- avoid polluting global scope (like a staging area)\n\ndef lol():\n\n    h = 0.001\n\n    a = Value(2.0, label=\"a\")\n    b = Value(-3.0, label=\"b\")\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    d = e + c; d.label=\"d\"\n    f = Value(-2.0, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L1 = L.data #this is a float since L is a value object\n\n    #measuring derivative of L wrt a (bumping a)\n    a = Value(2.0 + h, label=\"a\")\n    b = Value(-3.0, label=\"b\")\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    d = e + c; d.label=\"d\"\n    f = Value(-2.0, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L2 = L.data\n\n    print((L2 - L1)/h)\n\nlol() #output: derivative of L wrt h = 6\n\n6.000000000000227\n\n\n\n#now change L by a small positive amount (h)\n\ndef lol():\n\n    h = 0.001\n\n    a = Value(2.0, label=\"a\")\n    b = Value(-3.0, label=\"b\")\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    d = e + c; d.label=\"d\"\n    f = Value(-2.0, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L1 = L.data #this is a float since L is a value object\n\n    #measuring derivative of L wrt L (bumping L)\n    a = Value(2.0, label=\"a\")\n    b = Value(-3.0, label=\"b\")\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    d = e + c; d.label=\"d\"\n    f = Value(-2.0, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L2 = L.data + h\n\n    print((L2 - L1)/h)\n\nlol() #output: derivative of L wrt L = 1 (\"base case\")\n\n1.000000000000334\n\n\nManual backprop – set L.grad to 1\n\nL.grad = 1.0\n\n\n#redraw graph\ndraw_dot(L)\n\n\n\n\nNow for derivative of \\(L\\) wrt \\(d\\) and \\(f\\)\nwrt \\(d\\) – what is \\(\\frac{dL}{dd}\\)? = \\(f\\)\nto write it out:\n\ndef of derivative – \\(lim_{h \\to 0} \\frac{f(x+h)-f(x)}{h}\\)\n\\(\\frac{d*f+h*f-d*f}{h}\\)\n\\(\\frac{h*f}{h}\\)\n\\(=f\\)\n\n\nf.grad = 4.0 #value of d\nd.grad = -2.0 #value of f\n\n\n#redraw w/ manually set grad values\ndraw_dot(L)\n\n\n\n\n\n#check -- want derivative L wrt f\n#now change L by a small positive amount (h)\n\ndef lol():\n\n    h = 0.001\n\n    a = Value(2.0, label=\"a\")\n    b = Value(-3.0, label=\"b\")\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    d = e + c; d.label=\"d\"\n    f = Value(-2.0, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L1 = L.data \n\n    a = Value(2.0, label=\"a\")\n    b = Value(-3.0, label=\"b\")\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    d = e + c; d.label=\"d\"\n    f = Value(-2.0 + h, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L2 = L.data\n\n    print((L2 - L1)/h)\n\nlol() #output: derivative of L wrt f (expect to see 4)\n\n3.9999999999995595\n\n\nlike a gradient check – deriving backprop + getting derivative w/ respect to all intermediate results\n\nnumerically verify – numerical gradient: estimate using small step size\n\n\n#dl/dd\n\n#check -- want derivative L wrt d -- expect to see 2\n\ndef lol():\n\n    h = 0.001\n\n    a = Value(2.0, label=\"a\")\n    b = Value(-3.0, label=\"b\")\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    d = e + c; d.label=\"d\"\n    f = Value(-2.0, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L1 = L.data \n\n    a = Value(2.0, label=\"a\")\n    b = Value(-3.0, label=\"b\")\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    d = e + c; d.label=\"d\"\n    d.data += h\n    f = Value(-2.0, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L2 = L.data\n\n    print((L2 - L1)/h)\n\nlol() #output: derivative of L wrt d (expect to see -2)\n\n-2.000000000000668\n\n\n\ndraw_dot(L)\n\n\n\n\nneed to find \\(\\frac{dL}{dc}\\)\n\nproblem: how to derive this?\nwhat we know: derivative of \\(L\\) wrt \\(d\\)\nhow is \\(L\\) sensitive to \\(c\\)? (if we perturb \\(c\\), how will \\(L\\) respond?)\n\nif we know \\(\\frac{dL}{dd}\\) and how \\(c\\) impacts \\(d\\), then we should be able to know how \\(c\\) impacts \\(L\\)\n\n\nWhat is \\(\\frac{dd}{dc}\\)?\n\nwe know that \\(d = c+e\\) so:\n\\(\\frac{f(x+h)-f(x)}{h}\\)\n\\(= \\frac{(c+h+e)-(c+e)}{h}\\)\n\\(= \\frac{h}{h} = 1\\)\nlocal derivative – only knows it took \\(c+e\\) (nothing else); embedded in giant graph\n\nHow to put this info together? chain rule\n\ndef: if a variable \\(z\\) depends on a variable \\(y\\), which itself depends on variable \\(x\\) (they are dependent variables), then \\(z\\) depends on \\(x\\) as well via intermediate variable \\(y\\)\nneed to multiply derivatives (these intermediate rates of change)\nintuitive example: if a car travels twice as fast as a bike and the bike is 4x as fast as a walking man, then the car travels \\(2\\times 4 = 8\\) times as fast as the man\n\nso in this case:\n\nwant: \\(\\frac{dL}{dc}\\)\nknow:\n\n\\(\\frac{dL}{dd}\\)\n\\(\\frac{dd}{dc}\\)\n\nchain rule:\n\n\\(\\frac{dL}{dc} = \\frac{dL}{dd}*\\frac{dd}{dc}\\)\n\\(=1*-2\\) – just 1 times derivative of \\(L\\) wrt \\(d\\)\n\nplus nodes – just rerouting gradient – route to \\(c\\) and \\(e\\)\n\n\nc.grad = -2.0\ne.grad = -2.0\n\n\ndef lol():\n\n    h = 0.001\n\n    a = Value(2.0, label=\"a\")\n    b = Value(-3.0, label=\"b\")\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    d = e + c; d.label=\"d\"\n    f = Value(-2.0, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L1 = L.data \n\n    a = Value(2.0, label=\"a\")\n    b = Value(-3.0, label=\"b\")\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    d = e + c; d.label=\"d\"\n    d.data += h\n    f = Value(-2.0, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L2 = L.data\n\n    print((L2 - L1)/h)\n\nlol()\n\n-2.000000000000668\n\n\n\ndraw_dot(L)\n\n\n\n\nnode distributes derivative to all leaf nodes -&gt; backwards flow\n\n#for e\ndef lol():\n\n    h = 0.001\n\n    a = Value(2.0, label=\"a\")\n    b = Value(-3.0, label=\"b\")\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    d = e + c; d.label=\"d\"\n    f = Value(-2.0, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L1 = L.data \n\n    a = Value(2.0, label=\"a\")\n    b = Value(-3.0, label=\"b\")\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    e.data += h\n    d = e + c; d.label=\"d\"\n    f = Value(-2.0, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L2 = L.data\n\n    print((L2 - L1)/h)\n\nlol()\n\n-2.000000000000668\n\n\nsecond application of chain rule: recurse backwards again;\nremaining node:\n\nhave: \\(\\frac{dL}{de}=-2\\)\n\\(\\frac{dL}{da}=\\frac{dL}{de}*\\frac{de}{da}\\)\n\\(e=a*b\\)\n\\(\\frac{de}{da}\\)?\nwe get \\(\\frac{de}{da}=b\\), which is -3\n\n\n#dL/da = dL/de * de/da = -2*b = -2*-3\na.grad = -2.0*-3.0 \n\n#dL/db = dL/de * de/db = -2*a = -2*2\nb.grad = -2.0*2.0 \n\n\ndraw_dot(L)\n\n\n\n\n\n#verify these grad values\n\n#for a\ndef lol():\n\n    h = 0.001\n\n    a = Value(2.0, label=\"a\")\n    b = Value(-3.0, label=\"b\")\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    d = e + c; d.label=\"d\"\n    f = Value(-2.0, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L1 = L.data \n\n    a = Value(2.0, label=\"a\")\n    a.data += h\n    b = Value(-3.0, label=\"b\")\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    d = e + c; d.label=\"d\"\n    f = Value(-2.0, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L2 = L.data\n\n    print((L2 - L1)/h)\n\nlol()\n\n6.000000000000227\n\n\n\n#for b\ndef lol():\n\n    h = 0.001\n\n    a = Value(2.0, label=\"a\")\n    b = Value(-3.0, label=\"b\")\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    d = e + c; d.label=\"d\"\n    f = Value(-2.0, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L1 = L.data \n\n    a = Value(2.0, label=\"a\")\n    b = Value(-3.0, label=\"b\")\n    b.data += h\n    c = Value(10.0, label=\"c\")\n    e = a*b; e.label=\"e\"\n    d = e + c; d.label=\"d\"\n    f = Value(-2.0, label=\"f\")\n    L = d*f; L.label=\"L\" \n    L2 = L.data\n\n    print((L2 - L1)/h)\n\nlol() #=-4\n\n-3.9999999999995595\n\n\nRecap: manual backprop by iterating through nodes one by one and finding local derivatives - we know what the derivative of \\(L\\) wrt to a little output - know derivatives within operation (local); have pointers to children nodes of this operation - what happens: just go through and recurisvely multiply local derivatives\n\n#below: leaf nodes (usu have more control over this)\n#nudging in direction of gradient -- expect L to go up positively\na.data += 0.01 * a.grad\nb.data += 0.01 * b.grad\nc.data += 0.01 * c.grad\nf.data += 0.01 * f.grad\n\n#forward pass\ne = a*b \nd = e + c\nL = d*f\n\nprint(L.data) #expect less negative value than -8\n\n-7.286496\n\n\nabove example: shows one step of optimization\nNow a more complex example: MLP\nbasic structure: - some inputs \\(x\\)’s and synapse with weights \\(w\\)’s - synpase and inputs interact so many \\(x\\)’s times \\(w\\)’s - bias adds innate “trigger happy” mode - put through activation function; squashing\n\n#ex: tanh fn\nplt.plot(np.arange(-5,5,0.2), np.tanh(np.arange(-5,5,0.2))); plt.grid();\n\n#at 0 we get exactly zero; fn goes up to 1 and plateaus; same w/ -1\n\n\n\n\nessentially what happens in NN is activation fn applied to dot product of inputs and weights\n\n#inputs x1, x2 (2 dimensional neuron)\nx1 = Value(2.0, label=\"x1\")\nx2 = Value(0.0, label=\"x2\")\n\n#weights (synaptic strength of input)\nw1 = Value(-3.0, label=\"w1\")\nw2 = Value(1.0, label=\"w2\")\n\n#bias\nb = Value(6.7, label=\"b\")\n\n#x1*w1 + x2*w2 + b\nx1w1 = x1*w1; x1w1.label = \"x1*w1\"\nx2w2 = x2*w2; x2w2.label = \"x2*w2\"\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = \"x1*w1 + w2*w2\"\nn = x1w1x2w2 + b; n.label = \"n\"\n\ndraw_dot(n)\n\n\n\n\ntake this through activation fn\n\nex. \\(\\tanh x\\) is a hyperbolic fn (requires exponentiation)\n\n\\(\\tanh x = \\frac{e^{2x}-1}{e^{2x}+1}\\)\n\n#add exponentiation fn\nclass Value:\n    \"\"\"wraps+tracks a single scalar value; \"\"\"\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._prev = set(_children) #empty set if unspecified\n        self._op = _op\n        self.label = label\n    \n    def __repr__(self):\n        \"\"\"print out nicer looking expression; actual value of the object\"\"\"\n        return f\"Value(data={self.data})\"\n    \n    #children of value obj created through addition should be self and other\n    def __add__(self, other):\n        \"\"\"returns new value obj wrapping sum of the data float points\"\"\"\n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n    \n    def __mul__(self, other):\n        out = Value(self.data*other.data, (self, other), '*')\n        return out\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n        out = Value(t, (self, ), 'tanh')\n        return out\n\nas long as you know how to find the local derivative, can be as simple or complex of a fn as you want\n\n#inputs x1, x2 (2 dimensional neuron)\nx1 = Value(2.0, label=\"x1\")\nx2 = Value(0.0, label=\"x2\")\n\n#weights (synaptic strength of input)\nw1 = Value(-3.0, label=\"w1\")\nw2 = Value(1.0, label=\"w2\")\n\n#bias\nb = Value(6.7, label=\"b\")\n\n#x1*w1 + x2*w2 + b\nx1w1 = x1*w1; x1w1.label = \"x1*w1\"\nx2w2 = x2*w2; x2w2.label = \"x2*w2\"\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = \"x1*w1 + w2*w2\"\nn = x1w1x2w2 + b; n.label = \"n\"\no = n.tanh() #now that we have defined this method above in value class\ndraw_dot(o) #draw output\n\n\n\n\nNow \\(\\tanh\\) is the micro-grad supported node as an operation+as long as we know its derivative, we can backpropagate. Currently (above) it’s not squashing too much (from 0.7 to 0.6); try increasing bias to 8:\n\nx1 = Value(2.0, label=\"x1\")\nx2 = Value(0.0, label=\"x2\")\n\nw1 = Value(-3.0, label=\"w1\")\nw2 = Value(1.0, label=\"w2\")\n\n#higher value for bias now\nb = Value(8, label=\"b\")\n\nx1w1 = x1*w1; x1w1.label = \"x1*w1\"\nx2w2 = x2*w2; x2w2.label = \"x2*w2\"\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = \"x1*w1 + w2*w2\"\nn = x1w1x2w2 + b; n.label = \"n\"\no = n.tanh() \ndraw_dot(o) \n\n\n\n\nAbove, we can see that it is being squashed more (from 2 to 0.96)\nchanging \\(b\\) again (for simplicity’s sake):\n\nx1 = Value(2.0, label=\"x1\")\nx2 = Value(0.0, label=\"x2\")\n\nw1 = Value(-3.0, label=\"w1\")\nw2 = Value(1.0, label=\"w2\")\n\n#for pretty numbers\nb = Value(6.8813735870195432, label=\"b\")\n\nx1w1 = x1*w1; x1w1.label = \"x1*w1\"\nx2w2 = x2*w2; x2w2.label = \"x2*w2\"\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = \"x1*w1 + x2*w2\"\nn = x1w1x2w2 + b; n.label = \"n\"\no = n.tanh(); o.label = \"o\"\ndraw_dot(o) \n\n\n\n\nNow for backprop, need to find derivative of \\(o\\) wrt all the inputs\n\nwhat we care about: derivative wrt the weights \\(w_1\\) and \\(w_2\\) since those will be changing during optimization\n\n“base case” = \\(\\frac{do}{do} = 1\\)\n\n#do/do = 1\no.grad = 1.0 \n\n\n#do/dn = 1-tanh(n)^2 = 1-o^2\n1 - o.data**2 #=0.5\n\n0.4999999999999999\n\n\n\nn.grad = 0.5\n\n\ndraw_dot(o)\n\n\n\n\nNow to find derivative wrt plus operation (\\(b\\) and \\(x_1w_1+x_2w_2\\))\n\nrecall: plus: just a distributor of gradient\ngrad simply flows to both of these equally\nlocal derivative of this operation is 1 for every one of its nodes (and multiply this by 0.5 to get 0.5)\n\n\nx1w1x2w2.grad = 0.5\nb.grad = 0.5\n\nNext plus operation:\n\nx1w1.grad = 0.5\nx2w2.grad = 0.5\n\n\ndraw_dot(o)\n\n\n\n\nAbove – What the derivative is telling us at every point in time along this graph is that if we want the output to increase, the influence on these expressions is positive on the output; both are positively contributing to the output (0.5 is positive)\nNext:\n\nmultiplication operation – we know the derivative will just be the other term (multiplied by “previous” gradient of \\(x_2w_2\\))\nfirst backpropagate to \\(x_2\\) and \\(w_2\\)\n\n\nx2.grad = w2.data*x2w2.grad #local piece of chain rule\nw2.grad = x2.data*x2w2.grad\n\n\ndraw_dot(o)\n\n\n\n\nAbove: gradient on \\(w_2\\) is 0 b/c \\(x_2\\)’s value (x2.data) is 0. Gradient on \\(x_2\\) is 0.5 b/c \\(w_2\\)’s value is 1.\nintuitively: derivative always tells usinfuence of this input on the output\n\ngrad on \\(w_2\\) will be 0 b/c if you wiggle it, output will not change either\n\nNext: \\(x_1\\) and \\(w_1\\)\n\n#same idea:\nx1.grad = w1.data*x1w1.grad\nw1.grad = x1.data*x1w1.grad\n\n\ndraw_dot(o)\n\n\n\n\nNow have manually backpropagated across the whole graph – what if we want neuron’s output to increase?\nLook at the weights:\n\n\\(w_2\\) won’t do anything (grad of 0)\nhowever, \\(w_1\\) can be perturbed \\(\\rightarrow\\) output will go up if \\(w_1\\) increases\n\nStart codifying this (no more manual backprop):\ngoal: take out’s grad and propagate it into self’s grad and other’s grad\nAddition:\n\nself.grad = ?? and other.grad = ??\ntake local derivative and multiply it by “global” derivative (derivative of final output of expression wrt out aka “previous upstream” derivative)\n\nSo we know that self.grad = 1 times grad of “global derivative” \\(\\rightarrow\\) 1.0*out.grad\nMultiplication:\n\nlocal derivative of self will be other.data multiplied by out.grad\nand vice versa\n\n\n#store a function that does a little piece of the chain rule; chain output gradients into input gradients \n#by default, ._backward will not do anything (leaf node)\n#if creating new Value object from an operation, need to specify what ._backward does\n\nclass Value:\n    \"\"\"wraps+tracks a single scalar value; \"\"\"\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 #grad initialized at 0 -- \"base case\"\n        self._backward = lambda: None #in the case we are at a leaf node\n        self._prev = set(_children) \n        self._op = _op\n        self.label = label\n    \n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n    \n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad = 1.0*out.grad #derivative of itself will be 1\n            other.grad = 1.0*out.grad\n        out._backward = _backward\n\n        return out\n    \n    def __mul__(self, other):\n        out = Value(self.data*other.data, (self, other), '*')\n\n        def _backward():\n            self.grad = other.data*out.grad\n            other.grad = self.data*out.grad\n        out._backward = _backward\n\n        return out\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n        out = Value(t, (self, ), 'tanh')\n\n        def _backward():\n            self.grad = (1 - t**2) * out.grad\n        out._backward = _backward\n\n        return out\n\nCall ._backward in the right order:\n\nstart with all grads at 0\nwant to call backward, starting at output \\(o\\)\n\n\nx1 = Value(2.0, label=\"x1\")\nx2 = Value(0.0, label=\"x2\")\n\nw1 = Value(-3.0, label=\"w1\")\nw2 = Value(1.0, label=\"w2\")\n\nb = Value(6.8813735870195432, label=\"b\")\n\nx1w1 = x1*w1; x1w1.label = \"x1*w1\"\nx2w2 = x2*w2; x2w2.label = \"x2*w2\"\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = \"x1*w1 + x2*w2\"\nn = x1w1x2w2 + b; n.label = \"n\"\no = n.tanh(); o.label = \"o\"\ndraw_dot(o) \n\n\n\n\nNeed to set o.grad to 1 since it is initialized at 0\n\no.grad = 1.0\n\n\no._backward()\n\n\ndraw_dot(o)\n\n\n\n\nCall n._backward()\n\nn._backward()\n\nand so on until we get to leaf nodes:\n\nb._backward()\n\n\ndraw_dot(o)\n\n\n\n\n\nx1w1x2w2._backward()\n\n\ndraw_dot(o)\n\n\n\n\n\nx2w2._backward()\nx1w1._backward()\n\n\ndraw_dot(o)\n\n\n\n\nAbove: completed gradients (same answers as when we manually did it)\nNow – how to do this without having to manually call ._backward each time?\ntrying to go backwards through an expression…\n\ni.e. don’t want to call ._backward for any node until everything after has been done\nto order this (provide the direction): topological sort\n\nlaying out of a graph st all edges only go one way\nbuild topological order for propagation\n\n\nStart by setting o.grad = 1.0\n\no.grad = 1.0 #base case\n\nBuild topological order\n\ntopo = []\nvisited_vertices = set() #set of visited vertices\ndef build_topo(vertex):\n    if vertex not in visited_vertices:\n        visited_vertices.add(vertex)\n        for child in vertex._prev:\n            build_topo(child)\n        topo.append(vertex)\n\n#start at some root node (eg o) and go through all of its children; then add itself\n\n\n#invariant being maintained: o will only add itself to topo list after all of children have been processed\nbuild_topo(o)\n\n\nfor vertex in reversed(topo): #reversed order, starting at root node\n    vertex._backward()\n\nLooking at expression graph again:\n\ndraw_dot(o)\n\n\n\n\nHiding the functionality; define as part of Value class\n\n\n#defining an actual .backward method\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 \n        self._backward = lambda: None \n        self._prev = set(_children) \n        self._op = _op\n        self.label = label\n    \n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n    \n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad = 1.0*out.grad\n            other.grad = 1.0*out.grad\n        out._backward = _backward\n\n        return out\n    \n    def __mul__(self, other):\n        out = Value(self.data*other.data, (self, other), '*')\n\n        def _backward():\n            self.grad = other.data*out.grad\n            other.grad = self.data*out.grad\n        out._backward = _backward\n\n        return out\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n        out = Value(t, (self, ), 'tanh')\n\n        def _backward():\n            self.grad = (1 - t**2) * out.grad\n        out._backward = _backward\n\n        return out\n    \n    def backward(self):\n        topo = []\n        visited_vertices = set()\n        def build_topo(vertex):\n            if vertex not in visited_vertices:\n                visited_vertices.add(vertex)\n                for child in vertex._prev:\n                    build_topo(child)\n                topo.append(vertex)\n        build_topo(self)\n\n        self.grad = 1.0\n        for vertex in reversed(topo):\n            vertex._backward()\n\n\nx1 = Value(2.0, label=\"x1\")\nx2 = Value(0.0, label=\"x2\")\n\nw1 = Value(-3.0, label=\"w1\")\nw2 = Value(1.0, label=\"w2\")\n\nb = Value(6.8813735870195432, label=\"b\")\n\nx1w1 = x1*w1; x1w1.label = \"x1*w1\"\nx2w2 = x2*w2; x2w2.label = \"x2*w2\"\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = \"x1*w1 + x2*w2\"\nn = x1w1x2w2 + b; n.label = \"n\"\no = n.tanh(); o.label = \"o\"\n\n\no.backward()\n\n\ndraw_dot(o) #completed without manually calling ._backward\n\n\n\n\nIssue: calling \\(b = a+a\\) will result in forward pass but incorrect gradient\n\n#ex. (wrong gradient)\na = Value(3.0, label=\"a\")\nb = a + a; b.label = \"b\"\nb.backward()\ndraw_dot(b)\n\n\n\n\ngrad of \\(b\\) wrt \\(a\\) should be 2 (since \\(b=2a\\)), not 1.\nwhat’s happening here:\n\ncall .backward() on \\(b\\)\n\\(b\\) is the sum of \\(a\\) and \\(a\\), so we end up setting self.grad = 1.0*out.grad and other.grad = 1.0*out.grad\nend up setting gradient = 1, then overriding it and setting it to 1 again (since \\(a\\)’s are the same) \\(\\rightarrow\\) our grad will stay at 1\n\nanother example:\n\na = Value(-2.0, label=\"a\")\nb = Value(3.0, label=\"b\")\nd = a*b; d.label = \"d\"\ne = a+b; e.label = \"e\"\nf = d*e; f.label = \"f\"\n\nf.backward()\ndraw_dot(f)\n\n\n\n\nAbove, gradients are incorrect b/c we run into problems once we use the same variable(s) multiple times (ends up overwriting the gradients)\nsolution: multivariate way of conducting the chain rule; accumulate gradients using +=\n\n\n#make sure grads accumulate in case we reuse any variables in our expressions\n#instead of overwriting \"previous\" gradient, it adds it on top\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 \n        self._backward = lambda: None \n        self._prev = set(_children) \n        self._op = _op\n        self.label = label\n    \n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n    \n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0*out.grad\n            other.grad += 1.0*out.grad\n        out._backward = _backward\n\n        return out\n    \n    def __mul__(self, other):\n        out = Value(self.data*other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data*out.grad\n            other.grad += self.data*out.grad\n        out._backward = _backward\n\n        return out\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n        out = Value(t, (self, ), 'tanh')\n\n        def _backward():\n            self.grad += (1 - t**2) * out.grad\n        out._backward = _backward\n\n        return out\n    \n    def backward(self):\n        topo = []\n        visited_vertices = set()\n        def build_topo(vertex):\n            if vertex not in visited_vertices:\n                visited_vertices.add(vertex)\n                for child in vertex._prev:\n                    build_topo(child)\n                topo.append(vertex)\n        build_topo(self)\n\n        self.grad = 1.0\n        for vertex in reversed(topo):\n            vertex._backward()\n\n\n#ex. (now correct)\na = Value(3.0, label=\"a\")\nb = a + a; b.label = \"b\"\nb.backward()\ndraw_dot(b)\n\n\n\n\n\n#another example (now correct)\na = Value(-2.0, label=\"a\")\nb = Value(3.0, label=\"b\")\nd = a*b; d.label = \"d\"\ne = a+b; e.label = \"e\"\nf = d*e; f.label = \"f\"\n\nf.backward()\ndraw_dot(f)\n\n\n\n\nBroke down \\(\\tanh\\) into its explicit atoms; now can try breaking it down into a fn of \\(x\\)\nCurrently can’t add constant to a value object (Python is trying to access 1.data but that dne since 1 is just an integer)\n\na = Value(2.0)\na+1\n\nAttributeError: 'int' object has no attribute 'data'\n\n\n\n\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 \n        self._backward = lambda: None \n        self._prev = set(_children) \n        self._op = _op\n        self.label = label\n    \n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n    \n    def __add__(self, other):\n        #let other alone if it is another Value objet; but if it's an integer or float, wrap it into a value object so we can add them\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0*out.grad\n            other.grad += 1.0*out.grad\n        out._backward = _backward\n\n        return out\n    \n    def __mul__(self, other):\n        out = Value(self.data*other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data*out.grad\n            other.grad += self.data*out.grad\n        out._backward = _backward\n\n        return out\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n        out = Value(t, (self, ), 'tanh')\n\n        def _backward():\n            self.grad += (1 - t**2) * out.grad\n        out._backward = _backward\n\n        return out\n    \n    def backward(self):\n        topo = []\n        visited_vertices = set()\n        def build_topo(vertex):\n            if vertex not in visited_vertices:\n                visited_vertices.add(vertex)\n                for child in vertex._prev:\n                    build_topo(child)\n                topo.append(vertex)\n        build_topo(self)\n\n        self.grad = 1.0\n        for vertex in reversed(topo):\n            vertex._backward()\n\n\na = Value(2.0)\na + 1\n\nValue(data=3.0)\n\n\nnow do same for multiplication\n\n\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 \n        self._backward = lambda: None \n        self._prev = set(_children) \n        self._op = _op\n        self.label = label\n    \n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n    \n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0*out.grad\n            other.grad += 1.0*out.grad\n        out._backward = _backward\n\n        return out\n    \n    def __mul__(self, other):\n        #allows us to multiply by a constant without it being a value object originally\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data*other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data*out.grad\n            other.grad += self.data*out.grad\n        out._backward = _backward\n\n        return out\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n        out = Value(t, (self, ), 'tanh')\n\n        def _backward():\n            self.grad += (1 - t**2) * out.grad\n        out._backward = _backward\n\n        return out\n    \n    def backward(self):\n        topo = []\n        visited_vertices = set()\n        def build_topo(vertex):\n            if vertex not in visited_vertices:\n                visited_vertices.add(vertex)\n                for child in vertex._prev:\n                    build_topo(child)\n                topo.append(vertex)\n        build_topo(self)\n\n        self.grad = 1.0\n        for vertex in reversed(topo):\n            vertex._backward()\n\n\na = Value(2.0)\na*2\n\nValue(data=4.0)\n\n\n\n#however this won't work:\n2*a \n\n#under the hood:\n#2.__mul__(a) which won't work\n\nTypeError: unsupported operand type(s) for *: 'int' and 'Value'\n\n\nsolution: define __rmul__ method; will check if\n\n\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 \n        self._backward = lambda: None \n        self._prev = set(_children) \n        self._op = _op\n        self.label = label\n    \n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n    \n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0*out.grad\n            other.grad += 1.0*out.grad\n        out._backward = _backward\n\n        return out\n    \n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data*other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data*out.grad\n            other.grad += self.data*out.grad\n        out._backward = _backward\n\n        return out\n    \n    #swaps order of operands\n    def __rmul__(self, other): #other*self\n        return self*other\n    \n    def __radd__(self, other): #other + self\n        return self + other\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n        out = Value(t, (self, ), 'tanh')\n\n        def _backward():\n            self.grad += (1 - t**2) * out.grad\n        out._backward = _backward\n\n        return out\n    \n    def exp(self):\n        x = self.data\n        out = Value(math.exp(x), (self, ), 'exp')\n    \n        def _backward():\n            self.grad += out.data * out.grad \n        out._backward = _backward\n    \n        return out\n    \n    def backward(self):\n        topo = []\n        visited_vertices = set()\n        def build_topo(vertex):\n            if vertex not in visited_vertices:\n                visited_vertices.add(vertex)\n                for child in vertex._prev:\n                    build_topo(child)\n                topo.append(vertex)\n        build_topo(self)\n\n        self.grad = 1.0\n        for vertex in reversed(topo):\n            vertex._backward()\n\n\na = Value(2.0)\n2*a \n\nValue(data=4.0)\n\n\n\na = Value(2.0)\na.exp()\n\nValue(data=7.38905609893065)\n\n\n\n#returns TypeError\na = Value(2.0)\nb = Value(4.0)\na/b\n\nTypeError: unsupported operand type(s) for /: 'Value' and 'Value'\n\n\nImplementing something more powerful than division\n\nwant a/b to give us 0.5\nreshuffle since dividing \\(a\\) by \\(b\\) is the same as multiplying \\(a\\) and \\(b^{-1}\\)\n\noperation \\(x^k\\) for some constant \\(k\\); as a special case, \\(k=-1\\) for division\n\ndefine .__truediv__ and .__pow__\n\n\n\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 \n        self._backward = lambda: None \n        self._prev = set(_children) \n        self._op = _op\n        self.label = label\n    \n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n    \n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0*out.grad\n            other.grad += 1.0*out.grad\n        out._backward = _backward\n\n        return out\n    \n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data*other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data*out.grad\n            other.grad += self.data*out.grad\n        out._backward = _backward\n\n        return out\n    \n    def __pow__(self, other): #defining powers\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data**other, (self, ), f'**{other}')\n\n        def _backward():\n            self.grad += other*(self.data**(other - 1))*out.grad #just chain rule again\n        out._backward = _backward\n\n        return out\n    \n    def __rmul__(self, other): #other*self\n        return self*other\n    \n    def __radd__(self, other): #other + self\n        return self + other\n    \n    def __truediv__(self, other): #self/other\n        return self * other**-1\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n        out = Value(t, (self, ), 'tanh')\n\n        def _backward():\n            self.grad += (1 - t**2) * out.grad\n        out._backward = _backward\n\n        return out\n    \n    def exp(self):\n        x = self.data\n        out = Value(math.exp(x), (self, ), 'exp')\n    \n        def _backward():\n            self.grad += out.data * out.grad\n        out._backward = _backward\n    \n        return out\n    \n    def backward(self):\n        topo = []\n        visited_vertices = set()\n        def build_topo(vertex):\n            if vertex not in visited_vertices:\n                visited_vertices.add(vertex)\n                for child in vertex._prev:\n                    build_topo(child)\n                topo.append(vertex)\n        build_topo(self)\n\n        self.grad = 1.0\n        for vertex in reversed(topo):\n            vertex._backward()\n\n\na = Value(2.0)\nb = Value(4.0)\na/b #now shows what we want -- forward pass works\n\nValue(data=0.5)\n\n\ndefine subtraction operation via implementation of addition of a negation; to implement negation, multiplication by -1 (using things we’ve already built)\n\n\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 \n        self._backward = lambda: None \n        self._prev = set(_children) \n        self._op = _op\n        self.label = label\n    \n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n    \n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0*out.grad\n            other.grad += 1.0*out.grad\n        out._backward = _backward\n\n        return out\n    \n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data*other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data*out.grad\n            other.grad += self.data*out.grad\n        out._backward = _backward\n\n        return out\n    \n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data**other, (self, ), f'**{other}')\n\n        def _backward():\n            self.grad += other*(self.data**(other - 1))*out.grad \n        out._backward = _backward\n\n        return out\n    \n    def __neg__(self): #-self; implement via multiplication\n        return self*-1\n\n    def __sub__(self, other): #self-other; implement via addition of negation\n        return self + (-other)\n    \n    def __rmul__(self, other): #other*self\n        return self*other\n    \n    def __radd__(self, other): #other + self\n        return self + other\n    \n    def __truediv__(self, other): #self/other\n        return self * other**-1\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n        out = Value(t, (self, ), 'tanh')\n\n        def _backward():\n            self.grad += (1 - t**2) * out.grad\n        out._backward = _backward\n\n        return out\n    \n    def exp(self):\n        x = self.data\n        out = Value(math.exp(x), (self, ), 'exp')\n    \n        def _backward():\n            self.grad += out.data * out.grad\n        out._backward = _backward\n    \n        return out\n    \n    def backward(self):\n        topo = []\n        visited_vertices = set()\n        def build_topo(vertex):\n            if vertex not in visited_vertices:\n                visited_vertices.add(vertex)\n                for child in vertex._prev:\n                    build_topo(child)\n                topo.append(vertex)\n        build_topo(self)\n\n        self.grad = 1.0\n        for vertex in reversed(topo):\n            vertex._backward()\n\n\nx1 = Value(2.0, label=\"x1\")\nx2 = Value(0.0, label=\"x2\")\n\nw1 = Value(-3.0, label=\"w1\")\nw2 = Value(1.0, label=\"w2\")\n\nb = Value(6.8813735870195432, label=\"b\")\n\nx1w1 = x1*w1; x1w1.label = \"x1*w1\"\nx2w2 = x2*w2; x2w2.label = \"x2*w2\"\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = \"x1*w1 + x2*w2\"\nn = x1w1x2w2 + b; n.label = \"n\"\no = n.tanh(); o.label = \"o\"\no.backward()\n\n\ndraw_dot(o) \n\n\n\n\nnow take this and break \\(\\tanh\\) down into just a fn of \\(x\\)\n\nx1 = Value(2.0, label=\"x1\")\nx2 = Value(0.0, label=\"x2\")\n\nw1 = Value(-3.0, label=\"w1\")\nw2 = Value(1.0, label=\"w2\")\n\nb = Value(6.8813735870195432, label=\"b\")\n\nx1w1 = x1*w1; x1w1.label = \"x1*w1\"\nx2w2 = x2*w2; x2w2.label = \"x2*w2\"\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = \"x1*w1 + x2*w2\"\nn = x1w1x2w2 + b; n.label = \"n\"\n# -----\ne = (2*n).exp()\no = (e-1)/(e+1)\n# -----\no.label = \"o\"\no.backward()\n\n#expect to see a much longer graph now that tanh has been broken up into multiple operations\n#forward and backward pass should work in the same way\n\n\ndraw_dot(o)\n\n\n\n\nUpshot: the level at which you implement your operations is up to you – can do tiny expressions or more specific fns like \\(\\tanh\\)\nDoing this with modern NN libraries (such as PyTorch)\n\npreviously: using micrograd, a scalar-valued engine\nTensors: \\(n\\)-dimensional arrays of scalars\n\n\n#ex: create tensor (2x3 array of scalars in a single compact representation)\nTensor([[1, 2, 3], [4, 5, 6]])\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.]])\n\n\n\nTensor([[1, 2, 3], [4, 5, 6]]).shape #2x3\n\ntorch.Size([2, 3])\n\n\n\n#tensor with only a single element -&gt; cast it to be double so it is float64 instead of float32 (default in python)\nTensor([2.0]).double().dtype #similar to Value(2.0)\n\ntorch.float64\n\n\nDefault: these are leaf nodes – Python assumes all tensors don’t need gradients – here, specify that requires_grad = True\n\nUsually don’t need gradients for leaf nodes (more efficient)\n\nOnce we define our values, can perform arithmetic like we did with micrograd; get back a tensor with .data attribute and .grad attribute\n\none difference: need to take .item() of .data or .grad attribute in order to strip out the single-element tensor and just return the element\n\n\n#construct scalar-valued one element tensors that require gradients\nx1 = Tensor([2.0]).double()                ; x1.requires_grad = True\nx2 = Tensor([0.0]).double()                ; x2.requires_grad = True\nw1 = Tensor([-3.0]).double()               ; w1.requires_grad = True\nw2 = Tensor([1.0]).double()                ; w2.requires_grad = True\nb = Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\nn = x1*w1 + x2*w2 + b\no = torch.tanh(n)\n\nprint(o.data.item())\no.backward()\n\n#gradients\nprint('---')\nprint('x2', x2.grad.item())\nprint('w2', w2.grad.item())\nprint('x1', x1.grad.item())\nprint('w1', w1.grad.item())\n\n0.7071066904050358\n---\nx2 0.5000001283844369\nw2 0.0\nx1 -1.5000003851533106\nw1 1.0000002567688737\n\n\nCan do lots of these operations in parallel on all these tensors \\(\\rightarrow\\) high efficiency of PyTorch\nBuilding NNs\n\nstarting with 1 neuron\n\n\nclass Neuron:\n  \n    #constructor: takes # inputs to this neuron as a parameter\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)] #weight\n        self.b = Value(random.uniform(-1,1)) #bias\n\n    #w*x + b (dot product)\n    def __call__(self, x):\n        return 0.0 #temporary \n\n#what does __call__ do?\n#ex:\n\nx = [2.0, 3.0]\nn = Neuron(2) #2d neuron\n#feed 2 numbers into neuron to get output:\nn(x) #under the hood: python returns __call__\n\n0.0\n\n\nTo do forward pass of this neuron:\n\nmultiply all elements of \\(w\\) by all elements of \\(x\\) pairwise\n\nmaking this more efficient:\n\nsum() takes in an optional parameter start that by default is – added to sum\ncan put self.b there instead\n\n\nclass Neuron:\n  \n    #constructor: takes # inputs to this neuron as a parameter\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)] #weight\n        self.b = Value(random.uniform(-1,1)) #bias\n\n    #w*x + b (dot product)\n    def __call__(self, x):\n        #takes 2 iterators+creates new iterator that iterates over tuples of corresponding entries\n        #activation\n        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n        out = act.tanh()\n        return out\n\nNow define a new class: layer of neurons\n\nevaluated independently (not connected to each other) but all are fully connected to input\n\n\nclass Neuron:\n  \n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)] #weight\n        self.b = Value(random.uniform(-1,1)) #bias\n\n    def __call__(self, x):\n        \"\"\"w*x + b\"\"\"\n        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n        out = act.tanh()\n        return out\n\nclass Layer:\n  \n  def __init__(self, nin, nout):\n    self.neurons = [Neuron(nin) for _ in range(nout)]\n  \n  def __call__(self, x):\n    outs = [n(x) for n in self.neurons]\n    return outs\n\n\nx = [2.0, 3.0]\nn = Neuron(2)\nn(x)\n\nValue(data=-0.9982415984044078)\n\n\n\nx = [2.0, 3.0]\nn = Layer(2, 3) #2 inputs, 3 outputs\nn(x)\n\n[Value(data=0.9735622174316104),\n Value(data=-0.9999103412811101),\n Value(data=-0.9759477445597775)]\n\n\nNow define an entire MLP\nLayers feed into each other sequentially\n\nclass Neuron:\n  \n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)] #weight\n        self.b = Value(random.uniform(-1,1)) #bias\n\n    def __call__(self, x):\n        \"\"\"w*x + b\"\"\"\n        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n        out = act.tanh()\n        return out\n\nclass Layer:\n  \n  def __init__(self, nin, nout):\n    self.neurons = [Neuron(nin) for _ in range(nout)]\n  \n  def __call__(self, x):\n    outs = [n(x) for n in self.neurons]\n    return outs\n\n#instead of taking a single nout (# of neurons in a singl layer) want a list nouts with sizes of all layers we want in our mlp\nclass MLP:\n  \n  def __init__(self, nin, nouts):\n    sz = [nin] + nouts\n    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n  \n  #call sequentially\n  def __call__(self, x):\n    for layer in self.layers:\n      x = layer(x)\n    return x\n\n\n#forward pass in an MLP\nx = [2.0, 3.0, -1.0] #in puts\nn = MLP(3, [4, 4, 1]) #3 inputs, 2 layers of 4 neurons, 1 output\nn(x)\n\n[Value(data=-0.41001114452646087)]\n\n\nMake it look prettier – since the layer class will always return a call with a list…\n\nclass Neuron:\n  \n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)] #weight\n        self.b = Value(random.uniform(-1,1)) #bias\n\n    def __call__(self, x):\n        \"\"\"w*x + b\"\"\"\n        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n        out = act.tanh()\n        return out\n\nclass Layer:\n  \n    def __init__(self, nin, nout):\n      self.neurons = [Neuron(nin) for _ in range(nout)]\n  \n    def __call__(self, x):\n      outs = [n(x) for n in self.neurons]\n      return outs[0] if len(outs)==1 else outs #allows us to just get a single value at the last layer\n\nclass MLP:\n  \n    def __init__(self, nin, nouts):\n      sz = [nin] + nouts\n      self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n  \n  #call sequentially\n    def __call__(self, x):\n      for layer in self.layers:\n        x = layer(x)\n      return x\n\n\nx = [2.0, 3.0, -1.0] \nn = MLP(3, [4, 4, 1]) \nn(x)\n\nValue(data=0.39485893381196424)\n\n\n\ndraw_dot(n(x))\n\n\n\n\nCan backpropagate through all of this into the weights using micrograd\n\n#binary classifier NN\n\n#4 possible inputs into NN\nxs = [\n    [2.0, 3.0, -1.0],\n    [3.0, -1.0, 0.5],\n    [0.5, 1.0, 1.0],\n    [1.0, 1.0, -1.0]\n]\n\n#4 desired targets\nys = [1.0, -1.0, -1.0, 1.0] \n\n\n#outputs of NN on these 4 examples:\nypred = [n(x) for x in xs]\nypred\n\n[Value(data=0.39485893381196424),\n Value(data=0.7619356815122641),\n Value(data=0.6342471968220058),\n Value(data=0.5609407605888754)]\n\n\noutput:\n\n[Value(data=0.39485893381196424), Value(data=0.7619356815122641), Value(data=0.6342471968220058), Value(data=0.5609407605888754)]\nwant this to be closer to 1, -1, -1, 1\n\nhow to tune the weights to better predict desired targets?\n\nloss: calculate a single number that measures the total performance of your NN; measures how well your model is performing\n\nEx. MSE\n\n#pair up ground truths and predictions\n#final loss: sum of all these numbers:\nloss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\nloss\n\nValue(data=6.334149971806139)\n\n\nHow to minimize loss\n\nloss.backward() \n\n\nn.layers[0].neurons[0].w[0].grad #can look at grad for a single weight\n\n0.5004820034715549\n\n\n\ndraw_dot(loss)\n\n\n\n\nbackprop goes through and finds grad for all inputs\n\ninput scalars \\(x\\)’s are probably not changeable though\nweights more important\n\nWant to gather parameters of net so we can operate on all of them simultaneously+nudge a tiny amount\n\nclass Neuron:\n  \n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)] #weight\n        self.b = Value(random.uniform(-1,1)) #bias\n\n    def __call__(self, x):\n        \"\"\"w*x + b\"\"\"\n        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n        out = act.tanh()\n        return out\n    \n    def parameters(self):\n       \"\"\"returns a list of all ws and bs\"\"\"\n       return self.w + [self.b]\n\nclass Layer:\n  \n    def __init__(self, nin, nout):\n      self.neurons = [Neuron(nin) for _ in range(nout)]\n  \n    def __call__(self, x):\n      outs = [n(x) for n in self.neurons]\n      return outs[0] if len(outs)==1 else outs #allows us to just get a single value at the last layer\n    \n    def parameters(self):\n        \"\"\"take params of neuron and add to this list\"\"\"\n        return [p for neuron in self.neurons for p in neuron.parameters()]\n    \n        #same thing:\n        # params = []\n        # for neuron in self.neurons:\n        #     ps = neuron.parameters()\n        #     params.extend(ps)\n        # return params\n\nclass MLP:\n  \n    def __init__(self, nin, nouts):\n      sz = [nin] + nouts\n      self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n  \n    def __call__(self, x):\n      for layer in self.layers:\n        x = layer(x)\n      return x\n    \n    def parameters(self):\n        \"\"\"take params of layer and add to this list\"\"\"\n        return [p for layer in self.layers for p in layer.parameters()]\n\n\nx = [2.0, 3.0, -1.0] \nn = MLP(3, [4, 4, 1]) \nn(x)\n\nValue(data=0.7919463881036437)\n\n\n\nn.parameters() #returns all weights and biases of your NN\n\n[Value(data=-0.9801197195122864),\n Value(data=0.24559000590076874),\n Value(data=-0.2714268698568405),\n Value(data=0.6347365825489222),\n Value(data=0.7650871947814988),\n Value(data=0.7868093832274312),\n Value(data=0.2890033736061921),\n Value(data=0.8639412113809737),\n Value(data=-0.5085653462266857),\n Value(data=-0.5128056586030709),\n Value(data=0.1429648228800824),\n Value(data=-0.14770052696960634),\n Value(data=-0.03518311762506121),\n Value(data=-0.4240384732810216),\n Value(data=0.1686276505601787),\n Value(data=0.38380762005421376),\n Value(data=0.6831318842909966),\n Value(data=-0.9176880357284365),\n Value(data=0.21449731161504904),\n Value(data=-0.25433090170840167),\n Value(data=-0.5611664399444614),\n Value(data=-0.883697849628027),\n Value(data=-0.21990344721946786),\n Value(data=-0.36053666423910835),\n Value(data=0.9136541471667761),\n Value(data=-0.6601793564176934),\n Value(data=0.7147198357391136),\n Value(data=-0.12156235150511185),\n Value(data=0.045127533135210784),\n Value(data=0.2833392691409886),\n Value(data=-0.1264906497530689),\n Value(data=-0.6481900696204237),\n Value(data=0.674541951897913),\n Value(data=0.5631037197910671),\n Value(data=-0.1709184661590304),\n Value(data=-0.629614624014943),\n Value(data=-0.6735260357634596),\n Value(data=-0.301172263916456),\n Value(data=-0.7427848705051261),\n Value(data=0.7238871672756049),\n Value(data=-0.12247887349938513)]\n\n\n\nlen(n.parameters()) #41 parameters\n\n41\n\n\n\nxs = [\n    [2.0, 3.0, -1.0],\n    [3.0, -1.0, 0.5],\n    [0.5, 1.0, 1.0],\n    [1.0, 1.0, -1.0]\n]\nys = [1.0, -1.0, -1.0, 1.0] #desired targets\n\n\nypred = [n(x) for x in xs] #forward\nypred\n\n[Value(data=0.7919463881036437),\n Value(data=0.7201916145303128),\n Value(data=0.4533527289748783),\n Value(data=0.4768599362321087)]\n\n\n\n#loss\nloss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\nloss\n\nValue(data=5.388255177261324)\n\n\n\nloss.backward()\n\n\nn.layers[0].neurons[0].w[0].grad\n\n-0.12471425161578942\n\n\n\nn.layers[0].neurons[0].w[0].data\n\n-0.9801197195122864\n\n\niterate over 41 parameters and change p.data slightly based on information (tiny update)\ngradient in gradient descent: think of as vector pointing in direction of increased loss\n\nfor p in n.parameters():\n    p.data += -0.01*p.grad #step size = 0.01; negative to decrease loss\n\n\nn.layers[0].neurons[0].w[0].data\n\n-0.9788725769961285\n\n\n\n#recalculate forward pass -- compare to 5.388255177261324\nypred = [n(x) for x in xs]\nloss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\nloss #4.81465461280663 (this is a bit lower than before)\n\nValue(data=4.81465461280663)\n\n\nNow that we did forward pass, can do backward pass:\n\ncall loss.backward()\nstep size update\n\n\n#backward\nloss.backward()\n\n\n#update\nfor p in n.parameters():\n    p.data += -0.01*p.grad\n\nforward pass again… (with step size)\n\nypred = [n(x) for x in xs] #forward\nloss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred)) #new loss\n\n\nloss #lower than before\n\nValue(data=3.7029846198524257)\n\n\nbackward again… and so on\nJust iteratively doing forward pass, backward pass, update\n\nypred\n\n[Value(data=0.6132503547438835),\n Value(data=0.38573925521177194),\n Value(data=-0.011235826181103867),\n Value(data=0.1903819067708203)]\n\n\n\n#do this iteratively:\n\nfor k in range(20):\n    #forward pass\n    ypred = [n(x) for x in xs]\n    loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred)) #mse\n    \n    #backward pass\n    for p in n.parameters():\n        p.grad = 0.0\n    loss.backward()\n  \n    #simple sgd update\n    for p in n.parameters():\n        p.data += -0.1 * p.grad\n  \n    print(k, loss.data)\n\nCan increase step size to make this go faster but beware of overstepping\nUpshot\n\nNN = mathematical expressions that take input as data (weights+parameters)\nmathematical expression for forward pass, followed by loss function\nloss fn tries to measure accuracy of predictions (low if net is behaving well)\nbackward loss using backpropagation to get gradient\nknow how to tune parameters to decrease loss locally\niterate that process many times (gradient descent)\n\nOther notes:\n\npreviously just had a tiny dataset of 4 examples, but in the case where your data is a million examples, pick out a random subset (batch) and only process the batch through forward-backward-update (don’t need to forward the entire training set)\ncan make your own custom functions using class &lt;fn_name&gt;(torch.autograd.Function) as long as you can define forward (and know the local derivatives), PyTorch can backpropagate through your fn"
  },
  {
    "objectID": "posts/wine_nn/index.html",
    "href": "posts/wine_nn/index.html",
    "title": "Wine dataset MLP example",
    "section": "",
    "text": "from torch import nn, optim, from_numpy\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nReading in dataset – in this example, a built-in dataset was used\n\n\n#read in data\nwine = load_wine()\nX = from_numpy(wine.data).float()\ny = from_numpy(wine.target).long()\n\n\nSplit into test and train (note: may want to split into test, train, eval for more complex NNs)\n\n\nxtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.2, random_state=2023)\n\n\nInstantiating a custom dataset class\n\n\n#dataset class\nclass WineDataset(Dataset):\n    def __init__(self, x, y):\n        self.n_samples = x.shape[0] # len(y)\n        self.x = x\n        self.y = y\n    \n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n\n    def __len__(self):\n        return self.n_samples\n\n\nCreate test, train dataset objects and dataloader objects (this makes dividing it into batches easier)\n\n\ntrain_data = WineDataset(xtrain, ytrain)\ntest_data = WineDataset(xtest, ytest)\n\n\ntrain_loader = DataLoader(train_data, batch_size=8, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=8, shuffle=True)\n\n\nCreate NN class+define its structure – method 1 (predetermined # hidden layers). This example uses ReLU as activation fn, sigmoid as fn applied to output layer since the target variable is categorical\n\n\nclass WineNN(nn.Module):\n    def __init__(self, nf, hL1, hL2, hL3, nO):\n        super().__init__()\n        self.hL1 = nn.Linear(nf, hL1)\n        self.hL2 = nn.Linear(hL1, hL2)\n        self.hL3 = nn.Linear(hL2, hL3)\n        self.oL = nn.Linear(hL2, nO)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.hL1(x)\n        out = self.relu(out)\n        out = self.hL2(out)\n        out = self.relu(out)\n        out = self.hL3(out)\n        out = self.relu(out)\n        out = self.oL(out)\n        y = self.sigmoid(out)\n        return y\n\nmethod 2 (easily adjust # nodes per layer without having to worry about different # hidden layers each time)\n\nclass FlexNet(nn.Module):\n    def __init__(self, n_in, hidden_layers, n_out):\n        super().__init__()\n        self.input_layer = nn.Linear(n_in, hidden_layers[0])\n        self.hidden_layers = nn.ModuleDict(\n            {f\"hl{i}\":nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers)-1)}\n        )\n        self.output_layer = nn.Linear(hidden_layers[-1], n_out)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.input_layer(x)\n        out = self.relu(out)\n        for name, layer in self.hidden_layers.items():\n            out = self.relu(layer(out))\n        out = self.output_layer(out)\n        y = self.sigmoid(out)\n        return y\n\n\nCreate your NN object (using the custom class you made)\n\n\n#method 1\nwine_net = WineNN(13, 4, 4, 4, 3)\n\n\n#method 2\nflex_net = FlexNet(13, [4, 4, 4], 3)\n\n\nDefine your loss fn (criterion) and optimizer (SGD, adam, etc.)\n\n\n#method 1\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(wine_net.parameters(), lr=0.01)\n\n\n#method 2\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(flex_net.parameters(), lr=0.01)\n\n\nSpecify training hyperparameters+train model (probably using a loop); track loss over epochs\n\n\n#method 1\nn_epochs = 200\nbatch_loss = 0\nepoch_loss = []\n\nfor epoch in range(n_epochs):\n    batch_loss = 0\n    #each iteration = a single batch\n    for batch, (inputs, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        #forward pass\n        output = wine_net(inputs)\n        #get loss\n        loss = criterion(output, labels)\n        #grad\n        loss.backward()\n        optimizer.step()\n        #print(f\"iteration {i}: loss {loss}\")\n        batch_loss += loss.item()\n    #take average\n    epoch_loss.append(batch_loss/(batch+1))\n    print(f\"epoch {epoch}: loss {batch_loss/(batch+1)}\")\n\nepoch 0: loss 1.1035196118884616\nepoch 1: loss 1.1029477251900568\nepoch 2: loss 1.1027646130985684\nepoch 3: loss 1.1029533280266657\nepoch 4: loss 1.10271065764957\nepoch 5: loss 1.1025322212113275\nepoch 6: loss 1.1026486886872187\nepoch 7: loss 1.1021430558628507\nepoch 8: loss 1.1016399595472548\nepoch 9: loss 1.1020683579974704\nepoch 10: loss 1.1015860239664714\nepoch 11: loss 1.1014065543810527\nepoch 12: loss 1.1012010044521756\nepoch 13: loss 1.1007436513900757\nepoch 14: loss 1.100804540846083\nepoch 15: loss 1.1012108325958252\nepoch 16: loss 1.1007501019371881\nepoch 17: loss 1.1003097825580173\nepoch 18: loss 1.1000993847846985\nepoch 19: loss 1.100281794865926\nepoch 20: loss 1.0994982322057087\nepoch 21: loss 1.0997626384099324\nepoch 22: loss 1.0991399486859639\nepoch 23: loss 1.0989764796362982\nepoch 24: loss 1.0990579790539212\nepoch 25: loss 1.0991833673583136\nepoch 26: loss 1.0992523564232721\nepoch 27: loss 1.0981981820530362\nepoch 28: loss 1.0983789828088548\nepoch 29: loss 1.0980063610606723\nepoch 30: loss 1.0982675949732463\nepoch 31: loss 1.0979865590731304\nepoch 32: loss 1.0979011721081204\nepoch 33: loss 1.0979888571633234\nepoch 34: loss 1.0976383553610907\nepoch 35: loss 1.0974456005626254\nepoch 36: loss 1.0970932642618816\nepoch 37: loss 1.097204777929518\nepoch 38: loss 1.0970725152227614\nepoch 39: loss 1.096924172507392\nepoch 40: loss 1.0963448153601751\nepoch 41: loss 1.0965119865205553\nepoch 42: loss 1.0962118440204196\nepoch 43: loss 1.096218228340149\nepoch 44: loss 1.095618916882409\nepoch 45: loss 1.0956075919999018\nepoch 46: loss 1.0953219731648762\nepoch 47: loss 1.0954916410975986\nepoch 48: loss 1.0955353710386488\nepoch 49: loss 1.0947879950205486\nepoch 50: loss 1.0952717529402838\nepoch 51: loss 1.0952551364898682\nepoch 52: loss 1.0946998132599726\nepoch 53: loss 1.0948020550939772\nepoch 54: loss 1.0944616595904033\nepoch 55: loss 1.0948318905300565\nepoch 56: loss 1.0945119592878554\nepoch 57: loss 1.0944747196303473\nepoch 58: loss 1.0945683320363362\nepoch 59: loss 1.0939074622260199\nepoch 60: loss 1.0938756465911865\nepoch 61: loss 1.0939034356011286\nepoch 62: loss 1.0937815176116095\nepoch 63: loss 1.0934905740949843\nepoch 64: loss 1.093288282553355\nepoch 65: loss 1.0932929317156475\nepoch 66: loss 1.0928763416078355\nepoch 67: loss 1.092831916279263\nepoch 68: loss 1.0930862228075664\nepoch 69: loss 1.093257983525594\nepoch 70: loss 1.0931887361738417\nepoch 71: loss 1.0924624866909451\nepoch 72: loss 1.092402868800693\nepoch 73: loss 1.0925499664412603\nepoch 74: loss 1.0924988389015198\nepoch 75: loss 1.0923377010557387\nepoch 76: loss 1.0922627051671345\nepoch 77: loss 1.0918546120325725\nepoch 78: loss 1.091749178038703\nepoch 79: loss 1.0919462905989752\nepoch 80: loss 1.0918671356307135\nepoch 81: loss 1.091435240374671\nepoch 82: loss 1.0916556980874803\nepoch 83: loss 1.0912510951360066\nepoch 84: loss 1.0918470819791157\nepoch 85: loss 1.091390483909183\nepoch 86: loss 1.0909644299083285\nepoch 87: loss 1.0911520918210347\nepoch 88: loss 1.090389092763265\nepoch 89: loss 1.0906575785742865\nepoch 90: loss 1.0901929206318326\nepoch 91: loss 1.0904839436213176\nepoch 92: loss 1.0908484326468573\nepoch 93: loss 1.0906624926461115\nepoch 94: loss 1.089851929081811\nepoch 95: loss 1.0900147292349074\nepoch 96: loss 1.0914795531166925\nepoch 97: loss 1.0898933410644531\nepoch 98: loss 1.0898751748932733\nepoch 99: loss 1.0895402961307101\nepoch 100: loss 1.0901741054322984\nepoch 101: loss 1.0899525218539767\nepoch 102: loss 1.0900302131970723\nepoch 103: loss 1.0895659857326083\nepoch 104: loss 1.0897157457139757\nepoch 105: loss 1.0899209049012926\nepoch 106: loss 1.0900744597117107\nepoch 107: loss 1.0899023811022441\nepoch 108: loss 1.0895145138104756\nepoch 109: loss 1.08957752916548\nepoch 110: loss 1.090044829580519\nepoch 111: loss 1.0897498660617404\nepoch 112: loss 1.0891498393482633\nepoch 113: loss 1.0892020397716098\nepoch 114: loss 1.089130613538954\nepoch 115: loss 1.0888184242778354\nepoch 116: loss 1.0894453591770596\nepoch 117: loss 1.0884952942530315\nepoch 118: loss 1.088833424780104\nepoch 119: loss 1.0900741550657485\nepoch 120: loss 1.0881368385420904\nepoch 121: loss 1.087913499938117\nepoch 122: loss 1.088478724161784\nepoch 123: loss 1.0887428257200453\nepoch 124: loss 1.0883739127053156\nepoch 125: loss 1.0883182750807867\nepoch 126: loss 1.0882495178116693\nepoch 127: loss 1.0886716975106134\nepoch 128: loss 1.0881531304783292\nepoch 129: loss 1.0874442789289687\nepoch 130: loss 1.0881986088222928\nepoch 131: loss 1.0883075329992506\nepoch 132: loss 1.0875961316956415\nepoch 133: loss 1.0870629019207425\nepoch 134: loss 1.087472465303209\nepoch 135: loss 1.0881112880176969\nepoch 136: loss 1.0880680680274963\nepoch 137: loss 1.087182018491957\nepoch 138: loss 1.0869742300775316\nepoch 139: loss 1.0869055456585355\nepoch 140: loss 1.087344812022315\nepoch 141: loss 1.0864411460028753\nepoch 142: loss 1.0869298775990803\nepoch 143: loss 1.0886027812957764\nepoch 144: loss 1.0885486801465352\nepoch 145: loss 1.0869460238350763\nepoch 146: loss 1.0882866316371493\nepoch 147: loss 1.0871798793474834\nepoch 148: loss 1.086258504125807\nepoch 149: loss 1.0869462821218703\nepoch 150: loss 1.0876785318056743\nepoch 151: loss 1.0879658261934917\nepoch 152: loss 1.0863981511857774\nepoch 153: loss 1.0875712368223402\nepoch 154: loss 1.0867549644576178\nepoch 155: loss 1.0868221521377563\nepoch 156: loss 1.0873284935951233\nepoch 157: loss 1.0874247153600056\nepoch 158: loss 1.0868449343575373\nepoch 159: loss 1.0865531365076702\nepoch 160: loss 1.0865427652994792\nepoch 161: loss 1.0880825916926067\nepoch 162: loss 1.0865484608544245\nepoch 163: loss 1.086406581931644\nepoch 164: loss 1.0873221423890855\nepoch 165: loss 1.0868662463294134\nepoch 166: loss 1.0856000317467585\nepoch 167: loss 1.086272074116601\nepoch 168: loss 1.0859976874457464\nepoch 169: loss 1.0862104230456882\nepoch 170: loss 1.0859081943829854\nepoch 171: loss 1.0858315957917108\nepoch 172: loss 1.0861900183889601\nepoch 173: loss 1.085291862487793\nepoch 174: loss 1.0855152673191495\nepoch 175: loss 1.0860816571447585\nepoch 176: loss 1.0857138766182794\nepoch 177: loss 1.0860220458772447\nepoch 178: loss 1.0865194731288486\nepoch 179: loss 1.08499194516076\nepoch 180: loss 1.085925665166643\nepoch 181: loss 1.0858814981248643\nepoch 182: loss 1.0861274003982544\nepoch 183: loss 1.0854872266451518\nepoch 184: loss 1.0860595968034532\nepoch 185: loss 1.086121678352356\nepoch 186: loss 1.0853958394792345\nepoch 187: loss 1.0844525794188182\nepoch 188: loss 1.0853349566459656\nepoch 189: loss 1.0859438445832994\nepoch 190: loss 1.0865548981560602\nepoch 191: loss 1.085860616630978\nepoch 192: loss 1.0852837430106268\nepoch 193: loss 1.0848489006360371\nepoch 194: loss 1.0845766796006098\nepoch 195: loss 1.087120532989502\nepoch 196: loss 1.085456755426195\nepoch 197: loss 1.0854012734360166\nepoch 198: loss 1.0847520894474454\nepoch 199: loss 1.0857134097152286\n\n\n\nWhen still playing around w/ nn structure+determining hyperparameters, you can plot loss over epochs\n\n\nplt.figure(figsize=(10,6))\nsns.lineplot(epoch_loss)\nplt.xlabel(\"epoch (1-200)\")\nplt.ylabel(\"cross-entropy loss\")\nplt.title(\"cross-entropy loss over 200 epochs\")\nplt.show()\n\n\n\n\n\n#method 2\nn_epochs = 200\nbatch_loss = 0\nepoch_loss = []\n\nfor epoch in range(n_epochs):\n    batch_loss = 0\n    #each iteration = a single batch\n    for batch, (inputs, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        #forward pass\n        output = flex_net(inputs)\n        #get loss\n        loss = criterion(output, labels)\n        #grad\n        loss.backward()\n        optimizer.step()\n        #print(f\"iteration {i}: loss {loss}\")\n        batch_loss += loss.item()\n    #take average\n    epoch_loss.append(batch_loss/(batch+1))\n    print(f\"epoch {epoch}: loss {batch_loss/(batch+1)}\")\n\nepoch 0: loss 1.0758992003069983\nepoch 1: loss 1.0755802591641743\nepoch 2: loss 1.0954784154891968\nepoch 3: loss 1.086965435081058\nepoch 4: loss 1.0983157025443182\nepoch 5: loss 1.0788868798149958\nepoch 6: loss 1.0852028992440965\nepoch 7: loss 1.0833798779381647\nepoch 8: loss 1.07219797372818\nepoch 9: loss 1.0846776001983218\nepoch 10: loss 1.084008405605952\nepoch 11: loss 1.069062242905299\nepoch 12: loss 1.066004004743364\nepoch 13: loss 1.0729810330602858\nepoch 14: loss 1.062520855002933\nepoch 15: loss 1.0717429882950253\nepoch 16: loss 1.0608669420083363\nepoch 17: loss 1.0530325108104281\nepoch 18: loss 1.0582917498217688\nepoch 19: loss 1.048289544052548\nepoch 20: loss 1.0456899073388841\nepoch 21: loss 1.046246074967914\nepoch 22: loss 1.0382493966155582\nepoch 23: loss 1.0410330825381808\nepoch 24: loss 1.0383899874157376\nepoch 25: loss 1.0405794845687018\nepoch 26: loss 1.039245198170344\nepoch 27: loss 1.0447058611445956\nepoch 28: loss 1.040022028817071\nepoch 29: loss 1.0385067893399134\nepoch 30: loss 1.0346433652771845\nepoch 31: loss 1.040452755159802\nepoch 32: loss 1.0338236623340182\nepoch 33: loss 1.0348394480016496\nepoch 34: loss 1.0315190023846097\nepoch 35: loss 1.0348602036635082\nepoch 36: loss 1.0318101876311832\nepoch 37: loss 1.032920668522517\nepoch 38: loss 1.0338921116458044\nepoch 39: loss 1.030531449450387\nepoch 40: loss 1.0345942311816745\nepoch 41: loss 1.0321497718493144\nepoch 42: loss 1.0284072558085124\nepoch 43: loss 1.0314678053061168\nepoch 44: loss 1.0307140681478713\nepoch 45: loss 1.0294452408949535\nepoch 46: loss 1.0238284468650818\nepoch 47: loss 1.0285656419065263\nepoch 48: loss 1.0301485160986583\nepoch 49: loss 1.0278050535255008\nepoch 50: loss 1.0291366179784138\nepoch 51: loss 1.0294345551066928\nepoch 52: loss 1.0284696817398071\nepoch 53: loss 1.027887417210473\nepoch 54: loss 1.0283823377556272\nepoch 55: loss 1.0240179465876684\nepoch 56: loss 1.024108562204573\nepoch 57: loss 1.0211945639716253\nepoch 58: loss 1.018980062670178\nepoch 59: loss 1.0265384548240237\nepoch 60: loss 1.0181521077950795\nepoch 61: loss 1.0233079857296414\nepoch 62: loss 1.015581038263109\nepoch 63: loss 1.0194327135880787\nepoch 64: loss 1.0193254053592682\nepoch 65: loss 1.0265762706597645\nepoch 66: loss 1.0255666143364377\nepoch 67: loss 1.016391204463111\nepoch 68: loss 1.0159449544217851\nepoch 69: loss 1.0204911364449396\nepoch 70: loss 1.010678105884128\nepoch 71: loss 1.0153994858264923\nepoch 72: loss 1.021962755256229\nepoch 73: loss 1.0189153287145827\nepoch 74: loss 1.0143067240715027\nepoch 75: loss 1.0070891446537442\nepoch 76: loss 1.0126522713237338\nepoch 77: loss 1.0084776447878943\nepoch 78: loss 1.009682810968823\nepoch 79: loss 1.0167745351791382\nepoch 80: loss 0.9994094901614718\nepoch 81: loss 1.002054105202357\nepoch 82: loss 1.0176017118824854\nepoch 83: loss 1.0072523554166157\nepoch 84: loss 0.9928684631983439\nepoch 85: loss 1.0101111729939778\nepoch 86: loss 1.0118306411637201\nepoch 87: loss 0.9833696683247884\nepoch 88: loss 0.9966344270441267\nepoch 89: loss 1.0149242679278057\nepoch 90: loss 1.0054545369413164\nepoch 91: loss 1.014212002356847\nepoch 92: loss 1.0113542907767825\nepoch 93: loss 1.0187519258923001\nepoch 94: loss 1.0080854329797957\nepoch 95: loss 1.000441633992725\nepoch 96: loss 0.992532577779558\nepoch 97: loss 1.0097742180029552\nepoch 98: loss 0.9927737679746416\nepoch 99: loss 1.004389289352629\nepoch 100: loss 0.9952792028586069\nepoch 101: loss 0.9944166640440623\nepoch 102: loss 1.0122214986218347\nepoch 103: loss 0.992783334520128\nepoch 104: loss 1.0132885873317719\nepoch 105: loss 0.9835924241277907\nepoch 106: loss 0.999792648686303\nepoch 107: loss 0.9840762846999698\nepoch 108: loss 0.9854936665958829\nepoch 109: loss 0.997794959280226\nepoch 110: loss 0.9719465904765658\nepoch 111: loss 0.9920585718419817\nepoch 112: loss 0.9930699931250678\nepoch 113: loss 0.9962947931554582\nepoch 114: loss 0.9979508750968509\nepoch 115: loss 0.9912625186973147\nepoch 116: loss 0.9890427986780802\nepoch 117: loss 0.9856837424967024\nepoch 118: loss 1.0160565740532346\nepoch 119: loss 0.9847754902309842\nepoch 120: loss 0.9861320389641656\nepoch 121: loss 0.9931310514609019\nepoch 122: loss 0.9988126125600603\nepoch 123: loss 0.9763712452517616\nepoch 124: loss 0.9902465475930108\nepoch 125: loss 1.0241407917605505\nepoch 126: loss 0.9962930844889747\nepoch 127: loss 1.0219404333167605\nepoch 128: loss 1.0118498305479686\nepoch 129: loss 0.9973408314916823\nepoch 130: loss 0.9869284927845001\nepoch 131: loss 0.9830329484409757\nepoch 132: loss 0.9844811194472842\nepoch 133: loss 0.9748289982477824\nepoch 134: loss 1.0123606787787542\nepoch 135: loss 0.996467391649882\nepoch 136: loss 0.9946328202883402\nepoch 137: loss 0.9884665409723917\nepoch 138: loss 0.991803463962343\nepoch 139: loss 1.0036283565892115\nepoch 140: loss 0.9830687410301633\nepoch 141: loss 0.973314086596171\nepoch 142: loss 0.9921116232872009\nepoch 143: loss 0.9962259232997894\nepoch 144: loss 0.97552090883255\nepoch 145: loss 0.9807691077391306\nepoch 146: loss 1.0016797681649525\nepoch 147: loss 0.9701012406084273\nepoch 148: loss 0.9847355352507697\nepoch 149: loss 1.0079118112723033\nepoch 150: loss 0.984410262770123\nepoch 151: loss 0.9768550362851884\nepoch 152: loss 0.9925394687387679\nepoch 153: loss 0.9934237632486556\nepoch 154: loss 0.9948908256159888\nepoch 155: loss 0.9798293974664476\nepoch 156: loss 0.9869266880883111\nepoch 157: loss 0.9657246602906121\nepoch 158: loss 0.9862294230196211\nepoch 159: loss 0.9670793612798055\nepoch 160: loss 0.9546366433302561\nepoch 161: loss 1.090085357427597\nepoch 162: loss 1.0338688128524356\nepoch 163: loss 0.9852095544338226\nepoch 164: loss 0.9724307523833381\nepoch 165: loss 1.0271649228201971\nepoch 166: loss 0.9859315388732486\nepoch 167: loss 0.9870808455679152\nepoch 168: loss 0.9829074177477095\nepoch 169: loss 0.9796078701814016\nepoch 170: loss 0.9765292406082153\nepoch 171: loss 1.0084395839108362\nepoch 172: loss 0.9688159492280748\nepoch 173: loss 0.9622737103038363\nepoch 174: loss 0.993135627773073\nepoch 175: loss 0.9596685965855917\nepoch 176: loss 0.9657837715413835\nepoch 177: loss 0.9892585906717513\nepoch 178: loss 0.9738571345806122\nepoch 179: loss 0.9579844971497854\nepoch 180: loss 0.9799589150481753\nepoch 181: loss 1.0239534444279141\nepoch 182: loss 0.9625405404302809\nepoch 183: loss 0.9889433185259501\nepoch 184: loss 0.9971031745274862\nepoch 185: loss 0.984661877155304\nepoch 186: loss 0.9411121441258324\nepoch 187: loss 0.9926096366511451\nepoch 188: loss 0.984059625201755\nepoch 189: loss 0.9524499939547645\nepoch 190: loss 0.9831621746222178\nepoch 191: loss 0.9570663703812493\nepoch 192: loss 0.9726736048857371\nepoch 193: loss 1.029220524761412\nepoch 194: loss 1.0296267171700795\nepoch 195: loss 1.0303357011742063\nepoch 196: loss 1.015145457453198\nepoch 197: loss 1.0043697953224182\nepoch 198: loss 0.9973857833279504\nepoch 199: loss 0.9982444743315378\n\n\n\nplt.figure(figsize=(10,6))\nsns.lineplot(epoch_loss)\nplt.xlabel(\"epoch (1-200)\")\nplt.ylabel(\"cross-entropy loss\")\nplt.title(\"cross-entropy loss over 200 epochs\")\nplt.show()"
  },
  {
    "objectID": "posts/pytorch_autograd/index.html",
    "href": "posts/pytorch_autograd/index.html",
    "title": "PyTorch tutorial: autograd demo",
    "section": "",
    "text": "original tutorial link\ntorch.autograd: automatic differentiation engine that powers NN training\nBackground\n\nNeural networks (NNs): collection of nested functions executed on some input data\nfns defined by parameters (weights+biases), which are stored in tensors\n2 steps in training NN:\n\nforward prop: NN makes best guess about correct output; runs input data through each of its functions to make guess\nbackward prop: NN adjusts parameters proportion to error in its guess by traversing backwards from output, collecting derivatives of error w/ respect to parameters of fns and optimizing parameters using gradient descent\n\n\nUsage in PyTorch\n\nExample: a single training step\n\n\nCreate random data tensor\n\nrandom data tensor represents single image w/ 3 channels, and height+width of 64\n\nlabel initialized to some random values\nlabel in pretrained models has shape (1,1000)\n\n\n\nimport torch\nfrom torchvision.models import resnet18, ResNet18_Weights\nmodel = resnet18(weights=ResNet18_Weights.DEFAULT)\ndata = torch.rand(1, 3, 64, 64)\nlabels = torch.rand(1, 1000)\n\nForward pass: run input data through model through each of its layers to make a prediction\n\nprediction = model(data)\n\nUse prediction+corresponding label to calculate error (loss). Backprop by calling .backward() on the error tensor.\n\nautograd calculates+Stores gradients for each model parameter in parameter’s .grad attribute\n\n\nloss = (prediction - labels).sum()\nloss.backward() # backward pass\n\nLoad optimizer; e.g. SGD w/ a learning rate of 0.01 and momentum of 0.9. Register all parameters of model in optimizer.\n\noptim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n\nInitiate gradient descent with .step().\n\noptimizer adjusts each parameter its gradient stored in .grad.\n\n\noptim.step()\nDifferentiation in Autograd\n\nhow does autograd collect gradients?\n\n\nCreate 2 tensors a and b with requires_grad=True to signal that every operation on them should be tracked.\n\nimport torch\n\na = torch.tensor([2., 3.], requires_grad=True)\nb = torch.tensor([6., 4.], requires_grad=True)\n\nCreate another tensor Q from a and b.\n\\[\nQ=3a^3-b^2\n\\]\n\nQ = 3*a**3 - b**2\n\nAssume a and b to be parameters of a NN, Q to be error. When training NN, want gradients of error wrt parameters:\n\\[\\begin{align*}\n\\frac{\\partial{Q}}{\\partial{a}} &= 9a^2\\\\\n\\frac{\\partial{Q}}{\\partial{b}} &= -2b\n\n\\end{align*}\\]\n\nautograd calculate gradients when we call .backward() on Q, storing in respective tensors’ .grad attribute.\nneed to explicitly pass a gradient argument in Q.backward() because it is a vector\ngradient is a tensor of same shape as Q and represents gradient of Q wrt itself; i.e.\n\\[\n  \\frac{dQ}{dQ} = 1\n  \\]\nequivalently can also aggregate Q into scalar and call backward implicitly like Q.sum().backward().\n\n\nexternal_grad = torch.tensor([1., 1.])\nQ.backward(gradient=external_grad)\n\nGradients now deposited in a.grad and b.grad:\n\n# check if collected gradients are correct\nprint(9*a**2 == a.grad)\nprint(-2*b == b.grad)\nVector calculus using autograd\nGiven vector valued function \\(\\vec{y} = f(\\vec{x})\\), \\(\\nabla \\vec{y}\\) wrt \\(\\vec{x}\\) is Jacobian matrix \\(J\\)\n\\[\n\\begin{align*}\nJ &= \\begin{pmatrix}\n\\frac{\\partial \\mathbf{y}}{\\partial x_1} & \\dots & \\frac{\\partial \\mathbf{y}}{x_n}\n\\end{pmatrix}\\\\\n&= \\begin{pmatrix}\n\\frac{\\partial y_1}{\\partial x_1} & \\dots & \\frac{\\partial y_1}{x_n}\\\\\n\\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial {y_m}}{\\partial x_1} & \\dots & \\frac{\\partial y_m}{x_n}\n\\end{pmatrix}\n\\end{align*}\n\\]\n\ntorch.autograd computes vector-Jacobian product → given any vector \\(\\vec{v}\\), compute the product \\(J^{T}\\cdot \\vec{v}\\)\n\nIf \\(\\vec{v}\\) is gradient of scalar function \\(l=g(\\vec{y})\\):\n\\[\n\\vec{v} = \\begin{pmatrix}\n\\frac{\\partial l}{\\partial y_1} & \\dots & \\frac{\\partial l}{\\partial y_m}\n\\end{pmatrix}^T\n\\]\nthen by the chain rule, vector-Jacobian product would be gradient of \\(l\\) wrt \\(\\vec{x}\\):\n\\[\nJ^T\\cdot \\vec{v} =\n\\begin{pmatrix}\n\\frac{\\partial y_1}{\\partial x_1} & \\dots & \\frac{\\partial y_m}{x_n}\\\\\n\\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial {y_m}}{\\partial x_1} & \\dots & \\frac{\\partial y_m}{x_n}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{\\partial l}{\\partial y_1}\\\\\n\\vdots \\\\\n\\frac{\\partial l}{\\partial y_m}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{\\partial l}{\\partial x_1}\\\\\n\\vdots\\\\\n\\frac{\\partial l}{\\partial x_n}\n\\end{pmatrix}\n\\]\ncharacteristic of vector-Jacobian product is what we use in the above example; external_grad = \\(\\vec{v}\\)\nComputational Graph\nautograd keeps record of data (tensors) and all executed operations (along w/ resulting new tensors) in a DAG consisting of Function objects\n\nleaves = input tensors; roots are output tensors\ntrace graph from roots to leaves → automatically compute gradients using chain rule\n\nForward pass: autograd does 2 things simulatneously:\n\nrun requested oepration to compute resulting tensor\nmaintain operation’s gradient function in DAG\n\nBackward pass when .backward() called on DAG root. autograd then:\n\ncomputes gradients from each .grad_fn\naccumulates them in respective tensor’s .grad attribute\nusing chain rule, propagates all the way to leaf tensors\n\n\n\n\nhttps://pytorch.org/tutorials/_images/dag_autograd.png\n\n\n\nleaf nodes in blue = a and b\narrows: in direction of forward pass\nnodes: backward functions of each operation in forward pass\n\nExclusion from the DAG\n\ntorch.autograd tracks operations on all tensors which have their requires_grad flag set to True\ntensors that don’t require gradients → set attribute to False → exclude from gradient computation DAG\noutput tensor of operation will require gradients even if only a single input tensor has requires_grad=True\n\nx = torch.rand(5, 5)\ny = torch.rand(5, 5)\nz = torch.rand((5, 5), requires_grad=True)\n\na = x + y\nprint(f\"Does `a` require gradients? : {a.requires_grad}\")\n#false\n\nb = x + z\nprint(f\"Does `b` require gradients?: {b.requires_grad}\")\n#true\nfrozen parameters: parameters that don’t compute gradients\n\nuseful to freeze part of model if you know beforehand you don’t need gradients of those parameters (reduces autograd computations)\nin finetuning, freeze most of model+only modify classifier layers to make predictions on new labels\n\nfrom torch import nn, optim\n\nmodel = resnet18(weights=ResNet18_Weights.DEFAULT)\n\n# Freeze all the parameters in the network\nfor param in model.parameters():\n    param.requires_grad = False\nEx. finetune model on new dataset w/ 10 label\n\nin resnet, classifier is last linear layer model.fc\ncan replace it with new linear layer that acts as our classifier (unfrozen by default)\n\nmodel.fc = nn.Linear(512, 10)\nNow all parameters besides model.fc parameters are frozen; only weights and bias of model.fc compute gradients\n# Optimize only the classifier\noptimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n\nhere, we register all parameters in optimizer but only weights and bias of classifier end up computing gradients (and are updated in gradient descent)\nsame exclusionary functionality available as context manager in torch.no_grad()"
  },
  {
    "objectID": "posts/pp_train_test_data/index.html",
    "href": "posts/pp_train_test_data/index.html",
    "title": "Preprocessing, training, testing data",
    "section": "",
    "text": "fig 1\n\n\n\n\n\nfig 2\n\n\n\n\n\nfig 3"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tiffanie lab blog",
    "section": "",
    "text": "Enformer usage hackathon demo\n\n\n\n\n\n\n\ndemos\n\n\nw5\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nz2h 2. Intro to language modeling: building makemore\n\n\n\n\n\n\n\ndemos\n\n\nw4\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nz2h micrograd questions\n\n\n\n\n\n\n\npractice\n\n\nw4\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nz2h 1. Intro to NN+backprop: building micrograd\n\n\n\n\n\n\n\ndemos\n\n\nw4\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nPreprocessing, training, testing data\n\n\n\n\n\n\n\npresentations\n\n\nw3\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nEnformer paper summary\n\n\n\n\n\n\n\nreadings\n\n\nw3\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nW3 thesis updates\n\n\n\n\n\n\n\npresentations\n\n\nw3\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nWine dataset MLP example\n\n\n\n\n\n\n\npractice\n\n\nw3\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch tutorial: NN demo\n\n\n\n\n\n\n\ndemos\n\n\nw2\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nNeural networks (3b1b)\n\n\n\n\n\n\n\ndemos\n\n\nw2\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch tutorial: training classifier demo\n\n\n\n\n\n\n\ndemos\n\n\nw2\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch tutorial: autograd demo\n\n\n\n\n\n\n\ndemos\n\n\nw2\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nTranscriptome QGT lab\n\n\n\n\n\n\n\ndemos\n\n\nw2\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n  \n\n\n\n\nFunctional annotation of GWAS loci using transcriptome data\n\n\n\n\n\n\n\npresentations\n\n\nw2\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nPopulation structure\n\n\n\n\n\n\n\ndemos\n\n\nw2\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nConducting GWAS studies summary\n\n\n\n\n\n\n\nreadings\n\n\nw2\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nMatrix algebra basics (numpy) - part 1\n\n\n\n\n\n\n\npractice\n\n\nw1\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch tutorial: tensor demo\n\n\n\n\n\n\n\ndemos\n\n\nw1\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nLLM in molecular biology summary\n\n\n\n\n\n\n\nreadings\n\n\nw1\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nW1 Thesis updates\n\n\n\n\n\n\n\npresentations\n\n\nw1\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\ntiffanie\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "w5.html",
    "href": "w5.html",
    "title": "w5",
    "section": "",
    "text": "Enformer usage hackathon demo\n\n\n\n\n\n\ntiffanie\n\n\nJul 5, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "w4.html",
    "href": "w4.html",
    "title": "w4",
    "section": "",
    "text": "z2h 2. Intro to language modeling: building makemore\n\n\n\n\n\n\ntiffanie\n\n\nJun 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nz2h micrograd questions\n\n\n\n\n\n\ntiffanie\n\n\nJun 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nz2h 1. Intro to NN+backprop: building micrograd\n\n\n\n\n\n\ntiffanie\n\n\nJun 26, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "w1.html",
    "href": "w1.html",
    "title": "w1",
    "section": "",
    "text": "Matrix algebra basics (numpy) - part 1\n\n\n\n\n\n\ntiffanie\n\n\nJun 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPyTorch tutorial: tensor demo\n\n\n\n\n\n\ntiffanie\n\n\nJun 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nLLM in molecular biology summary\n\n\n\n\n\n\ntiffanie\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nW1 Thesis updates\n\n\n\n\n\n\ntiffanie\n\n\nJun 7, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "w3.html",
    "href": "w3.html",
    "title": "w3",
    "section": "",
    "text": "Preprocessing, training, testing data\n\n\n\n\n\n\ntiffanie\n\n\nJun 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEnformer paper summary\n\n\n\n\n\n\ntiffanie\n\n\nJun 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nW3 thesis updates\n\n\n\n\n\n\ntiffanie\n\n\nJun 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWine dataset MLP example\n\n\n\n\n\n\ntiffanie\n\n\nJun 21, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "w2.html",
    "href": "w2.html",
    "title": "w2",
    "section": "",
    "text": "PyTorch tutorial: NN demo\n\n\n\n\n\n\ntiffanie\n\n\nJun 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNeural networks (3b1b)\n\n\n\n\n\n\ntiffanie\n\n\nJun 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPyTorch tutorial: training classifier demo\n\n\n\n\n\n\ntiffanie\n\n\nJun 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPyTorch tutorial: autograd demo\n\n\n\n\n\n\ntiffanie\n\n\nJun 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTranscriptome QGT lab\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nFunctional annotation of GWAS loci using transcriptome data\n\n\n\n\n\n\ntiffanie\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation structure\n\n\n\n\n\n\ntiffanie\n\n\nJun 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nConducting GWAS studies summary\n\n\n\n\n\n\ntiffanie\n\n\nJun 12, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]