[
  {
    "objectID": "posts/pytorch_tensor_post/index.html",
    "href": "posts/pytorch_tensor_post/index.html",
    "title": "PyTorch tutorial: tensor demo",
    "section": "",
    "text": "original tutorial link\ntensor: n-dimensional array; specialized data structure similar to arrays+matrices\n\nin PyTorch, used to encode inputs/outputs of a model+model parameters\nsimilar to ndarrays but run on GPUs or other accelerated hardware for computing\n\nTensor initialization — many ways:\n\ndirectly from data\ndata = [[1,2,],[3,4]]\nx_data = torch.tensor(data)\n#data type inferred\nfrom NumPy array\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array)\nfrom another tensor\n#argument tensor:\ndata = [[1,2,],[3,4]]\nx_data = torch.tensor(data)\n\n#new tensor:\nx_ones = torch.ones_like(x_data) # retains the properties of x_data\n\n#new tensor overriding properties:\nx_rand = torch.rand_like(x_data, dtype=torch.float)\n\nnew tensor retains properties (shape+datatype) of argument tensor unless explicitly overridden\n\nwith random or constant values:\nshape = (2, 3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nhere shape is a tuple of tensor dimensions; determines dimensionality of output tensor\n\n\nTensor attributes: describe shape, datatype, and device on which they are stored\ntensor_name.shape\ntensor_name.dtype\ntensor_name.device\nTensor operations: over 100 including T, index, slicing, mathematical operations, linalg, random sampling, etc.\n\nTensor API very similar to NumPy API\nNumpy-like index+slice:\ntensor = torch.ones(4, 4)\ntensor[:,1] = 0\nJoining tensors: concatenate sequence of tensors along a given dimension using torch.cat\nt1 = torch.cat([tensor, tensor, tensor], dim=1)\nmultiplying tensors element-wise: use * operator or .mul() method\ntensor.mul(tensor)\n#or\ntensor*tensor\nmatrix multiplication: use @ operator with transposed matrix or .matmul(&lt;transpose&gt;) method\ntensor.matmul(tensor.T)\n#or\ntensor @ tensor.T\nin-place operations: any operations w/ a _ suffix\n#ex:\nx.copy_(y)\nx.t_()\n#change x in place\n\ntensor.add_(5) \n#change tensor in place\n\nhowever can cause issues when computing derivatives b/c of immediate loss of history\n\n\nBridge with NumPy: tensors on CPU+NumPy arrays can share underlying memory locations; changing one will change other\n\ntensor to np array\nt = torch.ones(5)\nn = t.numpy()\n\nchange in tensor reflects in np array\nt.add_(1)\n#n will change if t changes\n\nnp array to tensor\nn = np.ones(5)\nt = torch.from_numpy(n)\n\nchanges in np array reflects in tensor\nnp.add(n, 1, out=n)\n#t will change if n changes"
  },
  {
    "objectID": "posts/llm_summary_post/index.html",
    "href": "posts/llm_summary_post/index.html",
    "title": "LLM in molecular biology summary",
    "section": "",
    "text": "LLM in molecular biology summary\narticle link\nLarge language models\n\nLLM: a type of neural network that acquires the ability to generate text mirroring human language by scrutinizing vast amounts of textual data\n\nself-supervised — model learns to predict subsequent word in a sentence based on preceding words\ncan identify patterns and try to predict (i.e. advanced form of autocomplete)\n\n\nPrimary types of language models and their unique features:\n\nword grams: predict next word in a sentence based on frequency of word pairs/word bags (sets of words) — disregard context or word order (generates text that bear little resemblance to human text)\nCNNs: analyze text data by considering relationships between adjacent words in a fixed window; good at identifying local patterns, but fall short in capturing long-range dependencies or comprehending complex sentence structures\nLSTMs (long short-term memory networks): variant of RNNs; store+process information from earlier parts of a text; outperform CNNs in understanding context/managing long-range dependencies, but falter w/ complex sentences+long text\nattention mechanisms: enable models to concentrate on pertinent parts of input when making predictions; number of attention “heads” allow model to focus on different parts of previous text when predicting the next word\n\nlike revisiting key points/details; model refers back to relevant parts of text+incorporates info into current context\nex. transformers are a class of language models that implement attention mechanisms\n\nLLMs: models such as GPT-3 are transformers that leverage attention mechanisms+are trained on vast amounts of data; considerable size facilitates the learning of intricate patterns/relationships/context within text; represent the most advanced language models presently available, capable of generating more accurate+coherent responses across a broad spectrum of topics\n\n2 LLMs that use transformer architecture: BERT+GPT series\nBERT (bidirectional encoder representations from transformers): a series of LLMs by Google+open sourced\n\ntrained using masked language modeling (hide/“mask” some percentage of input tokens at random, then predict those masked tokens)\n\nforces model to understand context from both left/right sides of input (bidirectional)\n\nalso uses next sentence prediction task\nduring training, model is given pairs of sentences+has to predict whether second sentence in pair is the next sentence in the original document\n\nGPT (generative pretrained transformer): series of LLMs introduced by OpenAI\n\ntrained using traditional language modeling task of autocomplete (predict next word in sentence)\nonly attends to left context (previous tokens) during training (unidirectional)\ngenerative model that is stronger in tasks involving text generation\n\nThe genetic dogma\n\nbiological trajectory of a human or any other organism is a complex interplay between genetics+environment (DNA and environment individual is exposed to) aka genotype-phenotype-environment\ncentral dogma of molec bio describes flow of genetic info within living organisms\n\nsource of genetic info: our DNA (exact replica of which is harbored in nucleus of every cell in our body)\n\neach individual possesses 2 nearly identical copies of human genome (one from mom, one from dad)\n\n\nhuman chromosome structure: chromatin tightly packed in hierarchical coil structures. from bottom, 146 nucleotide pairs wrapped around histone (like a bead), and histones are coiled and supercoiled to form compact chromosome that fits within the nucleus of a cell\nwithin genome: ~20,000 genes (DNA segments accountable for protein synthesis)\n~1% of genome codes for proteins while remainder comprises regions controlling gene expression, regions within genes that don’t code for proteins, regions contributing to DNA structure, and “junk” regions of selfish DNA that have “learned” to self-replicate\ncentral dogma of molec bio maps out molec info flow from genome → expression of genes+subsequent production of proteins (building blocks of life)\ngenes expressed within cells by transcription (copies genes into single-stranded molecule mRNA) and translation (mRNA → amino acid protein sequence); 4-letter nucleotide code of DNA segment translated into 20-amino acid code of protein sequence; protein folds in 3d to form functional protein structure\ntranscription → splicing → translation\nsplicing: excised segments aka introns; kept regions aka exons make up protein-coding part of mRNA\n\neach mature mRNA assembled from ~7 exons\nvital in higher organisms b/c a single gene can yield multiple different proteins by assembling different exon combinations during splicing\n\n20,000 genes → 70,000 known standard splice forms+larger # rare/aberrant splice forms\nafter transcription, mRNA transported to cell’s protein-synthesizing machinery (ribosome) where translation occurs\n\nmRNA sequence decoded by codons (each corresponds to 1/20 amino acids)\n\namino acids linked together in a chain to form protein sequence → folds into functional, 3d protein structure\ngene regulation: intricate processes that dictate when/where/in what quantity genes are expressed within cell, ensuring timely production of the right proteins in the right amounts\n\ngene regulation takes place at various levels (structuring of chromatin, chem modifications, through action of specific proteins known as transcription factors) that recruit RNA polymerase and/control when/where/what amount gene will be expressed (requires open chromatin for transcription)\n\nTranscription factors (TFs): proteins in gene regulation that bind to distinct DNA sequences near/within genes (transcription factor binding sites) and influence recruitment of RNA polymerase, the enzyme tasked w/ mRNA synthesis\n\ntranscription factors modulate expression of target genes, guaranteeing appropriate gene expression in response to diverse cellular signals/environmental conditions\nTFs themselves modulated by TFs, forming complex gene regulatory pathways\n\nPromoters+enhancers: DNA regions that play a role in gene expression control\n\npromoters: located adjacent to start of a gene (upstream/to left of gene start, in chemical direction of DNA)\nenhancers: more distant regulatory elements situated within introns or between genes\nboth harbor several TF binding sites\nwith assistance of TFs, a gene’s promoter+enhancers form 3-d structures that recruit and regulate RNA polymerase responsible for mRNA synthesis\n\nChromatin structure: an amalgamation of DNA+proteins (histones) that constitute chromosomes\n\nto fit within each cell’s nucleus, DNA is wound around proteins known as histones\nhistones: tetramers (structures formed by assembling 4 copies of histone protein)\neach histone wraps around 146 nucleotide pairs of DNA, creating a rosary structure that subsequently folds into a higher order helical structure (chromatin)\nchromatin organization determines which DNA regions are accessible for gene expression\nfor gene expression to occur, chromatin must be unfolded\ntightly packed chromatin prevents gene expression\n\nHistone modifications: chemical modifications (e.g. acetylation/methylation) that can affect the histone beads+influence chromatin structure+gene accessibility\n\nmodifications can either promote or inhibit gene expression depending on type+location of modification\npart of the histone code (sort of epigenetic code) i.e. additional layer of code superimposed on the genetic code inscribed in the DNA\n\nDNA methylation: chem modification where methyl group added to DNA molec usually at specific cytosine bases\n\ncan influence gene expression by affecting binding of transcription factors or changing chromatin structure, making it more compact/less accessible for transcription\nalso part of epigenetic code\ngene regulation is a dynamic process specific to each cell type; diff cells exhibit unique gene expression profiles → perform specialized functions\nthrough precise control of gene expression cells can respond to enviro stimuli, sustain homeostasis+execute complex processes essential for life\n\nBidirectional flow of info: some exceptions to unidirectional flow of info (e.g. central dogma: DNA → RNA → protein):\n\nreverse transcription: RNA converted back to DNA; facilitated by reverse transcriptase+common in retroviruses such as HIV\nDNA can also be transcribed into RNA molec besides mRNA — tRNA, rRNA, and other types of non-coding RNA, adding another level of complexity to the flow of genetic information\nrole of epigenetics by DNA methylation and histone modification\n\n\nVariation in our DNA\n\nevery individual biologically shaped by interplay between DNA+enviro influences\nDNA variants account for heritability of all our traits\norigins of DNA variants\n\nprimary mechanism: mutations between genomes of 2 parents+germline genomes that both parents contribute to offspring’s genome\ndrive genetic variation+account for differences from other species\nmost new variants are benign\nsmaller fraction can be deleterious esp if they damage a functional region\neven smaller fraction could be beneficial\n\nSelection: deleterious variants/harmful genetic alterations render an organism less “fit” — tend to be statistically eliminated from population\n\nrare variants generally more likely to be harmful\n\nCoalescence+DNA sequence conservation: effects of selection are highly informative — 2 regions of similarity between genes eventually coalesce — eventually there is an ancestor mammalian individual that had 2 kids that both inherited precise same DNA piece, each leading to each gene today\n\nmutations that took place in important parts of gene tended to make individuals less fit\nmore conserved parts of DNA region more likely to be functionally imporant\n\nData generation: short DNA segments w/ a specific property of interest such as binding a certain TF or being part of the open accessible chromatin are isolated in an experiment+sequenced\n\nother technologies like MS and affinity-based proteomics can measure the levels of all proteins in a biological sample\nX-ray crystallography provides 3-d protein structures\n\nLinking variation to function: want to correlate genetic variants across individuals’ genomes w/ specific phenotypes (e.g. presence vs absence of a particular disease) aka GWAS\n\nidentify statistically significant associations of certain genome locations (which could be genes or regulatory regions) with the phenotypes under study\nwhen measured phenotype isn’t binary but a quantifiable entity, regression can be performed between genomic variation and phenotype, with identified genetic loci termed quantitative trait loci\nbesides macroscopic phenotypes (disease status, height, hair color), genetic variation can be associated w/ molecular phenotypes such as gene expression levels (leading to expression quantitative trait loci — eQTLs), protein abundance (results in protein quantitative trait loci — pQTLs) and virtually every other molecular measurement\nlikely to be surpassed by application of LLMs\n\nLanguage models in molec bio\n\nmodeling molec bio doesn’t need artificial general intelligence (AGI); i.e. doesn’t require high-level planning, agency, or goals; limited need for combinatorics+algorithmic reasoning\n\nrequires what LLMs are good at: learning stat properties of intricate, noisy sequential data to best predict such data from lossy representations\n\n\n\nPredicting gene structure\n\nprimary function of DNA: encode genes that are transcribed+translated into proteins\nspecific segments of each gene translated into proteins determined by splicing mechanisms (segments are annotated)\nmutations can disrupt precise boundaries of splicing (splice sites)\nrare mutations can significantly impact resulting protein fn+ produce a completely different protein sequence\n\naccount for 10% rare genetic diseases\n\nFundamental computational task: predicting splice sites+deducing gene structure; implications for diagnosing genetic diseases\n\naccuracy is not high enough\n\nSpliceAI: employs earlier techniques for language model (not transformer tech or LLM), where language is DNA sequences\n\ndeep residual CNN\ndilated convolutions to efficiently expand the window size it can handle\naccepts 10k nucleotide windows of human genome as input → predict exact locations of intron-exon boundaries (donor/acceptor sites) — exon-intron and intron-exon borders\nprecise-recall AUC: 0.98\naccurate enough to perform mutational analysis in silico — artificially laters any position of DNA and determines whether this change introduces or eliminates a splice site within 10k nucleotides of alteration\ncan be utilized to aid genetic diagnosis\nachieved high accuracy by learning biomolec properties of DNA sequence that guide splicing machinery to splice sites (previously less known)\nnew question: how to extract biomolec rules that SpliceAI learned+gain insight into underlying biomolec mechanisms?\n\nPredicting protein structure\n\nprotein sequences directly translated from spliced mRNA sequences according to genetic code, then fold into 3d structures\nwant to predict protein structure from protein sequence — difficult\nnew open-source database (AlphaFold2) that provides high-accuracy structural predictions for various organisms\n\nAlphaFold2 methodology:\n\ncombines CNN operating on protein sequences w/ pairwise co-evolution feature\nidentifies pairs of sequence positions that co-vary across related protein seuqneces in diff species to predict 2D contact maps across protein seuqnece\n\ncontact map: score for every pair of positions in sequence — likelihood of 2 positions being in close proximity in 3D\n\nbuilds on these algorithsms and introduces some new improvements:\n\nbased on transformer LLM architecture — can better capture long-range interactions between AA in protein seq\nnovel energy-based score (Amber energy) introduced to directly optimize 3d protein structure → allows for end-to-end differentiable approach during structure optimization step\nimproved utilization of coevolutionary features by incorporating multiple sequence alignment (MSA) data boosts model’s ability to identify conserved structural features across homologous protein sequences\nrefine: fine-tune predicted protein structures using second model trained on output of first model → more accurate and consistent predictions\n\nnote: ensemble?\n\n\n\n\nPredicting impact of protein variants\n\n4 million positions in genomes of any 2 individuals vary\n\n20k such variants located within protein-coding regions\nsmall fraction of genetic diversity is deleterious → contributes to genetic diseases\n\nclue in determining if a variant is benign: compare human genetics to genetics of close relatives\n\nproteins conserved by evolution are even more similar on average\n\nsearch for mutations that confer serious genetic disease should start from mutations not on this list\nuse list to observe patterns within protein seq and structures that tend to tolerate variants+patterns that tend not to tolerate variants\n\ncan gain ability to annotate variants in proteins as likely benign+likely pathogenic\n\nPrimateAI-3D: transformer that learns to distinguish between benign+pathogenic variants in human proteins\n\naccomplished by learning patterns of protein positions where primate variants tend to be present vs protein positions where they tend to be absent\nuses both protein seq data as well as protein 3D models that are either experimentally reconstructed or computationally predicted by tools like AlphaFold+HHpred\ncan be applied to diagnosis of rare disease+prioritize variants that are likely deleterious, filter out benign variants\nanother application: discovery of genes associated w/ complex diseases\n\nlook for variants likely deleterious according to PrimateAI-3D, then look for abundance of such variants within specific gene across cohort\ngenes with genetic “burden” (signal of playing role in the disease)\n\nused primate AI-3D+developed improved rare variant polygenic risk score models (PRS) to identify individuals at high diseases risk\n\n\nModeling gene regulation\n\nmolecular components: DNA chromatin structure, chemical alterations within histones that DNA wraps around, attachment of TFs to promoters+enhancers, establishment of 3D DNA structure involving promoters, enhancers, bound transcription factors, and recruitment of RNA polymerase\n\nData generation informative of gene regulation\n\nexamples of info obtained, always related to a human cell line or tissue type:\n\nidentify precise locations across entire genome that have open chromatin vs tightly packed chromatin\n\n2 relevant assays: DNAse-seq and ATAC-seq\n\npinpoint all locations in genome where a specific transcriptions factor is bound\nidentify all location in genome where a specific histone chem modification has occurred\ndetermining level of mRNA available for a given gene i.e. expression level of particular gene\n\nlanguage models → culminate in transformer-based Enformer tool\n\naccept DNA sequence near a gene as input, output cell type-specific expression level of this gene for any gene in the genome\ntrained on task: given a genome region of 100k nucleotides and a specific cell type, predict available types of experimental data for this region, including status of open/packed chromatin, present histone modifications, specific bound TFs, and level of gene expression\n\nlanguage model ideal (rather than masked language modeling) for supervised training — predict all tracks simultaneously from DNA seq\n\nincorporates attention mechanisms+collates info from distant regions to predict status of given location\n\nEnformer performs well in predicting gene exp from sequence alone\n\nhowever doesn’t achieve reduction of collecting necessary experimental data (highly correlated replicates of same experiment) yet\ncan predict changes in gene exp caused by mutations present in diff individuals as well as by mutations artificially introduced through CRISPR experiments\nlimitations: perform poorly in predicting effects of distal enhancers+correctly determine direction of effect of personal variants in gene expression\n\nOrca model: language model based on convolutional encoder-decoder architecture that predicts 3D genome structure from proximity data provided by Hi-C experiments\n\nhierarchical multi-level convolutional encoder, multilevel decoder, predicts DNA structure at 9 levels of resolution for input DNA seq that are as long as the longest human chromosome\n\n\nFoundation models\n\nlarge DL architectures (such as transformer-based GPT models by OpenAI) that encode a vast amount of knowledge\n\ncan be fine-tuned for specific tasks\n\n\nscGPT: foundation model designed for single-cell transcriptomics, chromatin accessibility, and protein abundance\n\ntrained on single-cell data from 10 million human cells\neach cell contains expression values for a fraction of ~20k human genes\nmodel learns embeddings of this large cell x gene matrix → provides insights into underlying cellular states+active bio pathways\nconcept of “next gene” is unclear in single-cell data\n\nsolution: train model to generate data based on a gene prompt (collection of known gene values)+cell prompt\nstarting from known genes, model predicts remaining genes along w/ conf values\nfor K iterations, divides those into K bins, and the top 1/K most confident genes are fixed as known genes for next iteration\n\nonce trained, fine-tune for numerous downstream tasks: batch correction, cell annotation (ground truth: annotated collections of diff cell types), perturbation prediction (predict cell state after a given set of genes are experimentally perturbed), multiomics (each layer, transcriptome, chromatin, proteome, treated as a different language)\n\nNucleotide transformer\n\nfoundational model that focuses on raw DNA sequences\nsequences tokenized into words of 6 characters each (k-mers of length 6)+trained using BERT methodlogy\ntraining dtaa: ref human genome, 3200 additional diverse human genomes, genomes of 850 other species\nnucleotide transformer applied to 18 downstream tasks that encompass many of previously discussed ones (promoter pred, splice site donor/acceptor pred, histone modifications, etc.)\npredictions made either through probing (embeddings at different layers used as features for simple classifiers — e.g. logistic regression+perceptrons) or light, computationally inexpensive fine-tuning\n\nLooking forward\n\nAGI not required (understanding molec bio/link to human health doesn’t need to be an AI task)\nasking AI to learn complex stat properties of existing biological systems\nexpect it to learn one-step causality relationships (mutation → malfunction)\n\nif gene is underexpressed, other genes in cascade in/decrease\ntriangulate between correlations across modalities such as DNA variation, protein abundance, and phenotype (Mendelian randomization) and large-scale perturbation experiments → LLM can model cellular states\ngenome ↔︎ phenotype\n\nsignificant gatekeeper: data\n\nUK Biobank Project (UKB)\n\nlarge-scale biobank, biomedical database+research resources containing comprehensive genetic/health info from 1/2 million UK participants\nlots of other large-scale data initiatives"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tiffanie lab blog",
    "section": "",
    "text": "PyTorch tutorial: tensor demo\n\n\n\n\n\n\n\ndemos\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nLLM in molecular biology summary\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\ntiffanie\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]