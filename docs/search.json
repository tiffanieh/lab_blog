[
  {
    "objectID": "posts/mx_alg_basics_pt1/index.html",
    "href": "posts/mx_alg_basics_pt1/index.html",
    "title": "Matrix algebra basics (numpy) - part 1",
    "section": "",
    "text": "from numpy import diag, array, eye, outer, trace, kron, allclose, linalg as la\n\nCreating a matrix\n\n\\[\n\\mathbf{L} = \\begin{pmatrix}    1&2&3\\\\    4&5&6\\end{pmatrix}\n\\]\nmx_L = array([[1,2,3],[4,5,6]])\n\nMatrix addition\n\n\\[\n\\begin{align*}\n\\mathbf{A} &=\n\\begin{pmatrix}\n1&2\\\\\n3&4\n\\end{pmatrix}\\\\\n\\mathbf{B} &=\n\\begin{pmatrix}\n5&6\\\\\n7&8\n\\end{pmatrix}\\\\\n\\mathbf{A}+\\mathbf{B} &=\n\\begin{pmatrix}\n6&8\\\\\n10&12\n\\end{pmatrix}\n\\end{align*}\n\\]\nmx_A = array([[1,2],[3,4]])\nmx_B = array([[5,6],[7,8]])\nmx_Y = mx_A+mx_B\n\nMatrix multiplication\n\n\\[\n\\mathbf{AB} =\n\\begin{pmatrix}\n19&22\\\\\n47&50\n\\end{pmatrix}\n\\]\nmx_F = mx_A.dot(mx_B)\n\nMatrix transpose\n\n\\[\n\\mathbf{A}^T =\n\\begin{pmatrix}\n1&3\\\\\n2&4\n\\end{pmatrix}\n\\]\nmx_A_t = mx_A.T\n\nIdentity matrix\n\n\\[\n\\mathbf{I} =\n\\begin{pmatrix}\n1&0&0\\\\\n0&1&0\\\\\n0&0&1\n\\end{pmatrix}\n\\]\nmx_I = eye(3,3)\n\nInverse matrix\n\n\\[\n\\begin{align*}\n\\mathbf{C} &=\n\\begin{pmatrix}\n1&2\\\\\n3&4\n\\end{pmatrix}\\\\\n\\mathbf{C}^{-1} &=\n\\begin{pmatrix}\n-2&1\\\\\n\\frac{3}{2}&\\frac{-1}{2}\n\\end{pmatrix}\\\\\n\\end{align*}\n\\]\nmx_C = array([[1,2],[3,4]])\nmx_C_inv = la.inv(mx_C)\n\nEigenvalues+eigenvectors (\\(\\lambda_i\\), \\(\\mathbf{x_i}\\))\n\n\\[\n\\mathbf{Cx} = \\lambda\\mathbf{x}\n\\]\nC_evals, C_evecs = la.eig(mx_C)\n\nMatrix reshape\n\n\\[\n\\begin{align*}\n\\mathbf{g} &=\n\\begin{pmatrix}\n1&2&3&4&5&6&7&8&9\n\\end{pmatrix}\\\\\n\\mathbf{G} &=\n\\begin{pmatrix}\n1&2&3\\\\\n4&5&6\\\\\n7&8&9\n\\end{pmatrix}\n\\end{align*}\n\\]\nvec_g = array([1,2,3,4,5,6,7,8,9])\nmx_G = vec_g.reshape(3,3)\n\nElement-wise matrix multiplication\n\n\\[\n\\mathbf{A}\\odot\\mathbf{B} =\n\\begin{pmatrix}\n5&12\\\\\n21&32\n\\end{pmatrix}\n\\]\nmx_H = mx_A*mx_B\n\nMatrix determinant\n\n\\[\n\\begin{align*}\n\\det(\\mathbf{C}) &=\n\\begin{vmatrix}\n\\mathbf{C}\n\\end{vmatrix}\\\\\n\\begin{vmatrix}\n1&2\\\\\n3&4\n\\end{vmatrix}\n&= -2\n\\end{align*}\n\\]\ndet_mx_C = la.det(mx_C)"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html",
    "href": "posts/transcriptome_qgt_lab/index.html",
    "title": "Transcriptome QGT lab",
    "section": "",
    "text": "Predict whole blood expression\nCheck how well the prediction works with GEUVADIS expression data\nRun association between predicted expression and a phenotype\nCalculate association between expression levels and coronary artery disease risk using s-predixcan\nFine-map the coronary artery disease gwas results using torus\nCalculate colocalization probability using fastenloc\n(skip) Run transcriptome-wide mendelian randomization in one locus of interest\nRun cTWAS (fine-map SNPs and genes jointly)"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#questionnaire-01",
    "href": "posts/transcriptome_qgt_lab/index.html#questionnaire-01",
    "title": "Transcriptome QGT lab",
    "section": "Questionnaire 01",
    "text": "Questionnaire 01\n\nOpen and start filling questionnaire 01 Preliminary questionnaire https://forms.gle/fhNJAyjx7MJTy3yt8\nInstall packages as needed\n\n\n# List of packages you want to install\npackages &lt;- c(\"tidyverse\", \"data.table\", \"BEDMatrix\", \"Rfast\", \"susieR\", \"coloc\")\n\n# Function to check and install any missing packages\ncheck_and_install &lt;- function(pkg){\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg, dependencies = TRUE)\n    library(pkg, character.only = TRUE)\n  }\n}\n\n# Use the function to check and install packages\nsapply(packages, check_and_install)\n\n\nLoad Rstudio Libraries\n\n\nlibrary(tidyverse)\n\n## packages needed for susie+coloc\nlibrary(data.table)\nlibrary(BEDMatrix)\nlibrary(Rfast)\nlibrary(susieR)\nlibrary(coloc)\n##library(tidyverse)\nlibrary(R.utils)\n\n\nNavigate to starting directory\n\n\ncd \"/cloud/project/\"\n\n\nactivate the the imlabtools environment, which will make sure that the right version of python modules are available\n\n\nconda activate imlabtools\n\n\nTo define some variables to access the data more easily within the R session, run the following r chunk\n\n\nprint(getwd())\n\nlab=\"/cloud/project/QGT-Columbia-HKI-repo/\"\nCODE=glue::glue(\"{lab}/code\")\nsource(glue::glue(\"{CODE}/load_data_functions.R\"))\nsource(glue::glue(\"{CODE}/plotting_utils_functions.R\"))\n\nPRE=\"/cloud/project/QGT-Columbia-HKI-repo/box_files\"\nMODEL=glue::glue(\"{PRE}/models\")\nDATA=glue::glue(\"{PRE}/data\")\nRESULTS=glue::glue(\"{PRE}/results\")\nMETAXCAN=glue::glue(\"{PRE}/repos/MetaXcan-master/software\")\nFASTENLOC=glue::glue(\"{PRE}/repos/fastenloc-master\")\n\n# This is a reference table we'll use a lot throughout the lab. It contains information about the genes.\ngencode_df = load_gencode_df()\n\n\ncheck the values of the variables you just defined in R\n\n\nMODEL\n\nDATA\n\n\ndefine some variables to access the data more easily in the terminal. Remember we are running R code in the R console and command line code in the terminal.\n\n\nexport PRE=\"/cloud/project/QGT-Columbia-HKI-repo/box_files\"\nexport LAB=\"/cloud/project/QGT-Columbia-HKI-repo/\"\nexport CODE=$LAB/code\nexport DATA=$PRE/data\nexport MODEL=$PRE/models\nexport RESULTS=$PRE/results\nexport METAXCAN=$PRE/repos/MetaXcan-master/software\n\n\ncheck the values of the variables you just defined\n\n\n\necho $CODE\necho $RESULTS"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#questionnaire-02",
    "href": "posts/transcriptome_qgt_lab/index.html#questionnaire-02",
    "title": "Transcriptome QGT lab",
    "section": "Questionnaire 02",
    "text": "Questionnaire 02\n\nOpen and start filling questionnaire 02 Prediction https://forms.gle/T6kAHvFTxYfcQguW7"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#optional-assess-actual-prediction-performance",
    "href": "posts/transcriptome_qgt_lab/index.html#optional-assess-actual-prediction-performance",
    "title": "Transcriptome QGT lab",
    "section": "(Optional) Assess Actual Prediction Performance",
    "text": "(Optional) Assess Actual Prediction Performance\n\n## download and read observed expression data from GEUVADIS \n## from https://uchicago.box.com/s/4y7xle5l0pnq9d1fwmthe2ewhogrnlrv\n\nobs_exp&lt;- read_csv(glue::glue(\"{DATA}/predixcan/GEUVADIS.observed_df.csv.gz\"))\n\n## Note that the version of the ensemble id of the gene was removed\nhead(predicted_expression)\n\n## Q: how many genes were predicted?\nlength(unique(predicted_expression$gene_id))\n\n## inner join predicted expression with observed expression data (by IID and gene)\n## common errors occur when ensemble id's have versions in one set and not the other set\nfullset=inner_join(predicted_expression, obs_exp, by = c(\"gene_id\",\"IID\"))\n\n## calculate spearman correlation for all genes\ngenelist = unique(predicted_expression$gene_id)\ncorvec = rep(NA,length(genelist))\nnames(corvec) = genelist\nfor(gg in 1:length(genelist))\n{\n  ind = fullset$gene_id==genelist[gg]\n  corvec[gg] = cor(fullset$predicted_expression[ind], fullset$observed_expression[ind])\n}\n\n## what's the best performing gene?\n\n## plot the histogram of the prediction performance\nhist(corvec)\n\n## list the top 10 best performing genes\nhead(sort(corvec,decreasing = TRUE),2)\ntail(sort(corvec,decreasing = TRUE),2)\n\n## plot the correlation of the top 2 best performing genes bottom 2\n\ngeneid = \"ENSG00000100376\"\ngenename = gencode_df %&gt;% filter(gene_id==geneid) %&gt;% .[[\"gene_name\"]]\nfullset %&gt;% filter(gene_id==geneid) %&gt;% ggplot(aes(observed_expression, predicted_expression))+geom_point()+ggtitle(paste(genename, \"-\",geneid))\n\ngeneid = \"ENSG00000075234\"\ngenename = gencode_df %&gt;% filter(gene_id==geneid) %&gt;% .[[\"gene_name\"]]\nfullset %&gt;% filter(gene_id==geneid) %&gt;% ggplot(aes(observed_expression, predicted_expression))+geom_point()+ggtitle(paste(genename, \"-\", geneid))\n\ngeneid = \"ENSG00000070371\"\ngenename = gencode_df %&gt;% filter(gene_id==geneid) %&gt;% .[[\"gene_name\"]]\nfullset %&gt;% filter(gene_id==geneid) %&gt;% ggplot(aes(observed_expression, predicted_expression))+geom_point()+ggtitle(paste(genename, \"-\", geneid))\n\ngeneid = \"ENSG00000184164\"\ngenename = gencode_df %&gt;% filter(gene_id==geneid) %&gt;% .[[\"gene_name\"]]\nfullset %&gt;% filter(gene_id==geneid) %&gt;% ggplot(aes(observed_expression, predicted_expression))+geom_point()+ggtitle(paste(genename, \"-\", geneid))"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#questionnaire-03",
    "href": "posts/transcriptome_qgt_lab/index.html#questionnaire-03",
    "title": "Transcriptome QGT lab",
    "section": "Questionnaire 03",
    "text": "Questionnaire 03\n\nOpen and start filling questionnaire 03 PrediXcan https://forms.gle/3H319knWbLgnynNs9\n\nWe are going to use a simulated phenotype for which only UPK3A has an effect on the phenotype (\\(\\beta=-0.9887378\\))\n\\(Y = \\sum_k T_k \\beta_k + \\epsilon\\)\nwith random effects \\(\\beta_k \\sim (1-\\pi)\\cdot \\delta_0 + \\pi\\cdot N(0,1)\\)\nHere we are running associations between phenotype and predicted expression.\n\n\nexport PHENO=\"sim.spike_n_slab_0.01_pve0.1\"\n\nprintf \"association\\n\\n\"\npython3 $METAXCAN/PrediXcanAssociation.py \\\n--expression_file $RESULTS/predixcan/Whole_Blood__predict.txt \\\n--input_phenos_file $DATA/predixcan/phenotype/$PHENO.txt \\\n--input_phenos_column pheno \\\n--output $RESULTS/predixcan/$PHENO/Whole_Blood__association.txt \\\n--verbosity 9 \\\n--throw\n\nMore predicted phenotypes can be found in $DATA/predixcan/phenotype/. The naming of the phenotypes provides information about the genetic architecture: the number after pve is the proportion of variance of Y explained by the genetic component of expression. The number after spike_n_slab represents the probability that a gene is causal π (i.e. prob β≠0)"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#looking-at-association-results",
    "href": "posts/transcriptome_qgt_lab/index.html#looking-at-association-results",
    "title": "Transcriptome QGT lab",
    "section": "Looking at Association Results",
    "text": "Looking at Association Results\n\n## read association results\nPHENO=\"sim.spike_n_slab_0.01_pve0.1\"\n\npredixcan_association = load_predixcan_association(glue::glue(\"{RESULTS}/predixcan/{PHENO}/Whole_Blood__association.txt\"), gencode_df)\n\n## take a look at the results\ndim(predixcan_association)\npredixcan_association %&gt;% arrange(pvalue) %&gt;% select(gene_name,effect,se,pvalue,gene) %&gt;% head\npredixcan_association %&gt;% arrange(pvalue) %&gt;% ggplot(aes(pvalue)) + geom_histogram(bins=10)\n## compare distribution against the null (uniform)\ngg_qqplot(predixcan_association$pvalue, max_yval = 40)\n\nQ3.6) How many genes are significantly associated with the simulated phenotype? (count the genes with p-value&lt;0.05/total number of tests)\n\nlength(unique(predixcan_association$gene[which(predixcan_association$pvalue&lt;0.05/230)]))\n#7"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#comparing-the-estimated-effect-size-with-true-effect-size",
    "href": "posts/transcriptome_qgt_lab/index.html#comparing-the-estimated-effect-size-with-true-effect-size",
    "title": "Transcriptome QGT lab",
    "section": "Comparing the Estimated Effect Size with True Effect Size",
    "text": "Comparing the Estimated Effect Size with True Effect Size\n\ntruebetas = load_truebetas(glue::glue(\"{DATA}/predixcan/phenotype/gene-effects/{PHENO}.txt\"), gencode_df)\nbetas = (predixcan_association %&gt;% \n               inner_join(truebetas,by=c(\"gene\"=\"gene_id\")) %&gt;%\n               select(c('estimated_beta'='effect', \n                        'true_beta'='effect_size',\n                        'pvalue', \n                        'gene_id'='gene', \n                        'gene_name'='gene_name.x', \n                        'region_id'='region_id.x')))\nbetas %&gt;% arrange(pvalue) %&gt;% select(gene_name,estimated_beta,true_beta,pvalue) %&gt;% head\n## do you see examples of potential LD contamination?\nbetas %&gt;% mutate(causal= true_beta!=0) %&gt;% ggplot(aes(estimated_beta, true_beta,col=causal))+geom_point(alpha=0.6,size=5)+geom_abline()+theme_bw()\n\nThere are lots of large values that are just noise (looking at estimated and true values).\nQ3.7) How many genes have non-zero true effect sizes?\n\nsum(betas$true_beta!=0)\n#1\n\nQ3.8) How many estimated effect sizes are different from 0?\n\nsum(betas$estimated_beta!=0)\n#230\n\nQ3.9) Which gene is the most likely causal gene?\n\nbetas$gene_name[which(betas$pvalue==min(betas$pvalue))]\n#UPK3A\n\nQ3.10) How many genes have effects that are significantly (Bonferroni corrected) different from zero?\n\nlength(unique(betas$gene_id[which(betas$estimated_beta!=0&betas$pvalue&lt; (0.05/230))]))\n#7\n\n^After this, can further reduce # suspected effects using colocalization.\n\nUPK3A is the causal gene and has the most significant pvalue. RIBC2 is also significantly associated but has no causal role (we know because we simulated the phenotype that way). Why?\n\nHint: correlation between the genes"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#questionnaire-04",
    "href": "posts/transcriptome_qgt_lab/index.html#questionnaire-04",
    "title": "Transcriptome QGT lab",
    "section": "Questionnaire 04",
    "text": "Questionnaire 04\n\nOpen and start filling questionnaire 04 S-PrediXcan https://forms.gle/xJs2U66cnrqdb5cj6"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#running-s-predixcan",
    "href": "posts/transcriptome_qgt_lab/index.html#running-s-predixcan",
    "title": "Transcriptome QGT lab",
    "section": "Running S-PrediXcan",
    "text": "Running S-PrediXcan\n\n\npython $METAXCAN/SPrediXcan.py \\\n--gwas_file  $DATA/spredixcan/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz \\\n--snp_column panel_variant_id \\\n--effect_allele_column effect_allele \\\n--non_effect_allele_column non_effect_allele \\\n--zscore_column zscore \\\n--model_db_path $MODEL/gtex_v8_mashr/mashr_Whole_Blood.db \\\n--covariance $MODEL/gtex_v8_mashr/mashr_Whole_Blood.txt.gz \\\n--keep_non_rsid \\\n--additional_output \\\n--model_db_snp_key varID \\\n--throw \\\n--output_file $RESULTS/spredixcan/eqtl/CARDIoGRAM_C4D_CAD_ADDITIVE__PM__Whole_Blood.csv\n\nQ4.2) What prediction model did you use?\n\nWe can run the full genome because the summary statistics based PrediXcan is much faster than individual level one."
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#plot-and-interpret-results",
    "href": "posts/transcriptome_qgt_lab/index.html#plot-and-interpret-results",
    "title": "Transcriptome QGT lab",
    "section": "Plot and Interpret Results",
    "text": "Plot and Interpret Results\n\nspredixcan_association = load_spredixcan_association(glue::glue(\"{RESULTS}/spredixcan/eqtl/CARDIoGRAM_C4D_CAD_ADDITIVE__PM__Whole_Blood.csv\"), gencode_df)\ndim(spredixcan_association)\nspredixcan_association %&gt;% arrange(pvalue) %&gt;% head\nspredixcan_association %&gt;% arrange(pvalue) %&gt;% ggplot(aes(pvalue)) + geom_histogram(bins=20)\n\ngg_qqplot(spredixcan_association$pvalue)\n\nQ4.3) Which gene has the smallest p-value?\n\nspredixcan_association$gene_name[which(spredixcan_association$pvalue==min(spredixcan_association$pvalue, na.rm=TRUE))]\n\n\nQuestion: SORT1, considered to be a causal gene for LDL cholesterol and as a consequence of coronary artery disease, is not found here. Why?\n\nQ4.5) Is SORT1 (ENSG00000134243) significantly associated with CAD?\n\nsum(spredixcan_association$gene==\"ENSG00000134243\")\nsum(spredixcan_association$gene_name==\"SORT1\")\n\n\ncheck whether SORT1 is expressed in whole blood GTEx portal\ncheck whether SORT1 has eQTL in whole blood GTEx portal"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#run-s-predixcan-using-gene-expression-predicted-in-liver",
    "href": "posts/transcriptome_qgt_lab/index.html#run-s-predixcan-using-gene-expression-predicted-in-liver",
    "title": "Transcriptome QGT lab",
    "section": "Run S-PrediXcan using gene expression predicted in liver",
    "text": "Run S-PrediXcan using gene expression predicted in liver\n-[] Run s-predixcan with liver model, do you find SORT1? Is it significant?\n\n#loction Liver models \n#/cloud/project/QGT-Columbia-HKI-repo/box_files/models/gtex_v8_mashr/\npython $METAXCAN/SPrediXcan.py \\\n--gwas_file  $DATA/spredixcan/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz \\\n--snp_column panel_variant_id \\\n--effect_allele_column effect_allele \\\n--non_effect_allele_column non_effect_allele \\\n--zscore_column zscore \\\n--model_db_path $MODEL/gtex_v8_mashr/mashr_Liver.db \\\n--covariance $MODEL/gtex_v8_mashr/mashr_Liver.txt.gz \\\n--keep_non_rsid \\\n--additional_output \\\n--model_db_snp_key varID \\\n--throw \\\n--output_file $RESULTS/spredixcan/eqtl/CARDIoGRAM_C4D_CAD_ADDITIVE__PM__Liver.csv\n\n\nspredixcan_association_L= load_spredixcan_association(glue::glue(\"{RESULTS}/spredixcan/eqtl/CARDIoGRAM_C4D_CAD_ADDITIVE__PM__Liver.csv\"), gencode_df)\ndim(spredixcan_association_L)\nspredixcan_association_L %&gt;% arrange(pvalue) %&gt;% head\nspredixcan_association_L %&gt;% arrange(pvalue) %&gt;% ggplot(aes(pvalue)) + geom_histogram(bins=20)\n\ngg_qqplot(spredixcan_association_L$pvalue)\ncol_order= c(\"gene_name\",\"gene\",\"zscore\",\"effect_size\",\"pvalue\",\"var_g\",\"pred_perf_r2\", \"pred_perf_pval\",\"pred_perf_qval\", \"n_snps_used\", \"n_snps_in_cov\", \"n_snps_in_model\",\"best_gwas_p\",\"largest_weight\")\nspredixcan_association_L &lt;- spredixcan_association_L[, col_order]\nfilter(spredixcan_association_L, gene_name==\"SORT1\")\n\nQ4.5) Is SORT1 (ENSG00000134243) significantly associated with CAD?\n\nsum(spredixcan_association_L$gene==\"ENSG00000134243\")\nspredixcan_association_L$pvalue[which(spredixcan_association_L$gene_name==\"SORT1\")]\n#p value = 4.629983e-23\n\n4.629983e-23 &lt; 0.05/length(unique(spredixcan_association_L$gene_name))"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#optional-compare-zscores-in-liver-and-whole-blood.",
    "href": "posts/transcriptome_qgt_lab/index.html#optional-compare-zscores-in-liver-and-whole-blood.",
    "title": "Transcriptome QGT lab",
    "section": "(Optional) Compare zscores in liver and whole blood.",
    "text": "(Optional) Compare zscores in liver and whole blood.\n\nRecall that zscore is the effect size divided by the standard error\n\n\nspredixcan_association_L=rename(spredixcan_association_L, zscore_liver = \"zscore\")\nhead(spredixcan_association_L)\ntest=left_join(spredixcan_association, spredixcan_association_L, by=\"gene_name\")\ntest=select(test,\"gene_name\",\"zscore\",\"zscore_liver\")\ntest %&gt;% arrange(zscore_liver) %&gt;% head\n\ntest %&gt;% mutate(zscore_WB=zscore) %&gt;% ggplot(aes(zscore_WB,zscore_liver)) + geom_point(size=3,alpha=.6) + geom_abline()\n\n## S-PrediXcan association in liver and whole blood are significantly correlated\ncor.test(test$zscore,test$zscore_liver)\n\nQ4.6) In which region of the qqplot are Bonferroni significant genes located?\nAbove the horizontal line\n\ngg_qqplot(spredixcan_association_L$pvalue[which(spredixcan_association_L$pvalue&lt;0.05/230)]) #graphing only significant genes\n\n\ngg_qqplot(spredixcan_association_L$pvalue)"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#optional-run-multixcan",
    "href": "posts/transcriptome_qgt_lab/index.html#optional-run-multixcan",
    "title": "Transcriptome QGT lab",
    "section": "(Optional) Run MultiXcan",
    "text": "(Optional) Run MultiXcan\n\nmultixcan aggregates information across multiple tissues to boost the power to detect association. It was developed movivated by the fact that eQTLs are shared across multiple tissues, i.e. many genetic variants that regulate expression are common across tissues.\nbefore you run multixcan ensure you have run s-predixcan for all the tissues you want to multixcan. In this tutorial we have two tissues (liver and whole blood), ensure you have run s-predixcan with the two tissues before running multixcan.\nOne thing to note is to ensure similar naming pattern for the output files. This is to ensure the files are captured correctly when running multixcan’s filter.\n\n\n\npython $METAXCAN/SMulTiXcan.py \\\n--models_folder $MODEL/gtex_v8_mashr \\\n--models_name_pattern \"mashr_(.*).db\" \\\n--snp_covariance $MODEL/gtex_v8_expression_mashr_snp_smultixcan_covariance.txt.gz \\\n--metaxcan_folder $RESULTS/spredixcan/eqtl/ \\\n--metaxcan_filter \"CARDIoGRAM_C4D_CAD_ADDITIVE__PM__(.*).csv\" \\\n--metaxcan_file_name_parse_pattern \"(.*)__PM__(.*).csv\" \\\n--gwas_file $DATA/spredixcan/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz \\\n--snp_column panel_variant_id --effect_allele_column effect_allele --non_effect_allele_column non_effect_allele --zscore_column zscore --keep_non_rsid --model_db_snp_key varID \\\n--cutoff_condition_number 30 \\\n--verbosity 9 \\\n--throw \\\n--output $RESULTS/smultixcan/eqtl/CARDIoGRAM_C4D_CAD_ADDITIVE_smultixcan.txt"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#questionnaire-5",
    "href": "posts/transcriptome_qgt_lab/index.html#questionnaire-5",
    "title": "Transcriptome QGT lab",
    "section": "Questionnaire 5",
    "text": "Questionnaire 5\n\nOpen and start filling questionnaire 05 Colocalization https://forms.gle/NfH2MSdy4UyJzGAp7"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#run-colocalization",
    "href": "posts/transcriptome_qgt_lab/index.html#run-colocalization",
    "title": "Transcriptome QGT lab",
    "section": "Run colocalization",
    "text": "Run colocalization\nWhen you use coloc on your own data, you may want to check out coloc’s documentation, with good advice and tips on avoiding common mistakes https://cran.r-project.org/web/packages/coloc/vignettes\nDue to time constraints, we will run one region and one gene only\n\nFinemap GWAS of CAD\nFinemap eQTL of SORT1\n\nFor finemapping, we need the summary statistics (effect size, standard errors, etc) and the correlation between SNPs (LD matrix)"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#load-the-genotype-to-calculate-the-ld-matrix",
    "href": "posts/transcriptome_qgt_lab/index.html#load-the-genotype-to-calculate-the-ld-matrix",
    "title": "Transcriptome QGT lab",
    "section": "load the genotype to calculate the ld matrix",
    "text": "load the genotype to calculate the ld matrix\n\n# load the genotype to calculate the ld matrix\nX_mat &lt;- BEDMatrix(glue::glue(\"{DATA}/colocalization/geuvadis_chr1\"))\ncolnames(X_mat) &lt;- gsub(\"\\\\_.*\", \"\",colnames(X_mat))\ncolnames(X_mat) &lt;- str_replace_all(colnames(X_mat),\":\",\"_\")\nsnp_info &lt;- fread(glue::glue(\"{DATA}/colocalization/geuvadis_chr1.bim\")) %&gt;% \n  setnames(., colnames(.), c(\"chr\", \"snp\", \"cm\", \"pos\", \"alt\", \"ref\")) \n\nsnp_info$snp &lt;- str_replace_all(snp_info$snp,\":\",\"_\")"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#load-the-eqtl-and-gwas-effect-size-files",
    "href": "posts/transcriptome_qgt_lab/index.html#load-the-eqtl-and-gwas-effect-size-files",
    "title": "Transcriptome QGT lab",
    "section": "Load the eqtl and gwas effect size files",
    "text": "Load the eqtl and gwas effect size files\n\n# load the eqtl effect sizes\ngene_ss &lt;- fread(glue::glue(\"{DATA}/colocalization/Liver_chr1.txt\"))\n\n# load gwas effect sizes\ngwas &lt;- data.table::fread(glue::glue(\"{DATA}/spredixcan/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz\"))\n\n# filter to select genome wide significant snps at 5 × 10−8\nfiltered_regions &lt;- gwas %&gt;% dplyr::filter(pvalue &lt; 5e-8)\n\n# load the ld block - attempt to split genome into independent blocks\nldblocks &lt;-read_tsv(glue::glue(\"{DATA}/spredixcan/eur_ld.hg38.txt.gz\"))"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#find-regions-with-the-strongest-signal-in-the-gwas",
    "href": "posts/transcriptome_qgt_lab/index.html#find-regions-with-the-strongest-signal-in-the-gwas",
    "title": "Transcriptome QGT lab",
    "section": "Find regions with the strongest signal in the gwas",
    "text": "Find regions with the strongest signal in the gwas\n\n# get the loci where the significant snps are located\nfor (n in 1:nrow(filtered_regions)){\n  # extract genename, start and end\n  variant_id &lt;- as.character(filtered_regions[n,\"variant_id\"])\n  variant_chr &lt;- as.character(filtered_regions[n,\"chromosome\"])\n  variant_pos &lt;- as.numeric(filtered_regions[n,\"position\"])\n  #gene_end &lt;- as.numeric(genes[n,\"end\"])\n\n  locus &lt;- ldblocks %&gt;%\n    dplyr::filter(chr == variant_chr) %&gt;%\n    filter(variant_pos &gt;= start & variant_pos &lt; stop) %&gt;%\n    mutate(locus_name = paste0(chr,\"_\",start,\"_\",stop)) %&gt;%\n    dplyr::rename(locus_start=start,locus_end=stop) %&gt;%\n    mutate(variant_id = variant_id, position=variant_pos)\n\n  # create a data frame with info\n  if (exists('all_loci') && is.data.frame(get('all_loci'))) {\n    all_loci &lt;- rbind(all_loci,locus)\n  } else {\n    all_loci &lt;- locus\n  }\n}\n\n# select uniq loci\nd_loci &lt;- all_loci %&gt;%\n  dplyr::select(locus_name,chr,locus_start,locus_end) %&gt;%\n  dplyr::distinct()"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#select-a-region-to-run-coloc",
    "href": "posts/transcriptome_qgt_lab/index.html#select-a-region-to-run-coloc",
    "title": "Transcriptome QGT lab",
    "section": "select a region to run coloc",
    "text": "select a region to run coloc\n\n# select regions to fine map. we are going to use regions in chromosome 1\nuniq_loci &lt;- d_loci %&gt;% dplyr::filter(\"chr1\" == chr)\nn = 3 # chr1_107867043_109761309 region\n\n# extract information\nl_chr = as.numeric(str_remove(uniq_loci[n,\"chr\"],\"chr\"))\ns_chr = uniq_loci[n,]$chr\nl_start = uniq_loci[n,]$locus_start\nl_stop = uniq_loci[n,]$locus_end\nl_name = uniq_loci[n,]$locus_name"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#prepare-gwas-data-for-coloc",
    "href": "posts/transcriptome_qgt_lab/index.html#prepare-gwas-data-for-coloc",
    "title": "Transcriptome QGT lab",
    "section": "Prepare gwas data for coloc",
    "text": "Prepare gwas data for coloc\n\n# select snps for the region from the summary stats\nss &lt;- gwas %&gt;% \n  dplyr::filter(chromosome == s_chr) %&gt;%\n  dplyr::filter(position &gt;= l_start & position &lt;= l_stop) %&gt;% \n  dplyr::filter(! is.na(effect_size))\n\n# find the snps in the genotype to calculate the correlation\ng.snps &lt;- ss %&gt;% inner_join(snp_info %&gt;% mutate(chr = glue::glue(\"chr{chr}\")), \n                            by=c(\"chromosome\" = \"chr\",\"panel_variant_id\" = \"snp\"))\n\n\n# select genotype to calculate correlation\n#f_mat &lt;- X_mat[,g.snps$snp]\nf_mat &lt;- X_mat[,g.snps$panel_variant_id]\n\n# calculate corr\nR = cora(f_mat) # the package is for speed\n\n## clean up\nrm(f_mat)\n\nff &lt;- g.snps %&gt;% dplyr::filter(! is.na(effect_size)) %&gt;% #select(-snp) %&gt;% \n  dplyr::rename(snp=panel_variant_id,beta=effect_size) %&gt;% \n  mutate(varbeta = standard_error^2) %&gt;% \n  dplyr::select(beta,varbeta,snp,position) %&gt;% as.list()\n\nff$type &lt;- \"cc\"\nff$sdY &lt;- 1\n\nff$LD = R\nff$N = 184305\n\n## check the data (NULL means it's fine)\ncheck_dataset(ff)"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#prepare-eqtl-data-for-coloc",
    "href": "posts/transcriptome_qgt_lab/index.html#prepare-eqtl-data-for-coloc",
    "title": "Transcriptome QGT lab",
    "section": "Prepare eqtl data for coloc",
    "text": "Prepare eqtl data for coloc\n\n#Using SORT1 gene and liver tissue\ngene &lt;- gene_ss %&gt;% dplyr::filter(gene_id == \"ENSG00000134243.11\") %&gt;% \n  dplyr::rename(snp = variant_id,beta = slope, MAF = maf,\n                pvalue = pval_nominal) %&gt;% \n  mutate(varbeta = slope_se^2, name = snp) %&gt;% \n  filter(! is.na(varbeta)) %&gt;% \n  separate(name, into = c(\"chr\", \"position\",\"ref\",\"alt\",\"build\"),sep = \"_\")\n\n## calculate the ld matrix\n### get the snps\ngene.snps &lt;- gene %&gt;% mutate(position = as.integer(position)) %&gt;% \n  inner_join(snp_info %&gt;% mutate(chr = glue::glue(\"chr{chr}\")), \n                              by=c(\"chr\" = \"chr\",\"snp\" = \"snp\"))\n\n# select genotype to calculate correlation\ng_mat &lt;- X_mat[,gene.snps$snp]\n# calculate corr\ng.R = cora(g_mat)\n\n# clean up\nrm(g_mat)\n\n# format data for coloc\ngg &lt;- gene %&gt;% dplyr::filter(snp %in% gene.snps$snp) %&gt;%\n  mutate(position = as.integer(position)) %&gt;% \n  dplyr::select(beta,varbeta,snp,position,MAF, pvalue) %&gt;% as.list()\n\ngg$type &lt;- \"quant\"\ngg$LD = g.R\ngg$N = 208 # 670 for blood\n\n## check the data\ncheck_dataset(gg)"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#run-older-version-of-coloc-which-assumes-single-causal-variant",
    "href": "posts/transcriptome_qgt_lab/index.html#run-older-version-of-coloc-which-assumes-single-causal-variant",
    "title": "Transcriptome QGT lab",
    "section": "run older version of coloc which assumes single causal variant",
    "text": "run older version of coloc which assumes single causal variant\ncoloc.abf makes the simplifying assumption that each trait has at most one causal variant in the region under consideration\n\nmy.res &lt;- coloc.abf(dataset1=ff, dataset2=gg)\nsensitivity(my.res,\"H4 &gt; 0.9\")"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#run-coloc-allowing-multiple-causal-variants",
    "href": "posts/transcriptome_qgt_lab/index.html#run-coloc-allowing-multiple-causal-variants",
    "title": "Transcriptome QGT lab",
    "section": "run coloc allowing multiple causal variants",
    "text": "run coloc allowing multiple causal variants\nMultiple causal variants, using SuSiE to separate the signals\n\n# Run susie fine mapping\nS3 = runsusie(ff)\nS4 = runsusie(gg)\n#summary(S3)\n\n# Run coloc\nsusie.res=coloc.susie(S3,S4)\nprint(susie.res$summary)\n\nSuSiE can take a while to run on larger datasets, so it is best to run once per dataset with the =runsusie= function, store the results and feed those into subsequent analyses.\nplot the coloc result with the sensitivity function because weird effects are much easier to understand visually\n\nsensitivity(susie.res,\"H4 &gt; 0.9\",row=1,dataset1=ff,dataset2=gg,)\n\nQ5.1) What is fine-mapping of GWAS loci? An estimation of the probability that a variant is causal.\nQ5.2) What does PIP stand for? Posterior inclusion probability.\nQ5.3) What is the interpretation of PIP? The posterior probability that a variant is causal.\nQ5.4) What is the interpretation PIP? The posterior probability that a variant is causal.\nQ5.5) Relationship between p-value and PIP Typically, a causal variant will have a small p-value and a large PIP, so they will be somewhat correlated, but many variants with small p-values may have zero PIP.\nQ5.6) How do colocalization methods work? Colocalization tries to estimate the probability that the causal variants of the expression trait is the same as the causal variant of the complex trait.\nQ5.7) How many SNPs are used for finemapping the gwas?\n\nlength(ff$snp)\n#3853\n\nQ5.8) How many SNPs are used for finemapping the eqtl?\n\nlength(gg$snp)\n#5628\n\nQ5.9) How many SNPs were used to run coloc? Hint: overlap between gwas and eqtl SNPs\n\nlength(intersect(gg$snp,ff$snp))\n#2523"
  },
  {
    "objectID": "posts/transcriptome_qgt_lab/index.html#questionnaire-06",
    "href": "posts/transcriptome_qgt_lab/index.html#questionnaire-06",
    "title": "Transcriptome QGT lab",
    "section": "Questionnaire 06",
    "text": "Questionnaire 06\n\nOpen and start filling out questionnaire 06 cTWAS https://forms.gle/A4evWkbhR7cXLy36A\n\n\n#install.packages(\"R.utils\")\n\n#install.packages(\"remotes\")\n#remotes::install_github(\"simingz/ctwas\", ref = \"develop\")\n\nlibrary(ctwas)\n\n#get positions for region of interest (SORT1/PSRC1 locus)\nregion &lt;- unlist(strsplit(spredixcan_association$region_id[spredixcan_association$gene_name==\"PSRC1\"], \"_\"))\nchr &lt;- region[2]\nstart &lt;- as.numeric(region[3])\nend &lt;- as.numeric(region[4])\n\n#format summary statistics (and subset to variants in region to save memory)\nz_snp &lt;- data.table::fread(glue::glue(\"{DATA}/spredixcan/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz\"), select=c(\"chromosome\", \"position\", \"variant_id\", \"effect_allele\", \"non_effect_allele\", \"zscore\", \"sample_size\"))\nz_snp &lt;- z_snp[z_snp$chromosome==chr & z_snp$position &gt;= start & z_snp$position &lt;= end,]\nz_snp &lt;- z_snp[!is.na(z_snp$variant_id),-(1:2)]\ncolnames(z_snp) &lt;- c(\"id\", \"A1\", \"A2\", \"z\", \"ss\")\n\n#specify directories for LD matrices and weights\nld_R_dir &lt;- glue::glue(\"{DATA}/cTWAS/LD_matrices\")\nweight &lt;-  glue::glue(\"{MODEL}/gtex_v8_mashr/mashr_Liver.db\")\n\n#specify output locations and names for cTWAS\noutputdir &lt;- glue::glue(\"{DATA}/cTWAS/results/\")\noutname.e &lt;- \"CARDIoGRAM_Liver_expr\"\noutname &lt;- \"CARDIoGRAM_Liver_ctwas\"\n\n#impute gene z scores using cTWAS and save the results\n##################################\n# NOTE: we are skipping this step and using the precomputed values \n## takes ~10 minutes\n##################################\n# ctwas_imputation &lt;- impute_expr_z(z_snp=z_snp, weight=weight, ld_R_dir=ld_R_dir, outputdir=outputdir, outname=outname.e, harmonize_z=F, harmonize_wgt=F)\n# save(ctwas_imputation, file = paste0(outputdir, outname.e, \"_output.Rd\"))\nload(paste0(outputdir, outname.e, \"_output.Rd\"))\n\nz_gene &lt;- ctwas_imputation$z_gene\nld_exprfs &lt;- ctwas_imputation$ld_exprfs\nz_snp &lt;- ctwas_imputation$z_snp\n\n\n#make custom region file for single region\nld_regions_custom &lt;- data.frame(\"chr\" = chr, \"start\" = start, \"stop\" = end)\n\nwrite.table(ld_regions_custom, \n            file= paste0(outputdir, \"ld_regions_custom.txt\"),\n            row.names=F, col.names=T, sep=\"\\t\", quote = F)\n    \nld_regions_custom &lt;- paste0(outputdir, \"ld_regions_custom.txt\")\n\n#run cTWAS with pre-specified prior parameters at a single locus\n#estimating prior requires genome-wide data, too slow for demonstration\n#prior is 1% inclusion for genes and is 100x more likely than SNPs\n\n#if SNPs and genes have similar evidence from the data, we prioritize the gene\n\n#prior assumes genes have larger effect size than SNPs, in reasonable range for data we've looked at\n\n\n#jointly fine mapping all the genes and all the SNPs in this region\n#ctwas_rss(z_gene=z_gene, z_snp=z_snp, ld_exprfs=ld_exprfs, ld_R_dir = ld_R_dir, ld_regions_custom=ld_regions_custom, outputdir = outputdir, outname = outname, thin = 0.01,\n          #estimate_group_prior = F,\n          #estimate_group_prior_var = F,\n          #group_prior=c(0.01, 0.0001),\n          #group_prior_var=c(50, 25))\n\n#load results\nctwas_results &lt;- data.table::fread(paste0(outputdir,outname,\".susieIrss.txt\"))\n\nQ6.1) What’s the posterior inclusion probability of SORT1?\n\nctwas_results$susie_pip[which(ctwas_results$genename==\"SORT1\")]\n#0.999728\n\nQ6.2) How many SNPs are used to run cTWAS?\n\nctwas_results$\n\n\n#merge gene names into the results\nsqlite &lt;- RSQLite::dbDriver(\"SQLite\")\ndb = RSQLite::dbConnect(sqlite, weight)\nquery &lt;- function(...) RSQLite::dbGetQuery(db, ...)\nextra_table &lt;- query(\"select * from extra\")\nRSQLite::dbDisconnect(db)\n\nctwas_results$genename &lt;- extra_table$genename[match(ctwas_results$id, extra_table$gene)]\n\n#show results with highest PIPs\ncol_order_2= c(\"genename\", \"chrom\", \"id\", \"pos\", \"type\", \"region_tag1\", \"region_tag2\", \"cs_index\", \"susie_pip\" , \"mu2\")\nctwas_results &lt;- ctwas_results[, ..col_order_2]\nhead(ctwas_results[order(-ctwas_results$susie_pip),])\n\nQ6.2) How many SNPs are used to run cTWAS?\n\nsum(ctwas_results$type==\"SNP\")\n\nQ6.3) How many genes/transcripts have non zero probability of being causal (pip)?\n\nsum(ctwas_results$type==\"gene\"&ctwas_results$susie_pip!=0)\n\nQ6.4) How many snps have non zero pip?\n\nsum(ctwas_results$type==\"SNP\"&ctwas_results$susie_pip!=0)\n\nQ6.5) What is the most likely causal gene?\n\nctwas_results$genename[which(ctwas_results$susie_pip==max(ctwas_results$susie_pip))]\n\nDifference between genes and SNPs: in the prior When you define prior, all SNPs are non-informative prior (all equally likely to be causal) \\(\\to\\) 1/# SNPs\nall SNPS of a region are equally likely (1/# genes)\nbased on biological knowledge we can give higher prior probability to genes rather than SNPs\n(Still kind of arbitrary though)"
  },
  {
    "objectID": "posts/llm_summary/index.html",
    "href": "posts/llm_summary/index.html",
    "title": "LLM in molecular biology summary",
    "section": "",
    "text": "LLM in molecular biology summary\narticle link\nLarge language models\n\nLLM: a type of neural network that acquires the ability to generate text mirroring human language by scrutinizing vast amounts of textual data\n\nself-supervised — model learns to predict subsequent word in a sentence based on preceding words\ncan identify patterns and try to predict (i.e. advanced form of autocomplete)\n\n\nPrimary types of language models and their unique features:\n\nword grams: predict next word in a sentence based on frequency of word pairs/word bags (sets of words) — disregard context or word order (generates text that bear little resemblance to human text)\nCNNs: analyze text data by considering relationships between adjacent words in a fixed window; good at identifying local patterns, but fall short in capturing long-range dependencies or comprehending complex sentence structures\nLSTMs (long short-term memory networks): variant of RNNs; store+process information from earlier parts of a text; outperform CNNs in understanding context/managing long-range dependencies, but falter w/ complex sentences+long text\nattention mechanisms: enable models to concentrate on pertinent parts of input when making predictions; number of attention “heads” allow model to focus on different parts of previous text when predicting the next word\n\nlike revisiting key points/details; model refers back to relevant parts of text+incorporates info into current context\nex. transformers are a class of language models that implement attention mechanisms\n\nLLMs: models such as GPT-3 are transformers that leverage attention mechanisms+are trained on vast amounts of data; considerable size facilitates the learning of intricate patterns/relationships/context within text; represent the most advanced language models presently available, capable of generating more accurate+coherent responses across a broad spectrum of topics\n\n2 LLMs that use transformer architecture: BERT+GPT series\nBERT (bidirectional encoder representations from transformers): a series of LLMs by Google+open sourced\n\ntrained using masked language modeling (hide/“mask” some percentage of input tokens at random, then predict those masked tokens)\n\nforces model to understand context from both left/right sides of input (bidirectional)\n\nalso uses next sentence prediction task\nduring training, model is given pairs of sentences+has to predict whether second sentence in pair is the next sentence in the original document\n\nGPT (generative pretrained transformer): series of LLMs introduced by OpenAI\n\ntrained using traditional language modeling task of autocomplete (predict next word in sentence)\nonly attends to left context (previous tokens) during training (unidirectional)\ngenerative model that is stronger in tasks involving text generation\n\nThe genetic dogma\n\nbiological trajectory of a human or any other organism is a complex interplay between genetics+environment (DNA and environment individual is exposed to) aka genotype-phenotype-environment\ncentral dogma of molec bio describes flow of genetic info within living organisms\n\nsource of genetic info: our DNA (exact replica of which is harbored in nucleus of every cell in our body)\n\neach individual possesses 2 nearly identical copies of human genome (one from mom, one from dad)\n\n\nhuman chromosome structure: chromatin tightly packed in hierarchical coil structures. from bottom, 146 nucleotide pairs wrapped around histone (like a bead), and histones are coiled and supercoiled to form compact chromosome that fits within the nucleus of a cell\nwithin genome: ~20,000 genes (DNA segments accountable for protein synthesis)\n~1% of genome codes for proteins while remainder comprises regions controlling gene expression, regions within genes that don’t code for proteins, regions contributing to DNA structure, and “junk” regions of selfish DNA that have “learned” to self-replicate\ncentral dogma of molec bio maps out molec info flow from genome → expression of genes+subsequent production of proteins (building blocks of life)\ngenes expressed within cells by transcription (copies genes into single-stranded molecule mRNA) and translation (mRNA → amino acid protein sequence); 4-letter nucleotide code of DNA segment translated into 20-amino acid code of protein sequence; protein folds in 3d to form functional protein structure\ntranscription → splicing → translation\nsplicing: excised segments aka introns; kept regions aka exons make up protein-coding part of mRNA\n\neach mature mRNA assembled from ~7 exons\nvital in higher organisms b/c a single gene can yield multiple different proteins by assembling different exon combinations during splicing\n\n20,000 genes → 70,000 known standard splice forms+larger # rare/aberrant splice forms\nafter transcription, mRNA transported to cell’s protein-synthesizing machinery (ribosome) where translation occurs\n\nmRNA sequence decoded by codons (each corresponds to 1/20 amino acids)\n\namino acids linked together in a chain to form protein sequence → folds into functional, 3d protein structure\ngene regulation: intricate processes that dictate when/where/in what quantity genes are expressed within cell, ensuring timely production of the right proteins in the right amounts\n\ngene regulation takes place at various levels (structuring of chromatin, chem modifications, through action of specific proteins known as transcription factors) that recruit RNA polymerase and/control when/where/what amount gene will be expressed (requires open chromatin for transcription)\n\nTranscription factors (TFs): proteins in gene regulation that bind to distinct DNA sequences near/within genes (transcription factor binding sites) and influence recruitment of RNA polymerase, the enzyme tasked w/ mRNA synthesis\n\ntranscription factors modulate expression of target genes, guaranteeing appropriate gene expression in response to diverse cellular signals/environmental conditions\nTFs themselves modulated by TFs, forming complex gene regulatory pathways\n\nPromoters+enhancers: DNA regions that play a role in gene expression control\n\npromoters: located adjacent to start of a gene (upstream/to left of gene start, in chemical direction of DNA)\nenhancers: more distant regulatory elements situated within introns or between genes\nboth harbor several TF binding sites\nwith assistance of TFs, a gene’s promoter+enhancers form 3-d structures that recruit and regulate RNA polymerase responsible for mRNA synthesis\n\nChromatin structure: an amalgamation of DNA+proteins (histones) that constitute chromosomes\n\nto fit within each cell’s nucleus, DNA is wound around proteins known as histones\nhistones: tetramers (structures formed by assembling 4 copies of histone protein)\neach histone wraps around 146 nucleotide pairs of DNA, creating a rosary structure that subsequently folds into a higher order helical structure (chromatin)\nchromatin organization determines which DNA regions are accessible for gene expression\nfor gene expression to occur, chromatin must be unfolded\ntightly packed chromatin prevents gene expression\n\nHistone modifications: chemical modifications (e.g. acetylation/methylation) that can affect the histone beads+influence chromatin structure+gene accessibility\n\nmodifications can either promote or inhibit gene expression depending on type+location of modification\npart of the histone code (sort of epigenetic code) i.e. additional layer of code superimposed on the genetic code inscribed in the DNA\n\nDNA methylation: chem modification where methyl group added to DNA molec usually at specific cytosine bases\n\ncan influence gene expression by affecting binding of transcription factors or changing chromatin structure, making it more compact/less accessible for transcription\nalso part of epigenetic code\ngene regulation is a dynamic process specific to each cell type; diff cells exhibit unique gene expression profiles → perform specialized functions\nthrough precise control of gene expression cells can respond to enviro stimuli, sustain homeostasis+execute complex processes essential for life\n\nBidirectional flow of info: some exceptions to unidirectional flow of info (e.g. central dogma: DNA → RNA → protein):\n\nreverse transcription: RNA converted back to DNA; facilitated by reverse transcriptase+common in retroviruses such as HIV\nDNA can also be transcribed into RNA molec besides mRNA — tRNA, rRNA, and other types of non-coding RNA, adding another level of complexity to the flow of genetic information\nrole of epigenetics by DNA methylation and histone modification\n\n\nVariation in our DNA\n\nevery individual biologically shaped by interplay between DNA+enviro influences\nDNA variants account for heritability of all our traits\norigins of DNA variants\n\nprimary mechanism: mutations between genomes of 2 parents+germline genomes that both parents contribute to offspring’s genome\ndrive genetic variation+account for differences from other species\nmost new variants are benign\nsmaller fraction can be deleterious esp if they damage a functional region\neven smaller fraction could be beneficial\n\nSelection: deleterious variants/harmful genetic alterations render an organism less “fit” — tend to be statistically eliminated from population\n\nrare variants generally more likely to be harmful\n\nCoalescence+DNA sequence conservation: effects of selection are highly informative — 2 regions of similarity between genes eventually coalesce — eventually there is an ancestor mammalian individual that had 2 kids that both inherited precise same DNA piece, each leading to each gene today\n\nmutations that took place in important parts of gene tended to make individuals less fit\nmore conserved parts of DNA region more likely to be functionally imporant\n\nData generation: short DNA segments w/ a specific property of interest such as binding a certain TF or being part of the open accessible chromatin are isolated in an experiment+sequenced\n\nother technologies like MS and affinity-based proteomics can measure the levels of all proteins in a biological sample\nX-ray crystallography provides 3-d protein structures\n\nLinking variation to function: want to correlate genetic variants across individuals’ genomes w/ specific phenotypes (e.g. presence vs absence of a particular disease) aka GWAS\n\nidentify statistically significant associations of certain genome locations (which could be genes or regulatory regions) with the phenotypes under study\nwhen measured phenotype isn’t binary but a quantifiable entity, regression can be performed between genomic variation and phenotype, with identified genetic loci termed quantitative trait loci\nbesides macroscopic phenotypes (disease status, height, hair color), genetic variation can be associated w/ molecular phenotypes such as gene expression levels (leading to expression quantitative trait loci — eQTLs), protein abundance (results in protein quantitative trait loci — pQTLs) and virtually every other molecular measurement\nlikely to be surpassed by application of LLMs\n\nLanguage models in molec bio\n\nmodeling molec bio doesn’t need artificial general intelligence (AGI); i.e. doesn’t require high-level planning, agency, or goals; limited need for combinatorics+algorithmic reasoning\n\nrequires what LLMs are good at: learning stat properties of intricate, noisy sequential data to best predict such data from lossy representations\n\n\n\nPredicting gene structure\n\nprimary function of DNA: encode genes that are transcribed+translated into proteins\nspecific segments of each gene translated into proteins determined by splicing mechanisms (segments are annotated)\nmutations can disrupt precise boundaries of splicing (splice sites)\nrare mutations can significantly impact resulting protein fn+ produce a completely different protein sequence\n\naccount for 10% rare genetic diseases\n\nFundamental computational task: predicting splice sites+deducing gene structure; implications for diagnosing genetic diseases\n\naccuracy is not high enough\n\nSpliceAI: employs earlier techniques for language model (not transformer tech or LLM), where language is DNA sequences\n\ndeep residual CNN\ndilated convolutions to efficiently expand the window size it can handle\naccepts 10k nucleotide windows of human genome as input → predict exact locations of intron-exon boundaries (donor/acceptor sites) — exon-intron and intron-exon borders\nprecise-recall AUC: 0.98\naccurate enough to perform mutational analysis in silico — artificially laters any position of DNA and determines whether this change introduces or eliminates a splice site within 10k nucleotides of alteration\ncan be utilized to aid genetic diagnosis\nachieved high accuracy by learning biomolec properties of DNA sequence that guide splicing machinery to splice sites (previously less known)\nnew question: how to extract biomolec rules that SpliceAI learned+gain insight into underlying biomolec mechanisms?\n\nPredicting protein structure\n\nprotein sequences directly translated from spliced mRNA sequences according to genetic code, then fold into 3d structures\nwant to predict protein structure from protein sequence — difficult\nnew open-source database (AlphaFold2) that provides high-accuracy structural predictions for various organisms\n\nAlphaFold2 methodology:\n\ncombines CNN operating on protein sequences w/ pairwise co-evolution feature\nidentifies pairs of sequence positions that co-vary across related protein seuqneces in diff species to predict 2D contact maps across protein seuqnece\n\ncontact map: score for every pair of positions in sequence — likelihood of 2 positions being in close proximity in 3D\n\nbuilds on these algorithsms and introduces some new improvements:\n\nbased on transformer LLM architecture — can better capture long-range interactions between AA in protein seq\nnovel energy-based score (Amber energy) introduced to directly optimize 3d protein structure → allows for end-to-end differentiable approach during structure optimization step\nimproved utilization of coevolutionary features by incorporating multiple sequence alignment (MSA) data boosts model’s ability to identify conserved structural features across homologous protein sequences\nrefine: fine-tune predicted protein structures using second model trained on output of first model → more accurate and consistent predictions\n\nnote: ensemble?\n\n\n\n\nPredicting impact of protein variants\n\n4 million positions in genomes of any 2 individuals vary\n\n20k such variants located within protein-coding regions\nsmall fraction of genetic diversity is deleterious → contributes to genetic diseases\n\nclue in determining if a variant is benign: compare human genetics to genetics of close relatives\n\nproteins conserved by evolution are even more similar on average\n\nsearch for mutations that confer serious genetic disease should start from mutations not on this list\nuse list to observe patterns within protein seq and structures that tend to tolerate variants+patterns that tend not to tolerate variants\n\ncan gain ability to annotate variants in proteins as likely benign+likely pathogenic\n\nPrimateAI-3D: transformer that learns to distinguish between benign+pathogenic variants in human proteins\n\naccomplished by learning patterns of protein positions where primate variants tend to be present vs protein positions where they tend to be absent\nuses both protein seq data as well as protein 3D models that are either experimentally reconstructed or computationally predicted by tools like AlphaFold+HHpred\ncan be applied to diagnosis of rare disease+prioritize variants that are likely deleterious, filter out benign variants\nanother application: discovery of genes associated w/ complex diseases\n\nlook for variants likely deleterious according to PrimateAI-3D, then look for abundance of such variants within specific gene across cohort\ngenes with genetic “burden” (signal of playing role in the disease)\n\nused primate AI-3D+developed improved rare variant polygenic risk score models (PRS) to identify individuals at high diseases risk\n\n\nModeling gene regulation\n\nmolecular components: DNA chromatin structure, chemical alterations within histones that DNA wraps around, attachment of TFs to promoters+enhancers, establishment of 3D DNA structure involving promoters, enhancers, bound transcription factors, and recruitment of RNA polymerase\n\nData generation informative of gene regulation\n\nexamples of info obtained, always related to a human cell line or tissue type:\n\nidentify precise locations across entire genome that have open chromatin vs tightly packed chromatin\n\n2 relevant assays: DNAse-seq and ATAC-seq\n\npinpoint all locations in genome where a specific transcriptions factor is bound\nidentify all location in genome where a specific histone chem modification has occurred\ndetermining level of mRNA available for a given gene i.e. expression level of particular gene\n\nlanguage models → culminate in transformer-based Enformer tool\n\naccept DNA sequence near a gene as input, output cell type-specific expression level of this gene for any gene in the genome\ntrained on task: given a genome region of 100k nucleotides and a specific cell type, predict available types of experimental data for this region, including status of open/packed chromatin, present histone modifications, specific bound TFs, and level of gene expression\n\nlanguage model ideal (rather than masked language modeling) for supervised training — predict all tracks simultaneously from DNA seq\n\nincorporates attention mechanisms+collates info from distant regions to predict status of given location\n\nEnformer performs well in predicting gene exp from sequence alone\n\nhowever doesn’t achieve reduction of collecting necessary experimental data (highly correlated replicates of same experiment) yet\ncan predict changes in gene exp caused by mutations present in diff individuals as well as by mutations artificially introduced through CRISPR experiments\nlimitations: perform poorly in predicting effects of distal enhancers+correctly determine direction of effect of personal variants in gene expression\n\nOrca model: language model based on convolutional encoder-decoder architecture that predicts 3D genome structure from proximity data provided by Hi-C experiments\n\nhierarchical multi-level convolutional encoder, multilevel decoder, predicts DNA structure at 9 levels of resolution for input DNA seq that are as long as the longest human chromosome\n\n\nFoundation models\n\nlarge DL architectures (such as transformer-based GPT models by OpenAI) that encode a vast amount of knowledge\n\ncan be fine-tuned for specific tasks\n\n\nscGPT: foundation model designed for single-cell transcriptomics, chromatin accessibility, and protein abundance\n\ntrained on single-cell data from 10 million human cells\neach cell contains expression values for a fraction of ~20k human genes\nmodel learns embeddings of this large cell x gene matrix → provides insights into underlying cellular states+active bio pathways\nconcept of “next gene” is unclear in single-cell data\n\nsolution: train model to generate data based on a gene prompt (collection of known gene values)+cell prompt\nstarting from known genes, model predicts remaining genes along w/ conf values\nfor K iterations, divides those into K bins, and the top 1/K most confident genes are fixed as known genes for next iteration\n\nonce trained, fine-tune for numerous downstream tasks: batch correction, cell annotation (ground truth: annotated collections of diff cell types), perturbation prediction (predict cell state after a given set of genes are experimentally perturbed), multiomics (each layer, transcriptome, chromatin, proteome, treated as a different language)\n\nNucleotide transformer\n\nfoundational model that focuses on raw DNA sequences\nsequences tokenized into words of 6 characters each (k-mers of length 6)+trained using BERT methodlogy\ntraining dtaa: ref human genome, 3200 additional diverse human genomes, genomes of 850 other species\nnucleotide transformer applied to 18 downstream tasks that encompass many of previously discussed ones (promoter pred, splice site donor/acceptor pred, histone modifications, etc.)\npredictions made either through probing (embeddings at different layers used as features for simple classifiers — e.g. logistic regression+perceptrons) or light, computationally inexpensive fine-tuning\n\nLooking forward\n\nAGI not required (understanding molec bio/link to human health doesn’t need to be an AI task)\nasking AI to learn complex stat properties of existing biological systems\nexpect it to learn one-step causality relationships (mutation → malfunction)\n\nif gene is underexpressed, other genes in cascade in/decrease\ntriangulate between correlations across modalities such as DNA variation, protein abundance, and phenotype (Mendelian randomization) and large-scale perturbation experiments → LLM can model cellular states\ngenome ↔︎ phenotype\n\nsignificant gatekeeper: data\n\nUK Biobank Project (UKB)\n\nlarge-scale biobank, biomedical database+research resources containing comprehensive genetic/health info from 1/2 million UK participants\nlots of other large-scale data initiatives"
  },
  {
    "objectID": "posts/pytorch_tensor/index.html",
    "href": "posts/pytorch_tensor/index.html",
    "title": "PyTorch tutorial: tensor demo",
    "section": "",
    "text": "original tutorial link\ntensor: n-dimensional array; specialized data structure similar to arrays+matrices\n\nin PyTorch, used to encode inputs/outputs of a model+model parameters\nsimilar to ndarrays but run on GPUs or other accelerated hardware for computing\n\nTensor initialization — many ways:\n\ndirectly from data\ndata = [[1,2,],[3,4]]\nx_data = torch.tensor(data)\n#data type inferred\nfrom NumPy array\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array)\nfrom another tensor\n#argument tensor:\ndata = [[1,2,],[3,4]]\nx_data = torch.tensor(data)\n\n#new tensor:\nx_ones = torch.ones_like(x_data) # retains the properties of x_data\n\n#new tensor overriding properties:\nx_rand = torch.rand_like(x_data, dtype=torch.float)\n\nnew tensor retains properties (shape+datatype) of argument tensor unless explicitly overridden\n\nwith random or constant values:\nshape = (2, 3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nhere shape is a tuple of tensor dimensions; determines dimensionality of output tensor\n\n\nTensor attributes: describe shape, datatype, and device on which they are stored\ntensor_name.shape\ntensor_name.dtype\ntensor_name.device\nTensor operations: over 100 including T, index, slicing, mathematical operations, linalg, random sampling, etc.\n\nTensor API very similar to NumPy API\nNumpy-like index+slice:\ntensor = torch.ones(4, 4)\ntensor[:,1] = 0\nJoining tensors: concatenate sequence of tensors along a given dimension using torch.cat\nt1 = torch.cat([tensor, tensor, tensor], dim=1)\nmultiplying tensors element-wise: use * operator or .mul() method\ntensor.mul(tensor)\n#or\ntensor*tensor\nmatrix multiplication: use @ operator with transposed matrix or .matmul(&lt;transpose&gt;) method\ntensor.matmul(tensor.T)\n#or\ntensor @ tensor.T\nin-place operations: any operations w/ a _ suffix\n#ex:\nx.copy_(y)\nx.t_()\n#change x in place\n\ntensor.add_(5) \n#change tensor in place\n\nhowever can cause issues when computing derivatives b/c of immediate loss of history\n\n\nBridge with NumPy: tensors on CPU+NumPy arrays can share underlying memory locations; changing one will change other\n\ntensor to np array\nt = torch.ones(5)\nn = t.numpy()\n\nchange in tensor reflects in np array\nt.add_(1)\n#n will change if t changes\n\nnp array to tensor\nn = np.ones(5)\nt = torch.from_numpy(n)\n\nchanges in np array reflects in tensor\nnp.add(n, 1, out=n)\n#t will change if n changes"
  },
  {
    "objectID": "posts/pytorch_nn/index.html",
    "href": "posts/pytorch_nn/index.html",
    "title": "PyTorch tutorial: NN demo",
    "section": "",
    "text": "original tutorial link\nNeural Networks\n\ntorch.nn package\nnn depends on autograd to define models+differentiate them\nnn.Module contains:\n\nlayers\nmethod forward(input) that returns output\n\n\nEx. MNIST\n\n\n\nhttps://pytorch.org/tutorials/_images/mnist.png\n\n\n\nsimple feed-forward network: takes input, feeds through several layers one after the other, then finally gives output\n\nTypical training procedure for NN:\n\ndefine NN that has some learnable parameters (or weights)\niterate over dataset of inputs\nprocess input through network\ncompute loss\npropagate gradients back into network’s parameters\nupdate weights of network, typically w/ simple update rule: weight = weight - learning_rate * gradient\n\nDefine the network\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        # kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square, you can specify with a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\nprint(net)\n\njust define forward fn\n\nbackward fn (where gradients are computed) automatically defined by autograd\ncan use any Tensor operations in forward fn\n\nlearnable parameters of a model returned by net.parameters()\n\nparams = list(net.parameters())\n#net_name.parameters()\n\nlen(params) #10\n\nparams[0].size()) #conv1's .weight\nTrying a random 32x32 input:\ninput = torch.randn(1,1,32,32)\nout=net(input)\nRandom gradients to zero gradient buffers of all parameters and backprops (by default, gradients are accumulated in buffers (i.e, not overwritten) whenever .backward() is called)\nnet.zero_grad()\nout.backward(torch.randn(1,10))\n\ntorch.nn only supports mini batches\nentire torch.nn package only supports inputs that are a mini-batch of samples and not a single sample\n\nex. nn.Conv2d takes in 4D tensor of nSamples x nChannels x Height x Width\n\nsingle sample → input.unsqueeze(0) to add fake batch dimension\n\nClass recap\n\ntorch.Tensor - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\nnn.Module: neural network module. Convenient way of encapsulating parameters, w/ helpers for moving them to GPU/exporting/loading, etc.\nnn.Parameter: a kind of tensor automatically registered as a parameter when assigned as an attribute to a Module\nautograd.Function: implements forward and backward definitions of autograd operation. Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history\n\nLoss fn\ntakes (output, target) pair of inputs → compute value that estimates how far away output is from target\nlist of loss fns under nn package\nEx. a simple loss is nn.MSELoss: computes mean-squared error between output+target\noutput = net(input)\ntarget = torch.randn(10) #dummy target\ntarget = target.view(1,-1) #make same shape as output\ncriterion = nn.MSELoss()\n\nloss = criterion(output, target\n\nfollow loss in backward direction using .grad_fn attribute → see graph of computations like this:\n\ninput -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d\n      -&gt; flatten -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear\n      -&gt; MSELoss\n      -&gt; loss\n\ncall loss.backward → whole graph is differentiated wrt NN parameters+all Tensors in graph w/ requires_grad=True will have their .grad Tensor accumulated w/ gradient\n\n#follow a few steps backward:\n\n#mseloss\nprint(loss.grad_fn)  \n\n#linear\nprint(loss.grad_fn.next_functions[0][0])  \n\n#relu\nprint(loss.grad_fn.next_functions[0][0].next_functions[0][0])  \nBackprop\nbackpropagate error w/ loss.backward()\n**need to clear existing gradients or they will be accumulated to existing gradients\nEx. call loss.backward() and look at conv1’s bias gradients before+after backward\n#zero grad buffers of all params\nnet.zero_grad()\n\n#conv1.bias.grad before backward\nprint(net.conv1.bias.grad)\n#output: None\n\nloss.backward()\n\n#after backward\nprint(net.conv1.bias.grad)\n#output: tensor([-0.0024, -0.0051, -0.0055, -0.0025,  0.0090,  0.0032])\nUpdate the weights\nsimplest update rule — stochastic gradient descent (SGD)\nweight = weight - learning_rate*gradient\nimplementation:\nlearning_rate = 0.01\nfor f in net.parameters():\n    f.data.sub_(f.grad.data * learning_rate)\n\nwill want to use different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n\nuse torch.optim package\n\n\nimport torch.optim as optim\n\n# create your optimizer\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\n# in your training loop:\noptimizer.zero_grad()   # zero the gradient buffers\noutput = net(input)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step()    # Does the update\n**Note: gradient buffers have to be manually set to zero using optimizer.zero_grad()"
  },
  {
    "objectID": "posts/population_structure/index.html",
    "href": "posts/population_structure/index.html",
    "title": "Population structure",
    "section": "",
    "text": "#libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(devtools)\n\nLoading required package: usethis\n\nlibrary(glue)\nlibrary(qqman)\n\n\nFor example usage please run: vignette('qqman')\n\nCitation appreciated but not required:\nTurner, (2018). qqman: an R package for visualizing GWAS results using Q-Q and manhattan plots. Journal of Open Source Software, 3(25), 731, https://doi.org/10.21105/joss.00731.\n\nsource_gist(\"38431b74c6c0bf90c12f\")\n\nℹ Sourcing gist \"38431b74c6c0bf90c12f\"\nℹ SHA-1 hash of file is \"cbeca7fd9bf1602dee41c4f1880cc3a5e8992303\"\nif(!file.exists(glue::glue(\"~/Downloads/analysis_population_structure.tgz\"))){\n  system(glue::glue(\"wget -O ~/Downloads/analysis_population_structure.tgz https://uchicago.box.com/shared/static/zv1jyevq01mt130ishx25sgb1agdu8lj.tgz\"))\n  ## tar -xf file_name.tar.gz --directory /target/directory\n  system(glue::glue(\"tar xvf ~/Downloads/analysis_population_structure.tgz --directory ~/Downloads/\")) \n}\n\nwork.dir =\"~/Downloads/analysis_population_structure/\"\nTest HWE w/ population structure\nPopulation composition\npopinfo = read_tsv(paste0(work.dir,\"relationships_w_pops_051208.txt\"))\n\nRows: 1301 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): FID, IID, dad, mom, population\ndbl (2): sex, pheno\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npopinfo %&gt;% count(population)\n\n# A tibble: 11 × 2\n   population     n\n   &lt;chr&gt;      &lt;int&gt;\n 1 ASW           90\n 2 CEU          180\n 3 CHB           90\n 4 CHD          100\n 5 GIH          100\n 6 JPT           91\n 7 LWK          100\n 8 MEX           90\n 9 MKK          180\n10 TSI          100\n11 YRI          180\n\nsamdata = read_tsv(paste0(work.dir,\"phase3_corrected.psam\"),guess_max = 2500) \n\nRows: 2504 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): #IID, PAT, MAT, SuperPop, Population\ndbl (1): SEX\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nCombine into superpopulation\nsuperpop = samdata %&gt;% select(SuperPop,Population) %&gt;% unique()\nsuperpop = rbind(superpop, data.frame(SuperPop=c(\"EAS\",\"HIS\",\"AFR\"),Population=c(\"CHD\",\"MEX\",\"MKK\")))\nEffect of population structure in HWE\nif(!file.exists(glue::glue(\"{work.dir}output/allhwe.hwe\")))\n  system(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --hardy --out {work.dir}output/allhwe\"))\nCalculate HWE in this mixed population:\nallhwe = read.table(glue::glue(\"{work.dir}output/allhwe.hwe\"),header=TRUE,as.is=TRUE)\nhist(allhwe$P)\n\n\n\nqqunif(allhwe$P,main='HWE HapMap3 All Pop')\n\nWarning in qqunif(allhwe$P, main = \"HWE HapMap3 All Pop\"): thresholding p to\n1e-30\nWhat if we calculate with single population?\npop = \"CHB\"\npop = \"CEU\"\npop = \"YRI\"\nfor(pop in c(\"CHB\",\"CEU\",\"YRI\"))\n{\n  ## what if we calculate with single population?\n  popinfo %&gt;% filter(population==pop) %&gt;%\n    write_tsv(path=glue::glue(\"{work.dir}{pop}.fam\") )\n  if(!file.exists(glue::glue(\"{work.dir}output/hwe-{pop}.hwe\")))\n    system(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --hardy --keep {work.dir}{pop}.fam --out {work.dir}output/hwe-{pop}\"))\n  pophwe = read.table(glue::glue(\"{work.dir}output/hwe-{pop}.hwe\"),header=TRUE,as.is=TRUE)\n  hist(pophwe$P,main=glue::glue(\"HWE {pop} and founders only\"))\n  qqunif(pophwe$P,main=glue::glue(\"HWE {pop} and founders only\"))\n}\n\nWarning: The `path` argument of `write_tsv()` is deprecated as of readr 1.4.0.\nℹ Please use the `file` argument instead.\nEffect of population stratification on GWAS\nGWAS on a growth phenotype in HapMap samples\nigrowth = read_tsv(\"https://raw.githubusercontent.com/hakyimlab/igrowth/master/rawgrowth.txt\")\n\nRows: 3726 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): IID, pop, serum\ndbl (4): sex, experim, meas.by, growth\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# add FID to igrowth file\nigrowth = popinfo %&gt;% select(-pheno) %&gt;% inner_join(igrowth %&gt;% select(IID,growth), by=c(\"IID\"=\"IID\"))\nwrite_tsv(igrowth,path=glue::glue(\"{work.dir}igrowth.pheno\"))\nigrowth %&gt;% ggplot(aes(population,growth)) + geom_violin(aes(fill=population)) + geom_boxplot(width=0.2,col='black',fill='gray',alpha=.8) + theme_bw(base_size = 15)\n\nWarning: Removed 130 rows containing non-finite values (`stat_ydensity()`).\n\n\nWarning: Removed 130 rows containing non-finite values (`stat_boxplot()`).\nsummary( lm(growth~population,data=igrowth) )\n\n\nCall:\nlm(formula = growth ~ population, data = igrowth)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-58821 -18093  -2242  15896  98760 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    73080.8      938.2  77.894  &lt; 2e-16 ***\npopulationCEU  -2190.1     1175.4  -1.863   0.0625 .  \npopulationCHB   9053.1     2043.9   4.429 9.73e-06 ***\npopulationJPT   3476.8     2034.8   1.709   0.0876 .  \npopulationYRI  -7985.2     1137.2  -7.022 2.61e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24160 on 3591 degrees of freedom\n  (130 observations deleted due to missingness)\nMultiple R-squared:  0.0345,    Adjusted R-squared:  0.03342 \nF-statistic: 32.08 on 4 and 3591 DF,  p-value: &lt; 2.2e-16\nif(!file.exists(glue::glue(\"{work.dir}output/igrowth.assoc.linear\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --linear --pheno {work.dir}igrowth.pheno --pheno-name growth --maf 0.05 --out {work.dir}output/igrowth\"))\nigrowth.assoc = read.table(glue::glue(\"{work.dir}output/igrowth.assoc.linear\"),header=TRUE, as.is=TRUE)\nhist(igrowth.assoc$P)\nqqunif(igrowth.assoc$P)\nmanhattan(igrowth.assoc, chr=\"CHR\", bp=\"BP\", snp=\"SNP\", p=\"P\" )"
  },
  {
    "objectID": "posts/population_structure/index.html#calculate-principal-components-using-plink",
    "href": "posts/population_structure/index.html#calculate-principal-components-using-plink",
    "title": "Population structure",
    "section": "6. Calculate principal components using plink",
    "text": "6. Calculate principal components using plink\n\n# generate PCs using plink\nif(!file.exists(glue::glue(\"{work.dir}output/pca.eigenvec\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --pca --out {work.dir}output/pca\"))\n\n\n# read plink calculated PCs\npcplink = read.table(glue::glue(\"{work.dir}output/pca.eigenvec\"),header=FALSE, as.is=TRUE)\nnames(pcplink) = c(\"FID\",\"IID\",paste0(\"PC\", c(1:(ncol(pcplink)-2))) )\npcplink = popinfo %&gt;% left_join(superpop,by=c(\"population\"=\"Population\")) %&gt;% inner_join(pcplink, by=c(\"FID\"=\"FID\", \"IID\"=\"IID\"))\n\n# plot PC1 vs PC2\npcplink %&gt;% ggplot(aes(PC1,PC2,col=population,shape=SuperPop)) + geom_point(size=3,alpha=.7) + theme_bw(base_size = 15)\n\n\n\n\nRunning igrowth GWAS using PCs\n\nif(!file.exists(glue::glue(\"{work.dir}output/igrowth-adjPC.assoc.linear\")))\nsystem(glue::glue(\"~/bin/plink --bfile   {work.dir}hapmapch22 --linear --pheno {work.dir}igrowth.pheno --pheno-name growth --covar {work.dir}output/pca.eigenvec --covar-number 1-4 --hide-covar --maf 0.05 --out {work.dir}output/igrowth-adjPC\"))\nigrowth.adjusted.assoc = read.table(glue::glue(\"{work.dir}output/igrowth-adjPC.assoc.linear\"),header=TRUE,as.is=TRUE)\n\n#indadd = igrowth.adjusted.assoc$TEST==\"ADD\"\ntitulo = \"igrowh association adjusted for PCs\"\n\nhist(igrowth.adjusted.assoc$P,main=titulo)\n\n\n\n\n\nqqunif(igrowth.adjusted.assoc$P, main=titulo)"
  },
  {
    "objectID": "posts/w1_thesis_updates/index.html",
    "href": "posts/w1_thesis_updates/index.html",
    "title": "W1 Thesis updates",
    "section": "",
    "text": "single cell analysis\n\nmatrix: rows - cells; columns: genes; entries: count\n1k1k - 1000 cells/individual; 1000 individuals → 1 million cells\nsingle cell vs bulk (bulk: less noisy)\n\nbefore: generate predictions of bulk data\nnow: generate predictions of single cell data (harder since noisier)\n\nusing ranks — don’t need to remove data (2 branches of analysis — one with and one without outliers)\n\ncomparing portability of enformer vs predixcan between populations (2 methods)\n\nenformer DL model - personalized predictions\nindividuals annotated by ancestral origin, bulk data\nrun personal genome through enformer → gene expression; split between ancestral origin\n\nissue: disparities in predictions based on training\n\nportability:\n\\[\n  P_g = \\frac{|r_Y|+\\epsilon}{|r_C|+\\epsilon}\n  \\]\n\ncorrelation between observed and predicted; split based on populations\nerror = .1\n\npredixcan: population level data - performs better currently for both EUR and AFR predictions\n\nissue: portability\nloss in performance when you from european to african individuals is worse in predixcan compared to enformer\n\n\nmetaboXcan\n\nuse genotype to predict metabolite levels → phenotype\n\ngenotype → gene prediction → phenotype\nmetaboXcan: “shortcut” for total mRNA → metabolite level\n\n\npredict transcriptome factor binding\n\ngoal: to do that for individuals w/ phenotypes w/ or w/o disease (potentially compare binding in regions)\nTFPred — method on top of enformer (given genome sequence, apply TFPred scores to quantify androgen receptor bindings for regions)\ntrying to find what genes are regulated by androgen receptor bindings\ncorrelating binding affinity for androgen receptor with expression levels of gene (more or less of mRNA)\nsetup (each row: individual):\n\n\n\nbinding score\ngen exp\n\n\n\n\n…\n…\n\n\n\ntesting “cis effect” - testing transcriptome factor binding on the same chromosome\nwant to test “trans effects” / associations\n\nto make sure not due to proximity\n\n#genes in cis\npairwise associations in trans\ntrans effects — be careful about mappability (artifact from wrong alignment)"
  },
  {
    "objectID": "posts/functional_annotation_gwas/index.html",
    "href": "posts/functional_annotation_gwas/index.html",
    "title": "Functional annotation of GWAS loci using transcriptome data",
    "section": "",
    "text": "Lecture notes from quant genomics training (day 2)\nPredixCan (TWAS)\nGTEx consortium\n\ncollect 1000 organ donors and tissue samples from different sites of the body; all RNA-sequenced (have genotype data)\n\neQTL\n\n→ that data used to calculate eQTL (expression quantitative trait loci) — SNPs associated/correlated w/ expression levels of a gene\n\\(T = \\gamma \\cdot X+\\epsilon\\)\n\nX = genotype; gamma = effect size being estimated\n\n\nwe have genotype data to train predictors\n\ndifferent layers of genotype matrices make up transcriptome\nfit linear model to predict weight that gives us best representation of genetic component of expression\ntake genotype matrix → transform/impute transcriptome\n\ntake genotype data → predict transcriptome\n\nHow well are we predicting?\n\ncompare predicted and observed expression\npretty high correlation (promising)\n\nblack curve: measure of heritability of the expression level of that gene\n\nAdvantages of gene level association\n\nreduced multiple testing burden (from a million to around 20k)\nfunction of genes are much better annotated than SNPs\nvalidation in other model systems is possible (like in mice or zebrafish)\nreverse causality issues is less of an issue (germline DNA doesn’t change w/ disease status)\nprovides direction of effects; i.e. whether up or down regulation of a gene increases risk of a disease\ncandidate causal gene is a good target for drug development\n\nS-PrediXcan (summary-PrediXcan)\nregular PrediXcan: take genotype matrix (hard to get access to) → predict transcriptome → run association for every gene → get gene-level results\n\nhard to do since it’s hard to get the data in genotype matrix\n\n\nsolution: S-prediXcan — take SNP-level results and get gene-level results\n\ncan derive formula that gives you the gene level results from SNP-level results\n\nLimitations of TWAS methods\n\n\nLD contamination\n\nin TWAS, taking a bunch of eQTLs and predict mRNA to some degree (correlating that to disease level)\neQTL is nearby a SNP that is causing the disease — so not actually from eQTL; just from proximity (artifact of LD)\nsolutions:\n\ncan use fine-mapped predictors (improve quality of list of genes associated with disease)\npost-filtering w/ colocalization methods\n\n\nCo-regulation: cannot figure out which gene is the closer one\n\nmight find 2 different genes that are associated with a disease but one of them is not actually (just a confounder)\nmore difficult to tackle\n\n\nTake home message\n\ngene-level association methods (TWAS methods) have many advantages but can lead to false positives\nsignificant genes are excellent hypothesis generators that need to be confirmed with independent lines of evidence (smaller list of genes → less hypothesis tests to run)\n\nColocalization\n\ngiven a GWAS study and eQTL study, can try to find which one is the causal gene\nattempts to determine whether causal variants altering gene expression levels are same as causal variants that alter disease\nkey: fine-mapping to find causal variants\n\ngiven a statistical method, tries to find causal variants (i.e. what’s the probability that this SNP is the causal variant)\n\nex. (note: normally we don’t know ground truth) true effects\n\n\nsimulate phenotype based on this model and run a GWAS\nbelow: GWAS p-values\n\nhigher up on the plot —&gt; more significant correlation\n\nnot all points at the bottom\n\n\nfine-mapping\n\n\nPIP (posterior inclusion probability) - probability that this given variant is the causal variant using SuSi\n\n\nColocalization: are causal variants = ?\n\ntries to tell you whether causal SNP for one thing is same as causal signal in something else\nex. what’s mediating trait in FTO muscle and BMI → if we find something, can manipulate levels in one thing to see if it does anything to the other thing\neven if location is the same the SNP could be different\nnot colocalized if signal seem to be in the same region but if you do fine mapping that the causal variant for one thing is distinct from the causal variant for the other thing\n\nLimits of colocalization\n\nfine-mapping can be sensitive to LD reference\n\ncorrelation matrix that we use between SNPs\n\n\n\nOther TWAS methods\n\nTWAS/FUSION\nUTMOST\ncTWAS\n\nuses both fine mapping and association to calculate probability of gene being causal accounting for LD\n\n\nColocalization Methods\n\nCOLOC\nENLOC\nfastENLOC\neCAVIAR\n\nMendelian randomization\nRandomized trial vs Mendelian randomization\n\n\nMendelian randomization: uses genetic variant as instruments to try to find causal exposures\nrandomized trial\n\nex. in biomedical studies if you want to know the effect of a drug on the severity of a disease, need to run randomized trial (control+treatment) in order to determine causality\n\nMendelian\n\nrandomization occurs by genetic variant (risk allele present or absent) aka nature’s randomized trial\n\n\n\n\nlevel of evidence for causation is much higher in Mendelian randomization\n\nMendelian randomization question\n\ngoal: test association between a modifiable exposure+disease (ex. smoking, HDL cholesterol levels, etc.)\nproblem: if you just look at exposure and disease there could be confounders\nsolution: IV (SNPs)\n\n3 IV assumptions:\n\nindependence assumption between SNPs and confounders\nrelevance assumption between SNPs and exposure (reliable association)\nexclusion restriction assumption between SNPs and disease (no direct effect)\n\n\nTWAS can be thought of as Mendelian randomization\n\n“exposure” — baseline expression of gene\nwant to see if up or down regulation of gene is causing a disease\nif we use SNPs to predict expression level of gene as instrument, can investigate any associations\n\nSMR (summary data-based Mendelian randomization)\n\nsame as previous way but with eQTL as IV\n\nmultiple eQTL and multiple genes\n\nNew methods\n\nMetaboXcan: predict metabolite levels\n\n\n\nadded link between mRNA levels and metabolites (don’t just want list of genes and list of metabolites)\n\nDeep learning-based predictors\n\n\n\n\nTFXcan\nsingle cell PrediXcan\nEpigenomeXcan"
  },
  {
    "objectID": "posts/enformer_summary/index.html",
    "href": "posts/enformer_summary/index.html",
    "title": "Enformer paper summary",
    "section": "",
    "text": "paper link\nobjective: improve gene expression prediction accuracy from DNA sequences using a type of DL architecture (Enformer) that can include info from long-range interactions in genome → higher accuracy for variant effect predictions on gene expression, learned to predict enhancer-promoter interactions directly from DNA sequence → future steps: more effective fine-mapping of human disease associations\nMain\nPopulation-based association studies+models predicting gene expression+chromatin states from DNA sequences:\n\nthe former limited to common variants; hard to disentangle causality from assoc due to LD (linkage disequilibrium)\nalso hard to test all variants of interest in different relevant contexts in experimental validation\nsequence-based computational models have limited accuracy\n\nCurrently: deep CNNs to predict gene expr from DNA seqs for human+mouse genomes\n\ndrawback: can only consider seq elements &lt; 20 kb away from TSS (transcription start site) b/c locality of convolutions limits info flow in net between far apart elements\n\nmany regulatory elements can infleunce gene expr from far away so need to increase info flow between distal elements\n\n\nResults\nSolution: Enformer — a new model architecture to predict gene expr+chromatin states in humans+mice from DNA seqs\n\ntransformers: a class of DL models for NLP; recently applied to model short DNA seqs\n\nconsist of attention layers that transform each position in input sequence by computing weighted sum across representations of all other positions in sequence\nattention weight between any 2 positions depends on embeddings of current representation vectors+distance between\nmodel can refine prediction at a TSS by getting info from relevant regions (e.g. relevant enhancers)\nbetter info flow between distal elements since each position directly attends to all other positions in sequence\n\nvs convolutional layers — need many many layers to get to far away elements due to their local receptive field\ntransformer layers → increase receptive field → reach distal regulatory elements up to 100 kb away\n\nexpands # relevant enhancers seen by model from 47% to 84% (&lt;100 kb)\n\nEnformer better at predicting RNA expr at TsS of human protein-coding genes, consistent across all 4 types of genome wide tracks (CAGE — transcriptional activity, histone modifications, TF binding, DNA accessibility)\n\nesp CAGE — prob b/c tissue-specific gene expr depends a lot on distal elements\n\ngreater predictive accuracy than ExPecto (model trained to predict gene expr levels measured by RNA seq) for across-genes+across-tissues\n\nDemonstrating benefit of attention layers\n\n(compared to dilated convolutions in previous model)\nreplace attention layers w/ dilated convolutions+tuned learning rate for optimal performance → attention layers outperformed dilated convolutions across all model sizes, # layers, # training data pts\nobserved performance drop when restricting receptive field of Enformer to that of the previous model by replacing global attention layers w/ local\nincrease # params → improve model performance (consistent w/ recent advances in NLP)\nuse custom relative positional basis fns in transformer layers (distinguish between proximal and distal regulatory elements more easily)\n\nnoticeable performance improvement compared to typically used relative basis fns+absolute positional encodings in NLP lit\n\n\nEnformer attends to cell-type-specific enhancers\n\nwhat seq elements Enformer uses to make predictions? → compute 2 diff gene expr contribution scores:\n\ninput gradients (gradient \\(\\times\\) input)\nattention weights\n\nfor several genes w/ CRISPRi-validated enhancers\n\ncontribution scores — highlight most predictive input seq for particular gene’s expression\n\ninput gradients are tissue- or cell-type-specific (b/c computed wrt particular output CAGE sample)\nvs attention weights — internal to model+shared among all tissue+cell-type predictions\nobserved that contribution scores of several genes correlated w/ histone H3 acetylated at K27 (H3K27ac) and highlighted not just local promoter regions but also distal enhancers &gt;20 kb away\n\nsuggests that Enformer is looking at biologically relevant regions when making predictions; gene expr contribution scores could be used to prioritize relevant enhancers\n\nunsolved problem: how to link candidate enhancers id’ed vai biochemical annotations to target genes\n\nissue: computational models have historically low accuracy b/c of noisy labels+class imbalance\n\nevaluate ability of contribution scores to pinpoint relevant enhancers for a specific gene: compare several contribution scores across all tested enhancer-gene pairs in 2 large-scale CRISPRi studies performed on K562 cell line\nCRISPRi used to suppress activity of &gt; 10,000 candidate enhancers+measure effect on gene expr\ncontribution scores prioritize validated en-gene pairs w/ higher accuracy despite only using DNA seq as input — never trained to explicity locate enhancers\n\ncan be used for arbitrary seq variations lacking experimental data\n\nmodel uses diff enhancer seqs in diff cell types as expected → can prioritize candidate enhancers in cell types used for model training\n\nhas model learned about insulator elements (separate 2 topologically associating domains, i.e. TADs, minimize enhancer-promoter crosstalk between the two)\n\nused attention matrices (more efficient than input gradients to compute b/c of many output targets) of seqs centered at TAD boundaries+compared to attention from seqs w/ no particular alignment\nEnformer paid more attention to TAD boundaries than random positions; less attention to regions on opposite side of boundary — consistent w/ reduced inter-TAD interactions in biology\nsuggest that model has not only learned about role of tissue-specific enhancers+promoters, but also about insulator elements (and their role in inhibiting info flow between genomic compartments)\n\nEnformer improves variant effect prediction on eQTL data\n\ncentral goal: predict influence of genetic variants on cell-type-specific gene expr\n\n→ inform fine-mapping of many noncoding assoc w/ phenotypes of interest from GWAS\n\ncomputational models that predict regulatory activity from DNA seqs can process distinct alleles+compare preds to score genetic variants\nsuccessful model can produce eQTL results w/o having to measure lots of individual gene expr profiles\nchallenge: LD in profiled pop → transfers causal eQTL’s effect to nearby co-occurring variants’ measurements\nSLDP (signed linkage disequilibrium profile) regression: measures genome-wide statistical concordance between signed variant annotations (e.g. model preds) and GWAS summary stats (e.g. GTEx eQTLs) while accounting for LD\nEnformer preds for noncoding-variant activity seem to improve for samples w/ similar cell-type composition mostly\n\nhow to assess use of preds for identifying causal variants → classification task per tissue → discriminate likely causal variants from likely spurious eQTLs\n\neach variant represented by its pred diff vector (reference-alternative allele, summed across seq) for all 5313 human datasets\ntrained random forest classifiers\npreds enabled more accurate classifier for most GTEx tissues\n\nEnformer improves MPRA mutation effect prediction\n\neval Enformer’s performance on second, independent variant effect pred tas w/ dataset where MPRAs (massively parallel reporter assays) directly measured fnal effect of genetic variatns through sutration mutagenesis of several enhancers+promoters in variety of cell types\nfor each variant, eval effect as pred diff between ref+alt allele → retrieve 5313 features\ncompare 2 approaches:\n\ntrain lasso reg model on provided training set for each gene\npreselect subset of features corresponding to cell-type-matched/agnostic preds of changes in CAGE+DNase → generate summary stat of features\n\n\nLasso regression w/ Enformer preds as features had best average correlation across all loci\nFuture steps\n\nsystematically apply Enformer to fine-map existing GWAS studies\nprioritize rare or de novo variants for rare disorders\nimpute regulatory activity across species to study cis-regulatory evolution\n\nMethods\nModel architecture\ndiagram of typical Transformer architecture:\n\n\n\nhttps://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png\n\n\nEnformer architecture has 3 parts\n\n\n\nhttps://www.biorxiv.org/content/biorxiv/early/2021/04/08/2021.04.07.438649/F5.medium.gif\n\n\n\n7 convolutional blacks w/ pooling\n11 transformer blocks\na cropping layer followed by final pointwise convolutions branching into 2 organism-specific network heads\n\ninput: one-hot-encoded DNA seq\n\nA = \\(\\left[1,0,0,0\\right]\\)\nC = \\(\\left[0,1,0,0\\right]\\)\nG = \\(\\left[0,0,1,0\\right]\\)\nT = \\(\\left[0,0,0,1\\right]\\)\nN = \\(\\left[0,0,0,0\\right]\\)\n\nof length 196,608 bp → predicts 5313 genomic tracks for human genome, 1643 tracks for mouse genome (each of length 896 corresponding to 114,688 bp aggregated into 128-bp bins)\n\nconvolutional blocks w/ pooling first reduce the spatial dimension from 196,608 bp to 1526 so each seq position vector represents 128 bp\ntransformer blocks capture long-range interactions across seq\ncropping layer trims 320 positions on each\nto avoid computing loss on far ends b/c these regions are disadvantage (observe regulatory elements only on one side, toward seq center)\n2 output heads predict organism-specific tracks\n\nchanges: use transformer blocks instead of dilated convolutions, attention pooling instead of max pooling, twice as many channels, 1.5x longer input seq\nattention pooling: summarizes contiguous chunk of input sequence \\(\\mathbf{x}_{k:k+L_p}^{full}=\\mathbf{x} \\in R^{L_p\\times C}\\) across \\(L_p\\) positions for each of the \\(C\\) chanels\n\noutput value \\(\\mathbf{h} \\in \\mathbf{\\mathit{R}}^C\\):\n\n\\[\nh_j = \\frac{\\sum_i exp(x_i \\cdot w_j)x_{ij}}{\\sum_i exp(x_i\\cdot w_j)}\n\\]\nwhere i indexes seq position in pooling window, which is weighted by exponentiated dot product \\(\\mathbf{x}_i\\cdot \\mathbf{w}_j\\) and \\(\\mathbf{\\mathit{w}} \\in R^{C\\times K}\\) is a matrix of learned weights\n\napply attention pooling to contiguous chunks of original input seq using window size \\(L_p = 2\\) and stride of 2\n\ninitialize \\(\\mathbf{w}\\) to \\(2\\times \\mathbf{1}\\)\n\n\\(\\mathbf{1}\\) = identity matrix\nprioritizes larger value → similar operation to max pooling\nslightly better performance than random initialization or initialization w/ zeros (average pooling)\n\nMHA (multi-head attention) layers used to capture long-range interactions across sequence\n\neach head has separate weights for queries, keys, values that transform input seq\n\nseparate set of weights \\(\\mathbf{w}^q \\in \\mathbf{R}^{C\\times K}\\), \\(\\mathbf{w}^k \\in \\mathbf{R}^{C\\times K}\\), \\(\\mathbf{w}^v \\in \\mathbf{R}^{C\\times K}\\)\n→ transform input sequence \\(\\mathbf{x}\\in \\mathbf{R}^{L\\times C}\\) into:\n\nqueries \\(\\mathbf{q}_i=\\mathbf{x}_i\\mathbf{w}^q\\)\nkeys \\(\\mathbf{k}_j=\\mathbf{x}_j\\mathbf{w}^k\\)\nvalues \\(\\mathbf{v}_j=\\mathbf{x}_j\\mathbf{w}^v\\)\n\n\nqueries = current info at each position\nkeys = info each position looks for to attend to\ndot product between queries+keys plus relative positional encodings \\(\\mathbf{R}_{ij}\\) → attention matrix\n\ncomputed as \\(\\mathbf{a}_{ij} = softmax(\\mathbf{q}_i\\mathbf{k}_j^T/\\sqrt{K}+\\mathbf{R}_{ij})\\)\nentry \\(a_{ij}\\) = amount of weight that query at position i puts on key at position j\n\nvalues = info propagated to positions attending to a specific position\neach single attention head computes its output as weighted sum across all input positions: \\(\\mathbf{av}\\)\n\nlets query position use info across whole seq\nmultiple heads compute w/ independent parameters\n\noutputs from each head concatenated to form final layer output followed by linear layer to combine\n\nEnhancer prioritization:\n\nGradient \\(\\times\\) input: compute abs value of gradient of CAGE targets at TSS w. regard to input reference seq nucleotide\n\nequivalent to computing gradient \\(\\times\\) input attirbutions (since one-hot-encoding)\nCAGE at TSS = summing abs grad values from 3 adjacent bins (including bin overlapping TSS, one flanking bin on each side)\nenhancer-gene scare obtained by summing abs grad\\(\\times\\)input scores in 2-kb window centered at enhancer\n\nattention: averaged transformer attention matrices across all heads+layers → extract row corresponding to query index positioned at TSS\n\nkeys correspond to diff spatial positions\nattention values = how much model attended to these positions when making preds for TSS\nen-gene score — sum attention scores in 2-kb window centered at enhancer\n\nISM: in silico mutagenesis en-gene score\n\ncomputed by comparing K562 CAGE preds at TSS from ref seq with preds from modif seq where 2-kb enhancer seq was replaced by a random sequence: \\(|f(modified)-f(reference)|\\)\n\n\nother details for methodology on model training+eval can be found in paper*"
  },
  {
    "objectID": "posts/w3_thesis_updates/index.html",
    "href": "posts/w3_thesis_updates/index.html",
    "title": "W3 thesis updates",
    "section": "",
    "text": "PRS-cs, Prsice, and Lassosum\n\n\n\n\nSNP\n\\(\\beta\\)\nse\n\n\n\n\nChr 1\n\n\n\n\n\n\n\nclumping: looks for loci where you have significant SNPs; keep top SNPs\nthresholding: do that but still want p-value to be there (choose what threshold gives you best preds)\nLassosum: combines Lasso reg technique w/ sum stats\n\nModel fine-tuning on Aracena et al dataset\n\nwhole genome sequencing for each individual; for each sample measure RNA seq, look at histone marks+DNA methylation\naim: use this as a fine-tuning dataset\n\nex. can start “foundation” part of the model after in the middle → add fine-tuning head using Aracena et al for training\n\n\nTFXcan\ngenome-wide scale on multiple genes on multiple chromosomes and effects on other chromosomes → trans effect\n\ny: target gene; x: trans-acting promoter\nwant to investigate trans biology; want to remove cis effects b/c they are very large\n\nMulti-task prediction model\n\npredicting expression in 5 different cell types\ncould try shrinkage\nhighly expressed vs lowly expressed clusters"
  },
  {
    "objectID": "posts/conduct_gwas/index.html",
    "href": "posts/conduct_gwas/index.html",
    "title": "Conducting GWAS studies summary",
    "section": "",
    "text": "paper link\ndemo github link\n\n\nGlossary\n\n\nClumping: This is a procedure in which only the most significant SNP (i.e., lowest p value) in each LD block is identified and selected for further analyses. This reduces the correlation between the remaining SNPs, while retaining SNPs with the strongest statistical evidence.\nCo‐heritability: This is a measure of the genetic relationship between disorders. The SNP‐based co‐heritability is the proportion of covariance between disorder pairs (e.g., schizophrenia and bipolar disorder) that is explained by SNPs.\nGene: This is a sequence of nucleotides in the DNA that codes for a molecule (e.g., a protein)\nHeterozygosity: This is the carrying of two different alleles of a specific SNP. The heterozygosity rate of an individual is the proportion of heterozygous genotypes. High levels of heterozygosity within an individual might be an indication of low sample quality whereas low levels of heterozygosity may be due to inbreeding.\nIndividual‐level missingness: This is the number of SNPs that is missing for a specific individual. High levels of missingness can be an indication of poor DNA quality or technical problems.\nLinkage disequilibrium (LD): This is a measure of non‐random association between alleles at different loci at the same chromosome in a given population. SNPs are in LD when the frequency of association of their alleles is higher than expected under random assortment. LD concerns patterns of correlations between SNPs.\nMinor allele frequency (MAF): This is the frequency of the least often occurring allele at a specific location. Most studies are underpowered to detect associations with SNPs with a low MAF and therefore exclude these SNPs.\nPopulation stratification: This is the presence of multiple subpopulations (e.g., individuals with different ethnic background) in a study. Because allele frequencies can differ between subpopulations, population stratification can lead to false positive associations and/or mask true associations. An excellent example of this is the chopstick gene, where a SNP, due to population stratification, accounted for nearly half of the variance in the capacity to eat with chopsticks (Hamer & Sirota, 2000).\nPruning: This is a method to select a subset of markers that are in approximate linkage equilibrium. In PLINK, this method uses the strength of LD between SNPs within a specific window (region) of the chromosome and selects only SNPs that are approximately uncorrelated, based on a user‐specified threshold of LD. In contrast to clumping, pruning does not take the p value of a SNP into account.\nRelatedness: This indicates how strongly a pair of individuals is genetically related. A conventional GWAS assumes that all subjects are unrelated (i.e., no pair of individuals is more closely related than second‐degree relatives). Without appropriate correction, the inclusion of relatives could lead to biased estimations of standard errors of SNP effect sizes. Note that specific tools for analysing family data have been developed.\nSex discrepancy: This is the difference between the assigned sex and the sex determined based on the genotype. A discrepancy likely points to sample mix‐ups in the lab. Note, this test can only be conducted when SNPs on the sex chromosomes (X and Y) have been assessed.\nSingle nucleotide polymorphism (SNP): This is a variation in a single nucleotide (i.e., A, C, G, or T) that occurs at a specific position in the genome. A SNP usually exists as two different forms (e.g., A vs. T). These different forms are called alleles. A SNP with two alleles has three different genotypes (e.g., AA, AT, and TT).\nSNP‐heritability: This is the fraction of phenotypic variance of a trait explained by all SNPs in the analysis.\nSNP‐level missingness: This is the number of individuals in the sample for whom information on a specific SNP is missing. SNPs with a high level of missingness can potentially lead to bias.\nSummary statistics: These are the results obtained after conducting a GWAS, including information on chromosome number, position of the SNP, SNP(rs)‐identifier, MAF, effect size (odds ratio/beta), standard error, and p-value. Summary statistics of GWAS are often freely accessible or shared between researchers.\nThe Hardy–Weinberg (dis)equilibrium (HWE) law: This concerns the relation between the allele and genotype frequencies. It assumes an indefinitely large population, with no selection, mutation, or migration. The law states that the genotype and the allele frequencies are constant over generations. Violation of the HWE law indicates that genotype frequencies are significantly different from expectations (e.g., if the frequency of allele A = 0.20 and the frequency of allele T = 0.80; the expected frequency of genotype AT is 2 * 0.2 * 0.8 = 0.32) and the observed frequency should not be significantly different. In GWAS, it is generally assumed that deviations from HWE are the result of genotyping errors. The HWE thresholds in cases are often less stringent than those in controls, as the violation of the HWE law in cases can be indicative of true genetic association with disease risk.\n\n\nIntro\nAim of genome-wide association studies (GWAS): to identify single nucleotide polymorphisms (SNPs — variation in single nucleotide at specific position in genome; exists as 2 forms aka alleles) of which allele frequencies vary systematically as a fn of phenotypic trait values, since identifying trait-associated SNPs may reveal new insights into biological mechanisms behind phenotypes. SNP w/ 2 alleles → 3 different genotypes.\nHistorically studies suggest that psychiatric traits are influenced by SNPs — each having small individual effect sizes. GWAS relies strongly on in-depth knowledge of genetic architecture of the human genome, provided by the International HapMap Project (patterns of common SNPs within human DNA seq)+1000 Genomes project (map of both common and rare SNPs).\nGWAS results showed effect sizes of individual SNPs are small → want to find way to aggregate effect of SNPs → focus on PRS (polygenic risk score) analysis, since it can be applied to target samples w/ more modest sample sizes.\nPRS — combines effect sizes of multiple SNPs into an aggregated score (individual level, based on # risk variants carried, weighted by SNP effect sizes from an independent large-scaled discovery GWAS) used to predict disease risk.\n\ni.e. indication of total genetic risk for an individual for a specific trait (disease status); can be used in clinical pred/screening\nsignificantly associaed w/ case-control status for psychiatric traits; however discriminative accuracy still insufficient\nalso used to investigate if genetic effect sizes from a GWAS of a specific phenotype of interest can predict risk of another phenotype\npaper outline: quality control (QC) procedures before GWAS, commonly used tests of association, conduct analysis\n\nSoftware\nPLINK v1.08 will be used for QC procedures+stat analyses and R for visualization.\nPLINK: reads text-format or binary files, but binary is faster and recommended\n\ntext PLINK data: 2 files of different contents\n\ninfo on individuals+genotype (*.ped)\ninfo on genetic markers (*.map)\n\nbinary PLINK data: 3 files\n\nbinary file w/ IDs+genotypes for individuals (*.bed)\n\ne.g. individual IDs and genotypes\n\ntext file w/ info on individuals (*.fam)\n\ne.g. subject-related data like family relationship w/ other participants, sex, clinical diagnosis\n\ntext file w/ info on genetic markers (*.bim)\n\ne.g. info on phys position of SNPs\n\n\n\n\n\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001694/bin/MPR-27-e1608-g001.jpg\n\n\nPLINK — command line program → requires active shell waiting for commands\n\nafter prompt: indicate use of PLINK w/ plink (will need path to directory if not in current directory)\nafter plink other options, beginning w/ 2 dashes -- :\n\nprovide format+name of data files\n\ntext file: --file {your_file}\nbinary: --bfile {yourfile}\n\nafter: other options can be added; e.g. --assoc option for association analysis w/ \\(\\chi^2\\) test for each SNP to phenotype of interest\n\nmultiple options can be combined in a single command line, but default order is implemented\n--out {outfile} provides name to output file\n\nQC of genetic data\nAppropriate QC needed to generate reliable results since sources of errors include poor quality of DNA samples, poor DNA hybridization to array, poorly performing genotype probes, and sample mix-ups/contamination.\nData simulation w/ HapMap data:\n\nillustration of analysis steps w/ realistic genetic data using simulated dataset (\\(N=207\\)) w/ binary outcome measure\nrelatively small sample size+ethnically homogeneous dataset (*larger sample sizes will be needed to detect genetic risk factors of complex traits)\ndata from demo 1 in this md file (link)\n\n7 QC steps (key terms from beginning bolded) — hands-on lab in demos 1+2\n\nindividual+SNP missingness\ninconsistencies in assigned+genetic sex of subjects (see sex discrepancy)\nminor allele freq (MAF)\ndeviations from HWE (Hardy-Weinberg equilibrium)\nheterozygosity rate\nrelatedness\nethnic outliers (see population stratification)\n\nStep - command - fn - threshold table\n\n\n\n\n\n\n\n\n\nStep\nCommand\nFn\nThreshold\n\n\n\n\nmissingness of SNPs+individuals\n--geno, --mind\nexcludes SNPs missing in large proportion of subjects, remove SNPs w/ low genotype calls*; excludes individuals w/ high rates of genotype missingness, remove individual with low genotype calls\nrecommended: first filter SNPs+individuals based on relaxed threshold (0.2; &gt;20%) to filter out ones w/ very high levels of missingness; then apply filter w/ more stringent threshold (0.02);\n\n\nSNP filtering before individual filtering\n\n\n\n\n\nsex discrepancy\n--check-sex\nchecks for sex discrepancies between individuals recorded in dataset+sex based on X chromosome heterozygosity/homozygosity rates\ncan indicate sample mix-ups; want to see males X chromosome homozygosity estimate of &gt;0.8 and females of &lt;0.2\n\n\nMAF (minor allele freq)\n--maf\ninclude only SNPs above set MAF threshold\nSNPs w/ low MAF are rare → power lacking for detecting SNP-phenotype associations; SNPs also more prone to genotyping errors; MAF threshold depends on sample size — larger samples can use lower MAF thresholds; for large (\\(N = 100.000\\)) vs. moderate samples (\\(N = 10000\\)), 0.01 and 0.05 are commonly used as MAF threshold\n\n\nHWE\n--hwe\nexclude markers deviating from HWE\ncommon indicator of genotyping error; may also indicate evolutionary selection. binary traits — suggest excluding HWE where \\(p&lt;1\\times10^{-10}\\) in cases, \\(p&lt;1\\times10^{-6}\\) in controls; less strict case threshold avoids discarding disease-associated SNPs under selection; quantitative traits — recommend HWE \\(p &lt; 1\\times10^{-6}\\)\n\n\nheterozygosity\nscript from demo link\nexclude individuals w/ high or low heterozygosity rates\ndeviations can indicate sample contamination, inbreeding; suggest removing individuals deviating \\(\\pm 3s_x\\) from the samples’ heterozygosity rate mean.\n\n\nrelatedness\n--genome, --min\ncalculate identity by descent (IBD)** of all sample pairs, creates list of individuals w/ relatedness above set threshold**\nuse independent SNPs (pruning) for this analysis+limit to autosomal chromosomes only; cryptic relatedness can interfere with the association analysis — if you have a family-based sample (e.g., parent-offspring), don’t need to remove related pairs but should take family relatedness into account. for a population based sample suggested threshold is \\(\\hat{\\pi}=0.2\\)\n\n\npopulation stratification\n--genome, --cluster --mds-plot k\ncalculates IBD** of all sample pairs; produces \\(k\\)-dimensional representation of any substructure in data based on IBS\n\\(K\\)= # dimensions (to be defined; usually 10)\n\n\n\n\n*genotype calls: estimation of one unique SNP or genotype\n**identity by descent: matching segment of DNA shared by two or more people that has been inherited from a common ancestor without any intervening recombination\n\nControlling for population stratification\npopulation stratification: presence of multiple subpopulations (e.g., individuals with different ethnic background) in a study\n\ncan lead to false (+) associations and/or mask true associations\ne.g. chopstick gene (where a SNP due to population stratification accounted for nearly half of the variance in capacity to eat w. chopsticks)\nmay exist even within a single ethnic population\n\nOne method to correct: multidimensional scaling (MDS) approach — calculate genome-wide average proportion of alleles shared between any pair of individuals within sample to generate quantitative indices (components) of genetic variation for each individual. Individual component scores can be plotted to explore whether there are groups of individuals that are genetically more similar to each other than expected.\n\nplot scores of sample under investigation+population of known ethnic structure (such as HapMap/1KG data) — anchoring\nenables researcher to obtain ethnic info on their sample+determine possible ethnic outliers\noutliers can be removed; conduct new MDS analysis\n\nmain components used as covariates in association tests in order to correct for any remaining population stratification in population\n\n\n\n\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001694/bin/MPR-27-e1608-g003.jpg\n\n\nStatistical tests of association\nAfter QC+calculation of MDS components, select appropriate statistical test:\n\n1 df allelic test (trait value or log-odds of a binary trait increases linearly as a function of number of risk alleles — minor allele a vs major allele A)\n\neach allele becomes the statistical unit instead of an individual (main problem with this approach is that we would be ignoring the induced correlation in the data by the fact that we are, in some sense, “double counting” the observations)\n\nnon-additive tests*\n\ngenotypic association test (2 df — aa vs Aa vs AA)\ndominant gene action test (1 df — [aa & Aa] vs AA)\nrecessive gene action test (1 df — aa vs [Aa & AA])\n*not widely applied b/c power to detect non-additivity is low\n\n\nBinary outcome measure\n\nassociation between SNPs and a binary outcome (1=unaffected; 2=affected; 0/-9=missing) tested with --assoc (\\(\\chi^2\\) test of association — cannot include covariates) or --logistic (logistic regression analysis — allows covariates but more computational time)\n\nQuantitative outcome measure\n\nassociation between SNPs and quantitative outcome measures can be tested w/ assoc (automatically treats as numerical by performing asymptotic version of student’s \\(t\\) test to compare 2 means) and --linear (linear regression analysis w/ each individual SNP as a predictor; enables use of covariates but slower)\n\nCorrection for multiple testing\n\nmultiple testing burden since genotyping arrays can genotype up to 4 million markers concurrently, generating large # of tests\n3 widely applied alternatives for determining genome-wide significance:\n\nBonferroni: good for controlling Type I error rate; adjusted p w/ formula \\(\\frac{\\alpha}{n}\\) (where \\(\\alpha\\) is original p-value; here it is 0.05 and \\(n\\) is # SNPs tested)\n\nmany SNPs correlated due to linkage disequilibrium (LD) and thus not independent — too harsh of a correction and could lead to increase in proportion of false negative findings\n\nBenjamini-Hochberg: controls expected proportion of FP among all signals with FDR value below fixed threshold\n\nFDR method assumes SNPs are independent\nlimitation: SNPs and thus p values are not independent\n\n\n\n--adjust outputs unadjusted p value along w/ p values corrected w/ various multiple testing correction methods\n\n\npermutation methods: outcome measure labels randomly permuted mutiple times to remove any true association between outcome measure and genotype\n\nstatistical tests performed for all permuted datasets; compare empirical distribution of test statistic and p values under null to original test statistic\n--assoc and --mperm combined to generate 2 p-values: EMP1 (empirical p value — uncorrected) and EMP2 (empirical p value corrected for multiple testing)\n\n\n\nPRS analysis\nComputing a PRS\n\nsingle variant association analysis is the primary methdo in GWAS but needs large sample size to detect SNPs for many complex traits\nPRS aggregates genetic risk across genome in single individual polygenic score for a trait of interest\nlarge discovery sample required to reliably dtermine how much each SNP is expected to contribute to polygenic score (”weights”) of a specific trait\nin an independent target sample (doesn’t need to be as big) polygenic scores calculated based on genetic DNA profiles+these weights — around 200 subjects provides sufficient power to detect significan proportion of variance explained\ndiscovery+target samples should have same # subjects until the target sample includes 2,000 subjects\nmore available samples → additional subjects should be included in discovery sample to maximize accuracy of estimation of effect sizes\nhas been successfully used to show significant associations both within+across traits\n\ne.g. a significant association with disease risk was found despite fact that the available sample sizes were too small to detect genome-wide significant SNPs\n\n\n\n\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001694/bin/MPR-27-e1608-g004.jpg\n\n\n\nto conduct PRS analysis, trait specific weights (\\(\\beta\\)s for continuous traits+log of odds ratio for binary traits) are obtained from a discovery GWAS\n\nin target sample, PRS is calculated for each individual based on weighted sum of # risk alleles that he or she carries multiplied by trait-specific weights (SNP effect sizes publicly available for many complex traits)\n\nall common SNPs could be used in a PRS analysis, but it is customary to clump first (only the most significant SNP (i.e., lowest p* value) in each LD block is identified and selected for further analyses) the GWAS results, then compute risk scores\np value thresholds typically used to remove SNPs that show little or no statistical evidence for association\nmultiple PRS analyses will be performed with varying thresholds for p values\n\nConducting polygenic risk prediction analyses\n\nonce PRS has been calculated for all subjects in target sample, use scores in logistic reg analysis → predict trait expected to show genetic overlap w/ trait of interest\n\nprediction accuracy expressed w/ \\(R^2\\) measure of regression analysis (include at least a few MDS components as covariates in regression analysis to control for population stratification)\n\nto estimate how much variation is explained by PRS, \\(R^2\\) of a model that includes covariates+PRS will be compared\n\nincrease in \\(R^2\\) due to PRS indicates increase in prediction accuracy explained by genetic risk factors\n\nprediction accuracy of PRS depends mostly on (co-)heritability of analyzed traits, the number of SNPs, and the size of discovery sample\n\nsize of target sample only affects reliability of \\(R^2\\)"
  },
  {
    "objectID": "posts/nn_3b1b/index.html",
    "href": "posts/nn_3b1b/index.html",
    "title": "Neural networks (3b1b)",
    "section": "",
    "text": "original tutorial link\nNeuron: thing that holds a number between 0 and 1** (see revised definition at the end)\n\nex. MNIST dataset — each neuron is one of the 28x28 pixels, holding a number between 0 and 1 representing grayscale value — activation\n\nwhen activation is a high number, neuron is “lit up”\n\noutput layer: each neuron has activation between 0 and 1 representing how likely it is to be each digit (between 0-9)\nhidden layers in between: ? — don’t know what’s going on there yet\n\nActivations in one layer determine activations in the next layer, loosely analogous to how some group of neurons (biological) firing causes some other group of neurons to fire.\n\nEx. if you feed in an image lighting up all 784 neurons of the input layer according to the brightness of each pixel of the image, that pattern of acitvations causes some very specific pattern in next layer → next layer → some pattern in output layer → brightest neuron of output layer is prediction for the digit\n\nWhy use layers?\n\nnot all connections are equal — some might be stronger than others\ngoal/hope:\n\nhumans piece together various components like loops+lines when recognizing digits\nex. w/ NN, feed in an image w/ a loop up top → there is some specific neuron whose activation will be close to 1.0\n\nhow to recognize subcomponents? recognize various edges that make it up:\n\nhope is that each neuron in second layer of network corresponds to some little edge; when image comes in, lights up neurons associated w/ all specific little edges inside that image → light up neurons in third layer associated w/ larger scale patterns like loops+long lines → cause some neuron from final layer to fire which corresponds to the appropriate digit\n\n\nLayers break problems into small pieces\n\nhow would acitvations in one layer determine activations in the next?\ngoal: have some mehcanism that coudl combine pixels into edges → edges into patterns → patterns into digits\nex. want neuron in second layer to pick up on whether or not the image has an edge in a particular region\n→ Q: what parameters should exist?\n\nassign weight to each one of connections between our neuron+neurons from first layer\n\nex. w1, w2, w3…\n\nweights are just numbers → take all activations from first layer+compute weighted sum according to these weights\n\nw1a1+w2a2+w3a3+….wnan\n\nthink of weights as organized into a grid of their own (blue=positive, red=negative weights); brightness of pixel=loose depiction of weight value\n\nmake weights associated w/ almost all of the pixels 0 except for some positive weights associated w/ these pixels in region where we want to detect an edge\n\n→ take weighted sum = adding up values of pixels in our region of interest\n\nthis pattern of weights will also pick up on big blobs of activated pixels (not just edges); make sure to pick up on whether or not this is an edge by having some negative weights associated w/ surrounding pixels\n\n\n→ sum will be largest when these pixels are bright and surrounding pixels are dark\n\n\n\nSigmoid squishification\n\nresult of weighted sum like this can be any number; this example (categorical) we want activations to be values between 0 and 1\nsolution: pump weighted sum into some fn that squishes real number line into range between 0 and 1\ncommon fn example: sigmoid fn (aka logistic curve)\n\\[\n  \\sigma(x)=\\frac{1}{1+e^{-x}}\n  \\]\n\nvery neg inputs end up close to 0; very pos inputs close to 1\nsteadily increases around 0\n\nback to examaple: activation of the neuron here is measure of how positive the relevant weighted sum is\nmaybe don’t want neuron to light up when sum is greater than 0; what if you only want it to be active when the sum is &gt; 10?\n\n→ you want some bias for it to be active\nsolution: add in bias — some other number (such as -10) to the weighted sum before plugging it through the sigmoid fn\n\nbias tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active\n\n\n\\[\n  \\sigma(w_1a_1+w_2a_2+w_3a_3+\\cdots+w_na_n\\color{green}-10\\color{black})\n  \\]\nbetween just 2 layers, each neuron will have its own weights and its own bias\n\nin this example:\n\n\\(784\\times 16+16\\times 16+16\\times 10\\;\\text{weights}\\)\n\\(16+16+10\\; \\text{biases}\\)\n\ntotal: 13,002 weights and biases\n\n\nNotation review\n\nexample: actual function for first neuron of second layer:\n\\[\n  a_0^{(1)}=\\sigma(w_{0,0}a_0^{(0)}+w_{0,1}a_1^{(0)}+\\cdots+w_{0,n}a_n^{(0)}+b_0)\n  \\]\n\n\norganize all activations from one layer into a column as a vector\n\n\\[\n\\begin{pmatrix}\na_0^{(0)}\\\\\na_1^{(0)}\\\\\n\\vdots\\\\\na_n^{(0)}\n\\end{pmatrix}\n\\]\n\norganize all weights as a matrix — each row vector: weights for connections between one layer and a single neuron in next layer\n\n\nex. the first row in matrix below represents connections between first layer and first neuron of second layer\n\n\\[\n\\color{purple}\n\\begin{pmatrix}\nw_{0,0}&w_{0,1}&\\dots&w_{0,n}\\\\\nw_{1,0}&w_{1,1}&\\dots&w_{1,n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nw_{k,0}&w_{k,1}&\\dots&w_{k,n}\n\\end{pmatrix}\n\\]\n\neach component of matrix-vector product is weighted sum of one of the sets of weights\n\n\nex. taking weighted sum of activations in first layer according to weights for the first neuron of second layer corresponds to one of the terms in matrix vector product (the first one), and so on\n\n\\[\n\\color{purple}\\begin{pmatrix}\nw_{0,0}&w_{0,1}&\\dots&w_{0,n}\\\\\nw_{1,0}&w_{1,1}&\\dots&w_{1,n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nw_{k,0}&w_{k,1}&\\dots&w_{k,n}\n\\end{pmatrix}\n\\color{black}\n\\begin{pmatrix}\na_0^{(0)}\\\\\na_1^{(0)}\\\\\n\\vdots\\\\\na_n^{(0)}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n?\\\\\n?\\\\\n\\vdots\\\\\n?\n\\end{pmatrix}\n\\]\n\ninstead of adding bias to each value independently, organize biases into vector and add that whole vector to previous matrix-vector product\n\n\\[\n\\color{purple}\\begin{pmatrix}\nw_{0,0}&w_{0,1}&\\dots&w_{0,n}\\\\\nw_{1,0}&w_{1,1}&\\dots&w_{1,n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nw_{k,0}&w_{k,1}&\\dots&w_{k,n}\n\\end{pmatrix}\\color{black}\n\\begin{pmatrix}\na_0^{(0)}\\\\\na_1^{(0)}\\\\\n\\vdots\\\\\na_n^{(0)}\n\\end{pmatrix}\n+\n\\color{green}\n\\begin{pmatrix}\nb_0\\\\\nb_1\\\\\n\\vdots\\\\\nb_n\n\\end{pmatrix}\n\\]\n\napply sigmoid fn to each specific component of the resulting vector inside\n\n\\[\n\\color{teal}\\sigma\\left[\\color{purple}\\begin{pmatrix}\nw_{0,0}&w_{0,1}&\\dots&w_{0,n}\\\\\nw_{1,0}&w_{1,1}&\\dots&w_{1,n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nw_{k,0}&w_{k,1}&\\dots&w_{k,n}\n\\end{pmatrix}\\color{black}\n\\begin{pmatrix}\na_0^{(0)}\\\\\na_1^{(0)}\\\\\n\\vdots\\\\\na_n^{(0)}\n\\end{pmatrix}\n+\n\\color{green}\n\\begin{pmatrix}\nb_0\\\\\nb_1\\\\\n\\vdots\\\\\nb_n\n\\end{pmatrix}\\color{teal}\\right]\n\\]\n\\[\n\\color{teal}\\sigma(\\mathbf{\\color{purple}W\\color{black}a^{(0)}}\\color{black}+\\color{green}\\mathbf{b}\\color{teal})\n\\]\ncommunicates full transition of activations from one layer to the next\nUpshot\nnew definition of neuron: a function — one takes in outputs of all the neurons in the previous layer and spits out a number between 0 and 1 (in this example)\nThe entire net is a (very complicated) fn — takes in 784 numbers as input and spits out 10 numbers as output.\n\\[\nf(a_0,\\dots,a_{783})=\\begin{pmatrix}\ny_0\\\\\n\\vdots\\\\\ny_9\n\\end{pmatrix}\n\\]\nnetwork → fn"
  },
  {
    "objectID": "posts/pytorch_train_classifier/index.html",
    "href": "posts/pytorch_train_classifier/index.html",
    "title": "PyTorch tutorial: training classifier demo",
    "section": "",
    "text": "original tutorial link\nWhat about data?\ncan use standard python packages that load data into np array → convert array into torch.*Tensor\nUseful packages:\n\nimages: Pillow, OpenCV\naudio: scipy, librosa\ntext: raw Python or Cython based loading, or NLTK, SpaCy\n\nFor vision: torchvision package has data loader for common datasets like ImageNet, CIFAR10, MNIST, etc. and data transformers for images, viz., torchvision.datasets and torch.utils.data.DataLoader\nDemo — ex: CIFAR10 dataset\n\n\nClasses: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\nimages are of size 3x32x32 (3 channel color images of 32x32 pixels)\n\nTraining an image classifier\nSteps:\n\nLoad+normalize CIFAR10 training+test datasets using torchvision\nDefine CNN\nDefine loss fn\nTrain net on training data\nTest net on test data\n\n(1) Load+normalize CIFAR10\nimport torchvision\nimport torchvision.transforms as transforms\noutput of torchvision datasets: PILImage images of range [0, 1] → transform to Tensors of normalized range [-1, 1]\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 4\n#check memory \n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n(2) Define CNN\nsame as NN code from NN demo — modify to take 3-channel images rather than 1 channel\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n#previously 1 channel:\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        # kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square, you can specify with a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n#now 3 channel:\nclass Net(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # 3 input image channels, 6 output channels, 5x5 square convolution\n        # kernel\n        self.conv1 = nn.Conv2d(3, 6, 5)\n                # Max pooling over a (2, 2) window\n                self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        # If the size is a square, you can specify with a single number\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\n(3) Define loss fn+optimizer\nex. classification cross-entropy loss+SGD w/ momentum\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n(4) Train net\nloop over data iterator+feed inputs to network/optimize\nbelow: 2 passes\nfor epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n            running_loss = 0.0\n\nprint('Finished Training')\ncan save trained model:\nPATH = './cifar_net.pth'\ntorch.save(net.state_dict(), PATH)\ninfo on serialization\n\nsaving+loading NN modules:\nfrequently serialized using a “state dict” — conains all of its parameters+persistent buffers\n\nbn = torch.nn.BatchNorm1d(3, track_running_stats=True)\nlist(bn.named_parameters())\n\n#output:\n#[('weight', Parameter containing: tensor([1., 1., 1.], requires_grad=True)),\n# ('bias', Parameter containing: tensor([0., 0., 0.], requires_grad=True))]\n\nlist(bn.named_buffers())\n\n#output:\n#[('running_mean', tensor([0., 0., 0.])),\n# ('running_var', tensor([1., 1., 1.])),\n# ('num_batches_tracked', tensor(0))]\n\n#state dict serialization:\nbn.state_dict()\n\n#output:\n#OrderedDict([('weight', tensor([1., 1., 1.])),\n#             ('bias', tensor([0., 0., 0.])),\n#             ('running_mean', tensor([0., 0., 0.])),\n#             ('running_var', tensor([1., 1., 1.])),\n#             ('num_batches_tracked', tensor(0))])\n\nrecommended to save only state dict — fn load_state_dict() can restore states from a state dict\n\ntorch.save(bn.state_dict(), 'bn.pt')\n#torch.load to load state dict from file\nbn_state_dict = torch.load('bn.pt') \nnew_bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n#restore w/ load_state_dict()\nnew_bn.load_state_dict(bn_state_dict)\n(5) Test net\ncheck if net has learnt anything — predict class label that net outputs+check against ground-truth\n\nif prediction is correct → add sample to list of correct predictions\n\n(if loading is necessary)\nnet = Net()\nnet.load_state_dict(torch.load(PATH))\noutputs: energies for 10 classes — higher energy = net thinks that the image is of the particular class more\noutputs = net(images)\nex. get index of highest energy:\n_, predicted = torch.max(outputs, 1)\n\nprint('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n                              for j in range(4)))\nnow look at whole dataset:\ncorrect = 0\ntotal = 0\n# since we're not training, don't need to calculate gradients for outputs\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = net(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n#accuracy of net on 10000 test images:\n100 * correct // total\n#compare to chance\n\n#chance: 10% accuracy\ncheck classes that performed well vs not well:\n#prepare to count preds for each class\ncorrect_pred = {classname: 0 for classname in classes}\ntotal_pred = {classname: 0 for classname in classes}\n\n#no grads needed\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predictions = torch.max(outputs, 1)\n        # collect the correct predictions for each class\n        for label, prediction in zip(labels, predictions):\n            if label == prediction:\n                correct_pred[classes[label]] += 1\n            total_pred[classes[label]] += 1\n\n#print accuracy for each class\nfor classname, correct_count in correct_pred.items():\n        accuracy = 100*float(correct_count) / total_pred[classname]\n        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\nTraining on GPU\ncan transfer NN onto GPU\n\ndefine device as first visible cuda device if we have CUDA available\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n# Assuming that we are on a CUDA machine, this should print a CUDA device:\nprint(device)\nrecursively go over all modules+conver params/buffers to CUDA tensors:\nnet.to(device)\n\n#send inputs+targets at every step to GPU too:\ninputs, labels = data[0].to(device), data[1].to(device)\nExercise: Try increasing the width of your network (argument 2 of the first nn.Conv2d, and argument 1 of the second nn.Conv2d – they need to be the same number), see what kind of speedup you get.\npreviously: width of 6\n\n\n\nnn width of 6\n\n\nnow: width of 20\n\n\n\nnn width of 20\n\n\nwider network → faster training time"
  },
  {
    "objectID": "posts/wine_nn/index.html",
    "href": "posts/wine_nn/index.html",
    "title": "Wine dataset MLP example",
    "section": "",
    "text": "from torch import nn, optim, from_numpy\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nReading in dataset – in this example, a built-in dataset was used\n\n\n#read in data\nwine = load_wine()\nX = from_numpy(wine.data).float()\ny = from_numpy(wine.target).long()\n\n\nSplit into test and train (note: may want to split into test, train, eval for more complex NNs)\n\n\nxtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.2, random_state=2023)\n\n\nInstantiating a custom dataset class\n\n\n#dataset class\nclass WineDataset(Dataset):\n    def __init__(self, x, y):\n        self.n_samples = x.shape[0] # len(y)\n        self.x = x\n        self.y = y\n    \n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n\n    def __len__(self):\n        return self.n_samples\n\n\nCreate test, train dataset objects and dataloader objects (this makes dividing it into batches easier)\n\n\ntrain_data = WineDataset(xtrain, ytrain)\ntest_data = WineDataset(xtest, ytest)\n\n\ntrain_loader = DataLoader(train_data, batch_size=8, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=8, shuffle=True)\n\n\nCreate NN class+define its structure – method 1 (predetermined # hidden layers). This example uses ReLU as activation fn, sigmoid as fn applied to output layer since the target variable is categorical\n\n\nclass WineNN(nn.Module):\n    def __init__(self, nf, hL1, hL2, hL3, nO):\n        super().__init__()\n        self.hL1 = nn.Linear(nf, hL1)\n        self.hL2 = nn.Linear(hL1, hL2)\n        self.hL3 = nn.Linear(hL2, hL3)\n        self.oL = nn.Linear(hL2, nO)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.hL1(x)\n        out = self.relu(out)\n        out = self.hL2(out)\n        out = self.relu(out)\n        out = self.hL3(out)\n        out = self.relu(out)\n        out = self.oL(out)\n        y = self.sigmoid(out)\n        return y\n\nmethod 2 (easily adjust # nodes per layer without having to worry about different # hidden layers each time)\n\nclass FlexNet(nn.Module):\n    def __init__(self, n_in, hidden_layers, n_out):\n        super().__init__()\n        self.input_layer = nn.Linear(n_in, hidden_layers[0])\n        self.hidden_layers = nn.ModuleDict(\n            {f\"hl{i}\":nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers)-1)}\n        )\n        self.output_layer = nn.Linear(hidden_layers[-1], n_out)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.input_layer(x)\n        out = self.relu(out)\n        for name, layer in self.hidden_layers.items():\n            out = self.relu(layer(out))\n        out = self.output_layer(out)\n        y = self.sigmoid(out)\n        return y\n\n\nCreate your NN object (using the custom class you made)\n\n\n#method 1\nwine_net = WineNN(13, 4, 4, 4, 3)\n\n\n#method 2\nflex_net = FlexNet(13, [4, 4, 4], 3)\n\n\nDefine your loss fn (criterion) and optimizer (SGD, adam, etc.)\n\n\n#method 1\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(wine_net.parameters(), lr=0.01)\n\n\n#method 2\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(flex_net.parameters(), lr=0.01)\n\n\nSpecify training hyperparameters+train model (probably using a loop); track loss over epochs\n\n\n#method 1\nn_epochs = 200\nbatch_loss = 0\nepoch_loss = []\n\nfor epoch in range(n_epochs):\n    batch_loss = 0\n    #each iteration = a single batch\n    for batch, (inputs, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        #forward pass\n        output = wine_net(inputs)\n        #get loss\n        loss = criterion(output, labels)\n        #grad\n        loss.backward()\n        optimizer.step()\n        #print(f\"iteration {i}: loss {loss}\")\n        batch_loss += loss.item()\n    #take average\n    epoch_loss.append(batch_loss/(batch+1))\n    print(f\"epoch {epoch}: loss {batch_loss/(batch+1)}\")\n\nepoch 0: loss 1.1035196118884616\nepoch 1: loss 1.1029477251900568\nepoch 2: loss 1.1027646130985684\nepoch 3: loss 1.1029533280266657\nepoch 4: loss 1.10271065764957\nepoch 5: loss 1.1025322212113275\nepoch 6: loss 1.1026486886872187\nepoch 7: loss 1.1021430558628507\nepoch 8: loss 1.1016399595472548\nepoch 9: loss 1.1020683579974704\nepoch 10: loss 1.1015860239664714\nepoch 11: loss 1.1014065543810527\nepoch 12: loss 1.1012010044521756\nepoch 13: loss 1.1007436513900757\nepoch 14: loss 1.100804540846083\nepoch 15: loss 1.1012108325958252\nepoch 16: loss 1.1007501019371881\nepoch 17: loss 1.1003097825580173\nepoch 18: loss 1.1000993847846985\nepoch 19: loss 1.100281794865926\nepoch 20: loss 1.0994982322057087\nepoch 21: loss 1.0997626384099324\nepoch 22: loss 1.0991399486859639\nepoch 23: loss 1.0989764796362982\nepoch 24: loss 1.0990579790539212\nepoch 25: loss 1.0991833673583136\nepoch 26: loss 1.0992523564232721\nepoch 27: loss 1.0981981820530362\nepoch 28: loss 1.0983789828088548\nepoch 29: loss 1.0980063610606723\nepoch 30: loss 1.0982675949732463\nepoch 31: loss 1.0979865590731304\nepoch 32: loss 1.0979011721081204\nepoch 33: loss 1.0979888571633234\nepoch 34: loss 1.0976383553610907\nepoch 35: loss 1.0974456005626254\nepoch 36: loss 1.0970932642618816\nepoch 37: loss 1.097204777929518\nepoch 38: loss 1.0970725152227614\nepoch 39: loss 1.096924172507392\nepoch 40: loss 1.0963448153601751\nepoch 41: loss 1.0965119865205553\nepoch 42: loss 1.0962118440204196\nepoch 43: loss 1.096218228340149\nepoch 44: loss 1.095618916882409\nepoch 45: loss 1.0956075919999018\nepoch 46: loss 1.0953219731648762\nepoch 47: loss 1.0954916410975986\nepoch 48: loss 1.0955353710386488\nepoch 49: loss 1.0947879950205486\nepoch 50: loss 1.0952717529402838\nepoch 51: loss 1.0952551364898682\nepoch 52: loss 1.0946998132599726\nepoch 53: loss 1.0948020550939772\nepoch 54: loss 1.0944616595904033\nepoch 55: loss 1.0948318905300565\nepoch 56: loss 1.0945119592878554\nepoch 57: loss 1.0944747196303473\nepoch 58: loss 1.0945683320363362\nepoch 59: loss 1.0939074622260199\nepoch 60: loss 1.0938756465911865\nepoch 61: loss 1.0939034356011286\nepoch 62: loss 1.0937815176116095\nepoch 63: loss 1.0934905740949843\nepoch 64: loss 1.093288282553355\nepoch 65: loss 1.0932929317156475\nepoch 66: loss 1.0928763416078355\nepoch 67: loss 1.092831916279263\nepoch 68: loss 1.0930862228075664\nepoch 69: loss 1.093257983525594\nepoch 70: loss 1.0931887361738417\nepoch 71: loss 1.0924624866909451\nepoch 72: loss 1.092402868800693\nepoch 73: loss 1.0925499664412603\nepoch 74: loss 1.0924988389015198\nepoch 75: loss 1.0923377010557387\nepoch 76: loss 1.0922627051671345\nepoch 77: loss 1.0918546120325725\nepoch 78: loss 1.091749178038703\nepoch 79: loss 1.0919462905989752\nepoch 80: loss 1.0918671356307135\nepoch 81: loss 1.091435240374671\nepoch 82: loss 1.0916556980874803\nepoch 83: loss 1.0912510951360066\nepoch 84: loss 1.0918470819791157\nepoch 85: loss 1.091390483909183\nepoch 86: loss 1.0909644299083285\nepoch 87: loss 1.0911520918210347\nepoch 88: loss 1.090389092763265\nepoch 89: loss 1.0906575785742865\nepoch 90: loss 1.0901929206318326\nepoch 91: loss 1.0904839436213176\nepoch 92: loss 1.0908484326468573\nepoch 93: loss 1.0906624926461115\nepoch 94: loss 1.089851929081811\nepoch 95: loss 1.0900147292349074\nepoch 96: loss 1.0914795531166925\nepoch 97: loss 1.0898933410644531\nepoch 98: loss 1.0898751748932733\nepoch 99: loss 1.0895402961307101\nepoch 100: loss 1.0901741054322984\nepoch 101: loss 1.0899525218539767\nepoch 102: loss 1.0900302131970723\nepoch 103: loss 1.0895659857326083\nepoch 104: loss 1.0897157457139757\nepoch 105: loss 1.0899209049012926\nepoch 106: loss 1.0900744597117107\nepoch 107: loss 1.0899023811022441\nepoch 108: loss 1.0895145138104756\nepoch 109: loss 1.08957752916548\nepoch 110: loss 1.090044829580519\nepoch 111: loss 1.0897498660617404\nepoch 112: loss 1.0891498393482633\nepoch 113: loss 1.0892020397716098\nepoch 114: loss 1.089130613538954\nepoch 115: loss 1.0888184242778354\nepoch 116: loss 1.0894453591770596\nepoch 117: loss 1.0884952942530315\nepoch 118: loss 1.088833424780104\nepoch 119: loss 1.0900741550657485\nepoch 120: loss 1.0881368385420904\nepoch 121: loss 1.087913499938117\nepoch 122: loss 1.088478724161784\nepoch 123: loss 1.0887428257200453\nepoch 124: loss 1.0883739127053156\nepoch 125: loss 1.0883182750807867\nepoch 126: loss 1.0882495178116693\nepoch 127: loss 1.0886716975106134\nepoch 128: loss 1.0881531304783292\nepoch 129: loss 1.0874442789289687\nepoch 130: loss 1.0881986088222928\nepoch 131: loss 1.0883075329992506\nepoch 132: loss 1.0875961316956415\nepoch 133: loss 1.0870629019207425\nepoch 134: loss 1.087472465303209\nepoch 135: loss 1.0881112880176969\nepoch 136: loss 1.0880680680274963\nepoch 137: loss 1.087182018491957\nepoch 138: loss 1.0869742300775316\nepoch 139: loss 1.0869055456585355\nepoch 140: loss 1.087344812022315\nepoch 141: loss 1.0864411460028753\nepoch 142: loss 1.0869298775990803\nepoch 143: loss 1.0886027812957764\nepoch 144: loss 1.0885486801465352\nepoch 145: loss 1.0869460238350763\nepoch 146: loss 1.0882866316371493\nepoch 147: loss 1.0871798793474834\nepoch 148: loss 1.086258504125807\nepoch 149: loss 1.0869462821218703\nepoch 150: loss 1.0876785318056743\nepoch 151: loss 1.0879658261934917\nepoch 152: loss 1.0863981511857774\nepoch 153: loss 1.0875712368223402\nepoch 154: loss 1.0867549644576178\nepoch 155: loss 1.0868221521377563\nepoch 156: loss 1.0873284935951233\nepoch 157: loss 1.0874247153600056\nepoch 158: loss 1.0868449343575373\nepoch 159: loss 1.0865531365076702\nepoch 160: loss 1.0865427652994792\nepoch 161: loss 1.0880825916926067\nepoch 162: loss 1.0865484608544245\nepoch 163: loss 1.086406581931644\nepoch 164: loss 1.0873221423890855\nepoch 165: loss 1.0868662463294134\nepoch 166: loss 1.0856000317467585\nepoch 167: loss 1.086272074116601\nepoch 168: loss 1.0859976874457464\nepoch 169: loss 1.0862104230456882\nepoch 170: loss 1.0859081943829854\nepoch 171: loss 1.0858315957917108\nepoch 172: loss 1.0861900183889601\nepoch 173: loss 1.085291862487793\nepoch 174: loss 1.0855152673191495\nepoch 175: loss 1.0860816571447585\nepoch 176: loss 1.0857138766182794\nepoch 177: loss 1.0860220458772447\nepoch 178: loss 1.0865194731288486\nepoch 179: loss 1.08499194516076\nepoch 180: loss 1.085925665166643\nepoch 181: loss 1.0858814981248643\nepoch 182: loss 1.0861274003982544\nepoch 183: loss 1.0854872266451518\nepoch 184: loss 1.0860595968034532\nepoch 185: loss 1.086121678352356\nepoch 186: loss 1.0853958394792345\nepoch 187: loss 1.0844525794188182\nepoch 188: loss 1.0853349566459656\nepoch 189: loss 1.0859438445832994\nepoch 190: loss 1.0865548981560602\nepoch 191: loss 1.085860616630978\nepoch 192: loss 1.0852837430106268\nepoch 193: loss 1.0848489006360371\nepoch 194: loss 1.0845766796006098\nepoch 195: loss 1.087120532989502\nepoch 196: loss 1.085456755426195\nepoch 197: loss 1.0854012734360166\nepoch 198: loss 1.0847520894474454\nepoch 199: loss 1.0857134097152286\n\n\n\nWhen still playing around w/ nn structure+determining hyperparameters, you can plot loss over epochs\n\n\nplt.figure(figsize=(10,6))\nsns.lineplot(epoch_loss)\nplt.xlabel(\"epoch (1-200)\")\nplt.ylabel(\"cross-entropy loss\")\nplt.title(\"cross-entropy loss over 200 epochs\")\nplt.show()\n\n\n\n\n\n#method 2\nn_epochs = 200\nbatch_loss = 0\nepoch_loss = []\n\nfor epoch in range(n_epochs):\n    batch_loss = 0\n    #each iteration = a single batch\n    for batch, (inputs, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        #forward pass\n        output = flex_net(inputs)\n        #get loss\n        loss = criterion(output, labels)\n        #grad\n        loss.backward()\n        optimizer.step()\n        #print(f\"iteration {i}: loss {loss}\")\n        batch_loss += loss.item()\n    #take average\n    epoch_loss.append(batch_loss/(batch+1))\n    print(f\"epoch {epoch}: loss {batch_loss/(batch+1)}\")\n\nepoch 0: loss 1.0758992003069983\nepoch 1: loss 1.0755802591641743\nepoch 2: loss 1.0954784154891968\nepoch 3: loss 1.086965435081058\nepoch 4: loss 1.0983157025443182\nepoch 5: loss 1.0788868798149958\nepoch 6: loss 1.0852028992440965\nepoch 7: loss 1.0833798779381647\nepoch 8: loss 1.07219797372818\nepoch 9: loss 1.0846776001983218\nepoch 10: loss 1.084008405605952\nepoch 11: loss 1.069062242905299\nepoch 12: loss 1.066004004743364\nepoch 13: loss 1.0729810330602858\nepoch 14: loss 1.062520855002933\nepoch 15: loss 1.0717429882950253\nepoch 16: loss 1.0608669420083363\nepoch 17: loss 1.0530325108104281\nepoch 18: loss 1.0582917498217688\nepoch 19: loss 1.048289544052548\nepoch 20: loss 1.0456899073388841\nepoch 21: loss 1.046246074967914\nepoch 22: loss 1.0382493966155582\nepoch 23: loss 1.0410330825381808\nepoch 24: loss 1.0383899874157376\nepoch 25: loss 1.0405794845687018\nepoch 26: loss 1.039245198170344\nepoch 27: loss 1.0447058611445956\nepoch 28: loss 1.040022028817071\nepoch 29: loss 1.0385067893399134\nepoch 30: loss 1.0346433652771845\nepoch 31: loss 1.040452755159802\nepoch 32: loss 1.0338236623340182\nepoch 33: loss 1.0348394480016496\nepoch 34: loss 1.0315190023846097\nepoch 35: loss 1.0348602036635082\nepoch 36: loss 1.0318101876311832\nepoch 37: loss 1.032920668522517\nepoch 38: loss 1.0338921116458044\nepoch 39: loss 1.030531449450387\nepoch 40: loss 1.0345942311816745\nepoch 41: loss 1.0321497718493144\nepoch 42: loss 1.0284072558085124\nepoch 43: loss 1.0314678053061168\nepoch 44: loss 1.0307140681478713\nepoch 45: loss 1.0294452408949535\nepoch 46: loss 1.0238284468650818\nepoch 47: loss 1.0285656419065263\nepoch 48: loss 1.0301485160986583\nepoch 49: loss 1.0278050535255008\nepoch 50: loss 1.0291366179784138\nepoch 51: loss 1.0294345551066928\nepoch 52: loss 1.0284696817398071\nepoch 53: loss 1.027887417210473\nepoch 54: loss 1.0283823377556272\nepoch 55: loss 1.0240179465876684\nepoch 56: loss 1.024108562204573\nepoch 57: loss 1.0211945639716253\nepoch 58: loss 1.018980062670178\nepoch 59: loss 1.0265384548240237\nepoch 60: loss 1.0181521077950795\nepoch 61: loss 1.0233079857296414\nepoch 62: loss 1.015581038263109\nepoch 63: loss 1.0194327135880787\nepoch 64: loss 1.0193254053592682\nepoch 65: loss 1.0265762706597645\nepoch 66: loss 1.0255666143364377\nepoch 67: loss 1.016391204463111\nepoch 68: loss 1.0159449544217851\nepoch 69: loss 1.0204911364449396\nepoch 70: loss 1.010678105884128\nepoch 71: loss 1.0153994858264923\nepoch 72: loss 1.021962755256229\nepoch 73: loss 1.0189153287145827\nepoch 74: loss 1.0143067240715027\nepoch 75: loss 1.0070891446537442\nepoch 76: loss 1.0126522713237338\nepoch 77: loss 1.0084776447878943\nepoch 78: loss 1.009682810968823\nepoch 79: loss 1.0167745351791382\nepoch 80: loss 0.9994094901614718\nepoch 81: loss 1.002054105202357\nepoch 82: loss 1.0176017118824854\nepoch 83: loss 1.0072523554166157\nepoch 84: loss 0.9928684631983439\nepoch 85: loss 1.0101111729939778\nepoch 86: loss 1.0118306411637201\nepoch 87: loss 0.9833696683247884\nepoch 88: loss 0.9966344270441267\nepoch 89: loss 1.0149242679278057\nepoch 90: loss 1.0054545369413164\nepoch 91: loss 1.014212002356847\nepoch 92: loss 1.0113542907767825\nepoch 93: loss 1.0187519258923001\nepoch 94: loss 1.0080854329797957\nepoch 95: loss 1.000441633992725\nepoch 96: loss 0.992532577779558\nepoch 97: loss 1.0097742180029552\nepoch 98: loss 0.9927737679746416\nepoch 99: loss 1.004389289352629\nepoch 100: loss 0.9952792028586069\nepoch 101: loss 0.9944166640440623\nepoch 102: loss 1.0122214986218347\nepoch 103: loss 0.992783334520128\nepoch 104: loss 1.0132885873317719\nepoch 105: loss 0.9835924241277907\nepoch 106: loss 0.999792648686303\nepoch 107: loss 0.9840762846999698\nepoch 108: loss 0.9854936665958829\nepoch 109: loss 0.997794959280226\nepoch 110: loss 0.9719465904765658\nepoch 111: loss 0.9920585718419817\nepoch 112: loss 0.9930699931250678\nepoch 113: loss 0.9962947931554582\nepoch 114: loss 0.9979508750968509\nepoch 115: loss 0.9912625186973147\nepoch 116: loss 0.9890427986780802\nepoch 117: loss 0.9856837424967024\nepoch 118: loss 1.0160565740532346\nepoch 119: loss 0.9847754902309842\nepoch 120: loss 0.9861320389641656\nepoch 121: loss 0.9931310514609019\nepoch 122: loss 0.9988126125600603\nepoch 123: loss 0.9763712452517616\nepoch 124: loss 0.9902465475930108\nepoch 125: loss 1.0241407917605505\nepoch 126: loss 0.9962930844889747\nepoch 127: loss 1.0219404333167605\nepoch 128: loss 1.0118498305479686\nepoch 129: loss 0.9973408314916823\nepoch 130: loss 0.9869284927845001\nepoch 131: loss 0.9830329484409757\nepoch 132: loss 0.9844811194472842\nepoch 133: loss 0.9748289982477824\nepoch 134: loss 1.0123606787787542\nepoch 135: loss 0.996467391649882\nepoch 136: loss 0.9946328202883402\nepoch 137: loss 0.9884665409723917\nepoch 138: loss 0.991803463962343\nepoch 139: loss 1.0036283565892115\nepoch 140: loss 0.9830687410301633\nepoch 141: loss 0.973314086596171\nepoch 142: loss 0.9921116232872009\nepoch 143: loss 0.9962259232997894\nepoch 144: loss 0.97552090883255\nepoch 145: loss 0.9807691077391306\nepoch 146: loss 1.0016797681649525\nepoch 147: loss 0.9701012406084273\nepoch 148: loss 0.9847355352507697\nepoch 149: loss 1.0079118112723033\nepoch 150: loss 0.984410262770123\nepoch 151: loss 0.9768550362851884\nepoch 152: loss 0.9925394687387679\nepoch 153: loss 0.9934237632486556\nepoch 154: loss 0.9948908256159888\nepoch 155: loss 0.9798293974664476\nepoch 156: loss 0.9869266880883111\nepoch 157: loss 0.9657246602906121\nepoch 158: loss 0.9862294230196211\nepoch 159: loss 0.9670793612798055\nepoch 160: loss 0.9546366433302561\nepoch 161: loss 1.090085357427597\nepoch 162: loss 1.0338688128524356\nepoch 163: loss 0.9852095544338226\nepoch 164: loss 0.9724307523833381\nepoch 165: loss 1.0271649228201971\nepoch 166: loss 0.9859315388732486\nepoch 167: loss 0.9870808455679152\nepoch 168: loss 0.9829074177477095\nepoch 169: loss 0.9796078701814016\nepoch 170: loss 0.9765292406082153\nepoch 171: loss 1.0084395839108362\nepoch 172: loss 0.9688159492280748\nepoch 173: loss 0.9622737103038363\nepoch 174: loss 0.993135627773073\nepoch 175: loss 0.9596685965855917\nepoch 176: loss 0.9657837715413835\nepoch 177: loss 0.9892585906717513\nepoch 178: loss 0.9738571345806122\nepoch 179: loss 0.9579844971497854\nepoch 180: loss 0.9799589150481753\nepoch 181: loss 1.0239534444279141\nepoch 182: loss 0.9625405404302809\nepoch 183: loss 0.9889433185259501\nepoch 184: loss 0.9971031745274862\nepoch 185: loss 0.984661877155304\nepoch 186: loss 0.9411121441258324\nepoch 187: loss 0.9926096366511451\nepoch 188: loss 0.984059625201755\nepoch 189: loss 0.9524499939547645\nepoch 190: loss 0.9831621746222178\nepoch 191: loss 0.9570663703812493\nepoch 192: loss 0.9726736048857371\nepoch 193: loss 1.029220524761412\nepoch 194: loss 1.0296267171700795\nepoch 195: loss 1.0303357011742063\nepoch 196: loss 1.015145457453198\nepoch 197: loss 1.0043697953224182\nepoch 198: loss 0.9973857833279504\nepoch 199: loss 0.9982444743315378\n\n\n\nplt.figure(figsize=(10,6))\nsns.lineplot(epoch_loss)\nplt.xlabel(\"epoch (1-200)\")\nplt.ylabel(\"cross-entropy loss\")\nplt.title(\"cross-entropy loss over 200 epochs\")\nplt.show()"
  },
  {
    "objectID": "posts/pytorch_autograd/index.html",
    "href": "posts/pytorch_autograd/index.html",
    "title": "PyTorch tutorial: autograd demo",
    "section": "",
    "text": "original tutorial link\ntorch.autograd: automatic differentiation engine that powers NN training\nBackground\n\nNeural networks (NNs): collection of nested functions executed on some input data\nfns defined by parameters (weights+biases), which are stored in tensors\n2 steps in training NN:\n\nforward prop: NN makes best guess about correct output; runs input data through each of its functions to make guess\nbackward prop: NN adjusts parameters proportion to error in its guess by traversing backwards from output, collecting derivatives of error w/ respect to parameters of fns and optimizing parameters using gradient descent\n\n\nUsage in PyTorch\n\nExample: a single training step\n\n\nCreate random data tensor\n\nrandom data tensor represents single image w/ 3 channels, and height+width of 64\n\nlabel initialized to some random values\nlabel in pretrained models has shape (1,1000)\n\n\n\nimport torch\nfrom torchvision.models import resnet18, ResNet18_Weights\nmodel = resnet18(weights=ResNet18_Weights.DEFAULT)\ndata = torch.rand(1, 3, 64, 64)\nlabels = torch.rand(1, 1000)\n\nForward pass: run input data through model through each of its layers to make a prediction\n\nprediction = model(data)\n\nUse prediction+corresponding label to calculate error (loss). Backprop by calling .backward() on the error tensor.\n\nautograd calculates+Stores gradients for each model parameter in parameter’s .grad attribute\n\n\nloss = (prediction - labels).sum()\nloss.backward() # backward pass\n\nLoad optimizer; e.g. SGD w/ a learning rate of 0.01 and momentum of 0.9. Register all parameters of model in optimizer.\n\noptim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n\nInitiate gradient descent with .step().\n\noptimizer adjusts each parameter its gradient stored in .grad.\n\n\noptim.step()\nDifferentiation in Autograd\n\nhow does autograd collect gradients?\n\n\nCreate 2 tensors a and b with requires_grad=True to signal that every operation on them should be tracked.\n\nimport torch\n\na = torch.tensor([2., 3.], requires_grad=True)\nb = torch.tensor([6., 4.], requires_grad=True)\n\nCreate another tensor Q from a and b.\n\\[\nQ=3a^3-b^2\n\\]\n\nQ = 3*a**3 - b**2\n\nAssume a and b to be parameters of a NN, Q to be error. When training NN, want gradients of error wrt parameters:\n\\[\\begin{align*}\n\\frac{\\partial{Q}}{\\partial{a}} &= 9a^2\\\\\n\\frac{\\partial{Q}}{\\partial{b}} &= -2b\n\n\\end{align*}\\]\n\nautograd calculate gradients when we call .backward() on Q, storing in respective tensors’ .grad attribute.\nneed to explicitly pass a gradient argument in Q.backward() because it is a vector\ngradient is a tensor of same shape as Q and represents gradient of Q wrt itself; i.e.\n\\[\n  \\frac{dQ}{dQ} = 1\n  \\]\nequivalently can also aggregate Q into scalar and call backward implicitly like Q.sum().backward().\n\n\nexternal_grad = torch.tensor([1., 1.])\nQ.backward(gradient=external_grad)\n\nGradients now deposited in a.grad and b.grad:\n\n# check if collected gradients are correct\nprint(9*a**2 == a.grad)\nprint(-2*b == b.grad)\nVector calculus using autograd\nGiven vector valued function \\(\\vec{y} = f(\\vec{x})\\), \\(\\nabla \\vec{y}\\) wrt \\(\\vec{x}\\) is Jacobian matrix \\(J\\)\n\\[\n\\begin{align*}\nJ &= \\begin{pmatrix}\n\\frac{\\partial \\mathbf{y}}{\\partial x_1} & \\dots & \\frac{\\partial \\mathbf{y}}{x_n}\n\\end{pmatrix}\\\\\n&= \\begin{pmatrix}\n\\frac{\\partial y_1}{\\partial x_1} & \\dots & \\frac{\\partial y_1}{x_n}\\\\\n\\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial {y_m}}{\\partial x_1} & \\dots & \\frac{\\partial y_m}{x_n}\n\\end{pmatrix}\n\\end{align*}\n\\]\n\ntorch.autograd computes vector-Jacobian product → given any vector \\(\\vec{v}\\), compute the product \\(J^{T}\\cdot \\vec{v}\\)\n\nIf \\(\\vec{v}\\) is gradient of scalar function \\(l=g(\\vec{y})\\):\n\\[\n\\vec{v} = \\begin{pmatrix}\n\\frac{\\partial l}{\\partial y_1} & \\dots & \\frac{\\partial l}{\\partial y_m}\n\\end{pmatrix}^T\n\\]\nthen by the chain rule, vector-Jacobian product would be gradient of \\(l\\) wrt \\(\\vec{x}\\):\n\\[\nJ^T\\cdot \\vec{v} =\n\\begin{pmatrix}\n\\frac{\\partial y_1}{\\partial x_1} & \\dots & \\frac{\\partial y_m}{x_n}\\\\\n\\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial {y_m}}{\\partial x_1} & \\dots & \\frac{\\partial y_m}{x_n}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{\\partial l}{\\partial y_1}\\\\\n\\vdots \\\\\n\\frac{\\partial l}{\\partial y_m}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{\\partial l}{\\partial x_1}\\\\\n\\vdots\\\\\n\\frac{\\partial l}{\\partial x_n}\n\\end{pmatrix}\n\\]\ncharacteristic of vector-Jacobian product is what we use in the above example; external_grad = \\(\\vec{v}\\)\nComputational Graph\nautograd keeps record of data (tensors) and all executed operations (along w/ resulting new tensors) in a DAG consisting of Function objects\n\nleaves = input tensors; roots are output tensors\ntrace graph from roots to leaves → automatically compute gradients using chain rule\n\nForward pass: autograd does 2 things simulatneously:\n\nrun requested oepration to compute resulting tensor\nmaintain operation’s gradient function in DAG\n\nBackward pass when .backward() called on DAG root. autograd then:\n\ncomputes gradients from each .grad_fn\naccumulates them in respective tensor’s .grad attribute\nusing chain rule, propagates all the way to leaf tensors\n\n\n\n\nhttps://pytorch.org/tutorials/_images/dag_autograd.png\n\n\n\nleaf nodes in blue = a and b\narrows: in direction of forward pass\nnodes: backward functions of each operation in forward pass\n\nExclusion from the DAG\n\ntorch.autograd tracks operations on all tensors which have their requires_grad flag set to True\ntensors that don’t require gradients → set attribute to False → exclude from gradient computation DAG\noutput tensor of operation will require gradients even if only a single input tensor has requires_grad=True\n\nx = torch.rand(5, 5)\ny = torch.rand(5, 5)\nz = torch.rand((5, 5), requires_grad=True)\n\na = x + y\nprint(f\"Does `a` require gradients? : {a.requires_grad}\")\n#false\n\nb = x + z\nprint(f\"Does `b` require gradients?: {b.requires_grad}\")\n#true\nfrozen parameters: parameters that don’t compute gradients\n\nuseful to freeze part of model if you know beforehand you don’t need gradients of those parameters (reduces autograd computations)\nin finetuning, freeze most of model+only modify classifier layers to make predictions on new labels\n\nfrom torch import nn, optim\n\nmodel = resnet18(weights=ResNet18_Weights.DEFAULT)\n\n# Freeze all the parameters in the network\nfor param in model.parameters():\n    param.requires_grad = False\nEx. finetune model on new dataset w/ 10 label\n\nin resnet, classifier is last linear layer model.fc\ncan replace it with new linear layer that acts as our classifier (unfrozen by default)\n\nmodel.fc = nn.Linear(512, 10)\nNow all parameters besides model.fc parameters are frozen; only weights and bias of model.fc compute gradients\n# Optimize only the classifier\noptimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n\nhere, we register all parameters in optimizer but only weights and bias of classifier end up computing gradients (and are updated in gradient descent)\nsame exclusionary functionality available as context manager in torch.no_grad()"
  },
  {
    "objectID": "posts/pp_train_test_data/index.html",
    "href": "posts/pp_train_test_data/index.html",
    "title": "Preprocessing, training, testing data",
    "section": "",
    "text": "fig 1\n\n\n\n\n\nfig 2\n\n\n\n\n\nfig 3"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tiffanie lab blog",
    "section": "",
    "text": "Preprocessing, training, testing data\n\n\n\n\n\n\n\npresentations\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nEnformer paper summary\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nW3 thesis updates\n\n\n\n\n\n\n\npresentations\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nWine dataset MLP example\n\n\n\n\n\n\n\npractice\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch tutorial: NN demo\n\n\n\n\n\n\n\ndemos\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nNeural networks (3b1b)\n\n\n\n\n\n\n\ndemos\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch tutorial: training classifier demo\n\n\n\n\n\n\n\ndemos\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch tutorial: autograd demo\n\n\n\n\n\n\n\ndemos\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nTranscriptome QGT lab\n\n\n\n\n\n\n\ndemos\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n  \n\n\n\n\nFunctional annotation of GWAS loci using transcriptome data\n\n\n\n\n\n\n\npresentations\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nPopulation structure\n\n\n\n\n\n\n\ndemos\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nConducting GWAS studies summary\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nMatrix algebra basics (numpy) - part 1\n\n\n\n\n\n\n\npractice\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch tutorial: tensor demo\n\n\n\n\n\n\n\ndemos\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nLLM in molecular biology summary\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\ntiffanie\n\n\n\n\n\n\n  \n\n\n\n\nW1 Thesis updates\n\n\n\n\n\n\n\npresentations\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\ntiffanie\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]