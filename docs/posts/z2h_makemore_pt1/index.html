<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.296">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="tiffanie">
<meta name="dcterms.date" content="2023-06-30">

<title>tiffanie lab blog - z2h 2. Intro to language modeling: building makemore</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">tiffanie lab blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../w1.html" rel="" target="">
 <span class="menu-text">w1</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../w2.html" rel="" target="">
 <span class="menu-text">w2</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../w3.html" rel="" target="">
 <span class="menu-text">w3</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../w4.html" rel="" target="">
 <span class="menu-text">w4</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../w5.html" rel="" target="">
 <span class="menu-text">w5</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/tiffanieh" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">z2h 2. Intro to language modeling: building makemore</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">demos</div>
                <div class="quarto-category">w4</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>tiffanie </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 30, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> Generator</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>makemore: autoregressive (using past data to predict future behavior) character-level language model that “makes more” of things that you give it</p>
<p><strong>Example:</strong> can learn to make more names (or things that sound name-like) if you train makemore on a dataset of names:</p>
<p>https://github.com/karpathy/makemore/blob/master/names.txt</p>
<ul>
<li>each line is an example (e.g.&nbsp;“iris”)</li>
<li>within each example, treat it as a sequence of individual characters (the letters in “iris”)</li>
</ul>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#read in everything into one giant string -&gt; splitlines to get a list of strings</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> <span class="bu">open</span>(<span class="st">'../z2h_data/names.txt'</span>, <span class="st">'r'</span>).read().splitlines()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>words[:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>['emma',
 'olivia',
 'ava',
 'isabella',
 'sophia',
 'charlotte',
 'mia',
 'amelia',
 'harper',
 'evelyn']</code></pre>
</div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(words) <span class="co">#around 32k names</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>32033</code></pre>
</div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">min</span>(<span class="bu">len</span>(word) <span class="cf">for</span> word <span class="kw">in</span> words) <span class="co">#shortest is 2 characters</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>2</code></pre>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="bu">max</span>(<span class="bu">len</span>(word) <span class="cf">for</span> word <span class="kw">in</span> words) <span class="co">#longest: 15</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>15</code></pre>
</div>
</div>
<p><strong>Character-level language model:</strong> predicting next character in sequence given already some concrete sequence before</p>
<p>What does the existence of a word (e.g.&nbsp;“isabella”) in this dataset tell us?</p>
<ul>
<li>the character “i” is a very likely character to come first in sequence of a name</li>
<li>character “s” is likely to come after “i”</li>
<li>…“a” after “is”</li>
<li>…“b” after “isa”</li>
<li>…</li>
<li>one more example: after “isabella” the word is likely to end</li>
</ul>
<p>*a lot of info packed into this statistical structure of what’s likely to followy in character sequence!</p>
<p>Start by building <strong>bigram language model:</strong> works only w/ <strong>2</strong> characters at a time</p>
<ul>
<li>only looking at one character (given)</li>
<li>try to predict next character in sequence</li>
<li>very small local structure (just looking at previous to predict next)</li>
</ul>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">#ex</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> words[:<span class="dv">1</span>]:</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">#iterate 2 characters at a time -- slide across (zip will halt+return when shorter list is exhausted)</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(word, word[<span class="dv">1</span>:]):</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(ch1, ch2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>e m
m m
m a</code></pre>
</div>
</div>
<p>What we see from this above word: “e” is very likely to come first; “a” very likely to come last</p>
<ul>
<li><code>zip</code> will halt and return when we exhaust the shorter list</li>
<li>need to find a way to mark beginning and end so that everything is fully “paired”</li>
</ul>
<p>Printed below: we start with the bigram of the special start character and “e”, end with the bigram of “a” an the special end character</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> words[:<span class="dv">1</span>]:</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">#special array of characters</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">#hallucinate special start token</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">#list(w) to get list of individual characters instead of just a string of whole word</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    chs <span class="op">=</span> [<span class="st">'&lt;S&gt;'</span>] <span class="op">+</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'&lt;E&gt;'</span>]</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(ch1, ch2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;S&gt; e
e m
m m
m a
a &lt;E&gt;</code></pre>
</div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">#now with more words</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> words[:<span class="dv">3</span>]:</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    chs <span class="op">=</span> [<span class="st">'&lt;S&gt;'</span>] <span class="op">+</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'&lt;E&gt;'</span>]</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(ch1, ch2) <span class="co">#print individual character bigrams</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;S&gt; e
e m
m m
m a
a &lt;E&gt;
&lt;S&gt; o
o l
l i
i v
v i
i a
a &lt;E&gt;
&lt;S&gt; a
a v
v a
a &lt;E&gt;</code></pre>
</div>
</div>
<p>To learn statistics about which characters are likely to follow other characters, simplest way is to count. i.e.&nbsp;count how often any one of these combinations occurs in training set in these words</p>
<ul>
<li>need dictionary to maintain counts to map bigrams (tuple of character 1, character 2) to their frequency</li>
</ul>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>bigram_counts <span class="op">=</span> {}</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> words:</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    chs <span class="op">=</span> [<span class="st">'&lt;S&gt;'</span>] <span class="op">+</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'&lt;E&gt;'</span>]</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        bigram <span class="op">=</span> (ch1, ch2)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>        bigram_counts[bigram] <span class="op">=</span> bigram_counts.get(bigram, <span class="dv">0</span>)<span class="op">+</span><span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">#alternative method (not from demo)</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sort_dict(unsorted_dict):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""sort dictionary using swapped keys and values"""</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    unsorted_list <span class="op">=</span> [(value, key) <span class="cf">for</span> (key, value) <span class="kw">in</span> unsorted_dict.items()]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    sorted_list <span class="op">=</span> <span class="bu">sorted</span>(unsorted_list, reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    sorted_dict <span class="op">=</span> {key:value <span class="cf">for</span> value, key <span class="kw">in</span> sorted_list}</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sorted_dict</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="bu">sorted</span>(bigram_counts.items(), key <span class="op">=</span> <span class="kw">lambda</span> kv: <span class="op">-</span>kv[<span class="dv">1</span>]) <span class="co">#list with entries from most frequent to least</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>[(('n', '&lt;E&gt;'), 6763),
 (('a', '&lt;E&gt;'), 6640),
 (('a', 'n'), 5438),
 (('&lt;S&gt;', 'a'), 4410),
 (('e', '&lt;E&gt;'), 3983),
 (('a', 'r'), 3264),
 (('e', 'l'), 3248),
 (('r', 'i'), 3033),
 (('n', 'a'), 2977),
 (('&lt;S&gt;', 'k'), 2963),
 (('l', 'e'), 2921),
 (('e', 'n'), 2675),
 (('l', 'a'), 2623),
 (('m', 'a'), 2590),
 (('&lt;S&gt;', 'm'), 2538),
 (('a', 'l'), 2528),
 (('i', '&lt;E&gt;'), 2489),
 (('l', 'i'), 2480),
 (('i', 'a'), 2445),
 (('&lt;S&gt;', 'j'), 2422),
 (('o', 'n'), 2411),
 (('h', '&lt;E&gt;'), 2409),
 (('r', 'a'), 2356),
 (('a', 'h'), 2332),
 (('h', 'a'), 2244),
 (('y', 'a'), 2143),
 (('i', 'n'), 2126),
 (('&lt;S&gt;', 's'), 2055),
 (('a', 'y'), 2050),
 (('y', '&lt;E&gt;'), 2007),
 (('e', 'r'), 1958),
 (('n', 'n'), 1906),
 (('y', 'n'), 1826),
 (('k', 'a'), 1731),
 (('n', 'i'), 1725),
 (('r', 'e'), 1697),
 (('&lt;S&gt;', 'd'), 1690),
 (('i', 'e'), 1653),
 (('a', 'i'), 1650),
 (('&lt;S&gt;', 'r'), 1639),
 (('a', 'm'), 1634),
 (('l', 'y'), 1588),
 (('&lt;S&gt;', 'l'), 1572),
 (('&lt;S&gt;', 'c'), 1542),
 (('&lt;S&gt;', 'e'), 1531),
 (('j', 'a'), 1473),
 (('r', '&lt;E&gt;'), 1377),
 (('n', 'e'), 1359),
 (('l', 'l'), 1345),
 (('i', 'l'), 1345),
 (('i', 's'), 1316),
 (('l', '&lt;E&gt;'), 1314),
 (('&lt;S&gt;', 't'), 1308),
 (('&lt;S&gt;', 'b'), 1306),
 (('d', 'a'), 1303),
 (('s', 'h'), 1285),
 (('d', 'e'), 1283),
 (('e', 'e'), 1271),
 (('m', 'i'), 1256),
 (('s', 'a'), 1201),
 (('s', '&lt;E&gt;'), 1169),
 (('&lt;S&gt;', 'n'), 1146),
 (('a', 's'), 1118),
 (('y', 'l'), 1104),
 (('e', 'y'), 1070),
 (('o', 'r'), 1059),
 (('a', 'd'), 1042),
 (('t', 'a'), 1027),
 (('&lt;S&gt;', 'z'), 929),
 (('v', 'i'), 911),
 (('k', 'e'), 895),
 (('s', 'e'), 884),
 (('&lt;S&gt;', 'h'), 874),
 (('r', 'o'), 869),
 (('e', 's'), 861),
 (('z', 'a'), 860),
 (('o', '&lt;E&gt;'), 855),
 (('i', 'r'), 849),
 (('b', 'r'), 842),
 (('a', 'v'), 834),
 (('m', 'e'), 818),
 (('e', 'i'), 818),
 (('c', 'a'), 815),
 (('i', 'y'), 779),
 (('r', 'y'), 773),
 (('e', 'm'), 769),
 (('s', 't'), 765),
 (('h', 'i'), 729),
 (('t', 'e'), 716),
 (('n', 'd'), 704),
 (('l', 'o'), 692),
 (('a', 'e'), 692),
 (('a', 't'), 687),
 (('s', 'i'), 684),
 (('e', 'a'), 679),
 (('d', 'i'), 674),
 (('h', 'e'), 674),
 (('&lt;S&gt;', 'g'), 669),
 (('t', 'o'), 667),
 (('c', 'h'), 664),
 (('b', 'e'), 655),
 (('t', 'h'), 647),
 (('v', 'a'), 642),
 (('o', 'l'), 619),
 (('&lt;S&gt;', 'i'), 591),
 (('i', 'o'), 588),
 (('e', 't'), 580),
 (('v', 'e'), 568),
 (('a', 'k'), 568),
 (('a', 'a'), 556),
 (('c', 'e'), 551),
 (('a', 'b'), 541),
 (('i', 't'), 541),
 (('&lt;S&gt;', 'y'), 535),
 (('t', 'i'), 532),
 (('s', 'o'), 531),
 (('m', '&lt;E&gt;'), 516),
 (('d', '&lt;E&gt;'), 516),
 (('&lt;S&gt;', 'p'), 515),
 (('i', 'c'), 509),
 (('k', 'i'), 509),
 (('o', 's'), 504),
 (('n', 'o'), 496),
 (('t', '&lt;E&gt;'), 483),
 (('j', 'o'), 479),
 (('u', 's'), 474),
 (('a', 'c'), 470),
 (('n', 'y'), 465),
 (('e', 'v'), 463),
 (('s', 's'), 461),
 (('m', 'o'), 452),
 (('i', 'k'), 445),
 (('n', 't'), 443),
 (('i', 'd'), 440),
 (('j', 'e'), 440),
 (('a', 'z'), 435),
 (('i', 'g'), 428),
 (('i', 'm'), 427),
 (('r', 'r'), 425),
 (('d', 'r'), 424),
 (('&lt;S&gt;', 'f'), 417),
 (('u', 'r'), 414),
 (('r', 'l'), 413),
 (('y', 's'), 401),
 (('&lt;S&gt;', 'o'), 394),
 (('e', 'd'), 384),
 (('a', 'u'), 381),
 (('c', 'o'), 380),
 (('k', 'y'), 379),
 (('d', 'o'), 378),
 (('&lt;S&gt;', 'v'), 376),
 (('t', 't'), 374),
 (('z', 'e'), 373),
 (('z', 'i'), 364),
 (('k', '&lt;E&gt;'), 363),
 (('g', 'h'), 360),
 (('t', 'r'), 352),
 (('k', 'o'), 344),
 (('t', 'y'), 341),
 (('g', 'e'), 334),
 (('g', 'a'), 330),
 (('l', 'u'), 324),
 (('b', 'a'), 321),
 (('d', 'y'), 317),
 (('c', 'k'), 316),
 (('&lt;S&gt;', 'w'), 307),
 (('k', 'h'), 307),
 (('u', 'l'), 301),
 (('y', 'e'), 301),
 (('y', 'r'), 291),
 (('m', 'y'), 287),
 (('h', 'o'), 287),
 (('w', 'a'), 280),
 (('s', 'l'), 279),
 (('n', 's'), 278),
 (('i', 'z'), 277),
 (('u', 'n'), 275),
 (('o', 'u'), 275),
 (('n', 'g'), 273),
 (('y', 'd'), 272),
 (('c', 'i'), 271),
 (('y', 'o'), 271),
 (('i', 'v'), 269),
 (('e', 'o'), 269),
 (('o', 'm'), 261),
 (('r', 'u'), 252),
 (('f', 'a'), 242),
 (('b', 'i'), 217),
 (('s', 'y'), 215),
 (('n', 'c'), 213),
 (('h', 'y'), 213),
 (('p', 'a'), 209),
 (('r', 't'), 208),
 (('q', 'u'), 206),
 (('p', 'h'), 204),
 (('h', 'r'), 204),
 (('j', 'u'), 202),
 (('g', 'r'), 201),
 (('p', 'e'), 197),
 (('n', 'l'), 195),
 (('y', 'i'), 192),
 (('g', 'i'), 190),
 (('o', 'd'), 190),
 (('r', 's'), 190),
 (('r', 'd'), 187),
 (('h', 'l'), 185),
 (('s', 'u'), 185),
 (('a', 'x'), 182),
 (('e', 'z'), 181),
 (('e', 'k'), 178),
 (('o', 'v'), 176),
 (('a', 'j'), 175),
 (('o', 'h'), 171),
 (('u', 'e'), 169),
 (('m', 'm'), 168),
 (('a', 'g'), 168),
 (('h', 'u'), 166),
 (('x', '&lt;E&gt;'), 164),
 (('u', 'a'), 163),
 (('r', 'm'), 162),
 (('a', 'w'), 161),
 (('f', 'i'), 160),
 (('z', '&lt;E&gt;'), 160),
 (('u', '&lt;E&gt;'), 155),
 (('u', 'm'), 154),
 (('e', 'c'), 153),
 (('v', 'o'), 153),
 (('e', 'h'), 152),
 (('p', 'r'), 151),
 (('d', 'd'), 149),
 (('o', 'a'), 149),
 (('w', 'e'), 149),
 (('w', 'i'), 148),
 (('y', 'm'), 148),
 (('z', 'y'), 147),
 (('n', 'z'), 145),
 (('y', 'u'), 141),
 (('r', 'n'), 140),
 (('o', 'b'), 140),
 (('k', 'l'), 139),
 (('m', 'u'), 139),
 (('l', 'd'), 138),
 (('h', 'n'), 138),
 (('u', 'd'), 136),
 (('&lt;S&gt;', 'x'), 134),
 (('t', 'l'), 134),
 (('a', 'f'), 134),
 (('o', 'e'), 132),
 (('e', 'x'), 132),
 (('e', 'g'), 125),
 (('f', 'e'), 123),
 (('z', 'l'), 123),
 (('u', 'i'), 121),
 (('v', 'y'), 121),
 (('e', 'b'), 121),
 (('r', 'h'), 121),
 (('j', 'i'), 119),
 (('o', 't'), 118),
 (('d', 'h'), 118),
 (('h', 'm'), 117),
 (('c', 'l'), 116),
 (('o', 'o'), 115),
 (('y', 'c'), 115),
 (('o', 'w'), 114),
 (('o', 'c'), 114),
 (('f', 'r'), 114),
 (('b', '&lt;E&gt;'), 114),
 (('m', 'b'), 112),
 (('z', 'o'), 110),
 (('i', 'b'), 110),
 (('i', 'u'), 109),
 (('k', 'r'), 109),
 (('g', '&lt;E&gt;'), 108),
 (('y', 'v'), 106),
 (('t', 'z'), 105),
 (('b', 'o'), 105),
 (('c', 'y'), 104),
 (('y', 't'), 104),
 (('u', 'b'), 103),
 (('u', 'c'), 103),
 (('x', 'a'), 103),
 (('b', 'l'), 103),
 (('o', 'y'), 103),
 (('x', 'i'), 102),
 (('i', 'f'), 101),
 (('r', 'c'), 99),
 (('c', '&lt;E&gt;'), 97),
 (('m', 'r'), 97),
 (('n', 'u'), 96),
 (('o', 'p'), 95),
 (('i', 'h'), 95),
 (('k', 's'), 95),
 (('l', 's'), 94),
 (('u', 'k'), 93),
 (('&lt;S&gt;', 'q'), 92),
 (('d', 'u'), 92),
 (('s', 'm'), 90),
 (('r', 'k'), 90),
 (('i', 'x'), 89),
 (('v', '&lt;E&gt;'), 88),
 (('y', 'k'), 86),
 (('u', 'w'), 86),
 (('g', 'u'), 85),
 (('b', 'y'), 83),
 (('e', 'p'), 83),
 (('g', 'o'), 83),
 (('s', 'k'), 82),
 (('u', 't'), 82),
 (('a', 'p'), 82),
 (('e', 'f'), 82),
 (('i', 'i'), 82),
 (('r', 'v'), 80),
 (('f', '&lt;E&gt;'), 80),
 (('t', 'u'), 78),
 (('y', 'z'), 78),
 (('&lt;S&gt;', 'u'), 78),
 (('l', 't'), 77),
 (('r', 'g'), 76),
 (('c', 'r'), 76),
 (('i', 'j'), 76),
 (('w', 'y'), 73),
 (('z', 'u'), 73),
 (('l', 'v'), 72),
 (('h', 't'), 71),
 (('j', '&lt;E&gt;'), 71),
 (('x', 't'), 70),
 (('o', 'i'), 69),
 (('e', 'u'), 69),
 (('o', 'k'), 68),
 (('b', 'd'), 65),
 (('a', 'o'), 63),
 (('p', 'i'), 61),
 (('s', 'c'), 60),
 (('d', 'l'), 60),
 (('l', 'm'), 60),
 (('a', 'q'), 60),
 (('f', 'o'), 60),
 (('p', 'o'), 59),
 (('n', 'k'), 58),
 (('w', 'n'), 58),
 (('u', 'h'), 58),
 (('e', 'j'), 55),
 (('n', 'v'), 55),
 (('s', 'r'), 55),
 (('o', 'z'), 54),
 (('i', 'p'), 53),
 (('l', 'b'), 52),
 (('i', 'q'), 52),
 (('w', '&lt;E&gt;'), 51),
 (('m', 'c'), 51),
 (('s', 'p'), 51),
 (('e', 'w'), 50),
 (('k', 'u'), 50),
 (('v', 'r'), 48),
 (('u', 'g'), 47),
 (('o', 'x'), 45),
 (('u', 'z'), 45),
 (('z', 'z'), 45),
 (('j', 'h'), 45),
 (('b', 'u'), 45),
 (('o', 'g'), 44),
 (('n', 'r'), 44),
 (('f', 'f'), 44),
 (('n', 'j'), 44),
 (('z', 'h'), 43),
 (('c', 'c'), 42),
 (('r', 'b'), 41),
 (('x', 'o'), 41),
 (('b', 'h'), 41),
 (('p', 'p'), 39),
 (('x', 'l'), 39),
 (('h', 'v'), 39),
 (('b', 'b'), 38),
 (('m', 'p'), 38),
 (('x', 'x'), 38),
 (('u', 'v'), 37),
 (('x', 'e'), 36),
 (('w', 'o'), 36),
 (('c', 't'), 35),
 (('z', 'm'), 35),
 (('t', 's'), 35),
 (('m', 's'), 35),
 (('c', 'u'), 35),
 (('o', 'f'), 34),
 (('u', 'x'), 34),
 (('k', 'w'), 34),
 (('p', '&lt;E&gt;'), 33),
 (('g', 'l'), 32),
 (('z', 'r'), 32),
 (('d', 'n'), 31),
 (('g', 't'), 31),
 (('g', 'y'), 31),
 (('h', 's'), 31),
 (('x', 's'), 31),
 (('g', 's'), 30),
 (('x', 'y'), 30),
 (('y', 'g'), 30),
 (('d', 'm'), 30),
 (('d', 's'), 29),
 (('h', 'k'), 29),
 (('y', 'x'), 28),
 (('q', '&lt;E&gt;'), 28),
 (('g', 'n'), 27),
 (('y', 'b'), 27),
 (('g', 'w'), 26),
 (('n', 'h'), 26),
 (('k', 'n'), 26),
 (('g', 'g'), 25),
 (('d', 'g'), 25),
 (('l', 'c'), 25),
 (('r', 'j'), 25),
 (('w', 'u'), 25),
 (('l', 'k'), 24),
 (('m', 'd'), 24),
 (('s', 'w'), 24),
 (('s', 'n'), 24),
 (('h', 'd'), 24),
 (('w', 'h'), 23),
 (('y', 'j'), 23),
 (('y', 'y'), 23),
 (('r', 'z'), 23),
 (('d', 'w'), 23),
 (('w', 'r'), 22),
 (('t', 'n'), 22),
 (('l', 'f'), 22),
 (('y', 'h'), 22),
 (('r', 'w'), 21),
 (('s', 'b'), 21),
 (('m', 'n'), 20),
 (('f', 'l'), 20),
 (('w', 's'), 20),
 (('k', 'k'), 20),
 (('h', 'z'), 20),
 (('g', 'd'), 19),
 (('l', 'h'), 19),
 (('n', 'm'), 19),
 (('x', 'z'), 19),
 (('u', 'f'), 19),
 (('f', 't'), 18),
 (('l', 'r'), 18),
 (('p', 't'), 17),
 (('t', 'c'), 17),
 (('k', 't'), 17),
 (('d', 'v'), 17),
 (('u', 'p'), 16),
 (('p', 'l'), 16),
 (('l', 'w'), 16),
 (('p', 's'), 16),
 (('o', 'j'), 16),
 (('r', 'q'), 16),
 (('y', 'p'), 15),
 (('l', 'p'), 15),
 (('t', 'v'), 15),
 (('r', 'p'), 14),
 (('l', 'n'), 14),
 (('e', 'q'), 14),
 (('f', 'y'), 14),
 (('s', 'v'), 14),
 (('u', 'j'), 14),
 (('v', 'l'), 14),
 (('q', 'a'), 13),
 (('u', 'y'), 13),
 (('q', 'i'), 13),
 (('w', 'l'), 13),
 (('p', 'y'), 12),
 (('y', 'f'), 12),
 (('c', 'q'), 11),
 (('j', 'r'), 11),
 (('n', 'w'), 11),
 (('n', 'f'), 11),
 (('t', 'w'), 11),
 (('m', 'z'), 11),
 (('u', 'o'), 10),
 (('f', 'u'), 10),
 (('l', 'z'), 10),
 (('h', 'w'), 10),
 (('u', 'q'), 10),
 (('j', 'y'), 10),
 (('s', 'z'), 10),
 (('s', 'd'), 9),
 (('j', 'l'), 9),
 (('d', 'j'), 9),
 (('k', 'm'), 9),
 (('r', 'f'), 9),
 (('h', 'j'), 9),
 (('v', 'n'), 8),
 (('n', 'b'), 8),
 (('i', 'w'), 8),
 (('h', 'b'), 8),
 (('b', 's'), 8),
 (('w', 't'), 8),
 (('w', 'd'), 8),
 (('v', 'v'), 7),
 (('v', 'u'), 7),
 (('j', 's'), 7),
 (('m', 'j'), 7),
 (('f', 's'), 6),
 (('l', 'g'), 6),
 (('l', 'j'), 6),
 (('j', 'w'), 6),
 (('n', 'x'), 6),
 (('y', 'q'), 6),
 (('w', 'k'), 6),
 (('g', 'm'), 6),
 (('x', 'u'), 5),
 (('m', 'h'), 5),
 (('m', 'l'), 5),
 (('j', 'm'), 5),
 (('c', 's'), 5),
 (('j', 'v'), 5),
 (('n', 'p'), 5),
 (('d', 'f'), 5),
 (('x', 'd'), 5),
 (('z', 'b'), 4),
 (('f', 'n'), 4),
 (('x', 'c'), 4),
 (('m', 't'), 4),
 (('t', 'm'), 4),
 (('z', 'n'), 4),
 (('z', 't'), 4),
 (('p', 'u'), 4),
 (('c', 'z'), 4),
 (('b', 'n'), 4),
 (('z', 's'), 4),
 (('f', 'w'), 4),
 (('d', 't'), 4),
 (('j', 'd'), 4),
 (('j', 'c'), 4),
 (('y', 'w'), 4),
 (('v', 'k'), 3),
 (('x', 'w'), 3),
 (('t', 'j'), 3),
 (('c', 'j'), 3),
 (('q', 'w'), 3),
 (('g', 'b'), 3),
 (('o', 'q'), 3),
 (('r', 'x'), 3),
 (('d', 'c'), 3),
 (('g', 'j'), 3),
 (('x', 'f'), 3),
 (('z', 'w'), 3),
 (('d', 'k'), 3),
 (('u', 'u'), 3),
 (('m', 'v'), 3),
 (('c', 'x'), 3),
 (('l', 'q'), 3),
 (('p', 'b'), 2),
 (('t', 'g'), 2),
 (('q', 's'), 2),
 (('t', 'x'), 2),
 (('f', 'k'), 2),
 (('b', 't'), 2),
 (('j', 'n'), 2),
 (('k', 'c'), 2),
 (('z', 'k'), 2),
 (('s', 'j'), 2),
 (('s', 'f'), 2),
 (('z', 'j'), 2),
 (('n', 'q'), 2),
 (('f', 'z'), 2),
 (('h', 'g'), 2),
 (('w', 'w'), 2),
 (('k', 'j'), 2),
 (('j', 'k'), 2),
 (('w', 'm'), 2),
 (('z', 'c'), 2),
 (('z', 'v'), 2),
 (('w', 'f'), 2),
 (('q', 'm'), 2),
 (('k', 'z'), 2),
 (('j', 'j'), 2),
 (('z', 'p'), 2),
 (('j', 't'), 2),
 (('k', 'b'), 2),
 (('m', 'w'), 2),
 (('h', 'f'), 2),
 (('c', 'g'), 2),
 (('t', 'f'), 2),
 (('h', 'c'), 2),
 (('q', 'o'), 2),
 (('k', 'd'), 2),
 (('k', 'v'), 2),
 (('s', 'g'), 2),
 (('z', 'd'), 2),
 (('q', 'r'), 1),
 (('d', 'z'), 1),
 (('p', 'j'), 1),
 (('q', 'l'), 1),
 (('p', 'f'), 1),
 (('q', 'e'), 1),
 (('b', 'c'), 1),
 (('c', 'd'), 1),
 (('m', 'f'), 1),
 (('p', 'n'), 1),
 (('w', 'b'), 1),
 (('p', 'c'), 1),
 (('h', 'p'), 1),
 (('f', 'h'), 1),
 (('b', 'j'), 1),
 (('f', 'g'), 1),
 (('z', 'g'), 1),
 (('c', 'p'), 1),
 (('p', 'k'), 1),
 (('p', 'm'), 1),
 (('x', 'n'), 1),
 (('s', 'q'), 1),
 (('k', 'f'), 1),
 (('m', 'k'), 1),
 (('x', 'h'), 1),
 (('g', 'f'), 1),
 (('v', 'b'), 1),
 (('j', 'p'), 1),
 (('g', 'z'), 1),
 (('v', 'd'), 1),
 (('d', 'b'), 1),
 (('v', 'h'), 1),
 (('h', 'h'), 1),
 (('g', 'v'), 1),
 (('d', 'q'), 1),
 (('x', 'b'), 1),
 (('w', 'z'), 1),
 (('h', 'q'), 1),
 (('j', 'b'), 1),
 (('x', 'm'), 1),
 (('w', 'g'), 1),
 (('t', 'b'), 1),
 (('z', 'x'), 1)]</code></pre>
</div>
</div>
<p>Want to store this info in <strong>2d array</strong>:</p>
<ul>
<li>rows: first character of bigram</li>
<li>columns: second character</li>
<li>each entry = how often that first character follows second character in this dataset</li>
</ul>
<p>can use Tensors w/ PyTorch</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co">#create a 3x5 tensor of zeros</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.zeros((<span class="dv">3</span>, <span class="dv">5</span>))</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>a.dtype</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>torch.float32</code></pre>
</div>
</div>
<p>By default, <code>a.dtype</code> (data type) is <code>.float32</code>. This can be changed:</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co">#32-bit integers</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.zeros((<span class="dv">3</span>, <span class="dv">5</span>), dtype<span class="op">=</span>torch.int32)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>tensor([[0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0]], dtype=torch.int32)</code></pre>
</div>
</div>
<p>Tensors allow us to easily manipulate any of the individual entries.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>a[<span class="dv">1</span>, <span class="dv">3</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>tensor([[0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0]], dtype=torch.int32)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>a[<span class="dv">1</span>, <span class="dv">3</span>] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>tensor([[0, 0, 0, 0, 0],
        [0, 0, 0, 2, 0],
        [0, 0, 0, 0, 0]], dtype=torch.int32)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>a[<span class="dv">0</span>, <span class="dv">0</span>] <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>tensor([[5, 0, 0, 0, 0],
        [0, 0, 0, 2, 0],
        [0, 0, 0, 0, 0]], dtype=torch.int32)</code></pre>
</div>
</div>
<p>26 letters in the alphabet+2 special characters (start/end) = 28</p>
<p>Need to be able to index integers for the characters – construct a character array:</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> torch.zeros((<span class="dv">28</span>, <span class="dv">28</span>), dtype<span class="op">=</span>torch.int32)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(<span class="st">''</span>.join(words))))</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co">#s (string) to I (integer) mapping</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>char_to_id <span class="op">=</span> {char:<span class="bu">id</span> <span class="cf">for</span> <span class="bu">id</span>, char <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co">#manually map special chars</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>char_to_id[<span class="st">'&lt;S&gt;'</span>] <span class="op">=</span> <span class="dv">26</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>char_to_id[<span class="st">'&lt;E&gt;'</span>] <span class="op">=</span> <span class="dv">27</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> words:</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    chs <span class="op">=</span> [<span class="st">'&lt;S&gt;'</span>] <span class="op">+</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'&lt;E&gt;'</span>]</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>        id1 <span class="op">=</span> char_to_id[ch1]</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>        id2 <span class="op">=</span> char_to_id[ch2]</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>        N[id1, id2] <span class="op">+=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>plt.imshow(N)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>&lt;matplotlib.image.AxesImage at 0x12f783010&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-23-output-2.png" class="quarto-discovered-preview-image img-fluid"></p>
</div>
</div>
<p>Above: a bit ugly</p>
<p>to make a nicer visualization, first reversing our mappings so we can reverse our array and label w/ characters we want</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>id_to_char <span class="op">=</span> {<span class="bu">id</span>:char <span class="cf">for</span> char, <span class="bu">id</span> <span class="kw">in</span> char_to_id.items()}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">16</span>))</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(N, cmap<span class="op">=</span><span class="st">'Blues'</span>)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">28</span>):</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">28</span>):</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>        chstr <span class="op">=</span> id_to_char[i] <span class="op">+</span> id_to_char[j] <span class="co">#i to j (char 1 to char 2)</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>        plt.text(j, i, chstr, ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"bottom"</span>, color<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">#.item() since we are indexing tensors and want individual integer</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        plt.text(j, i, N[i, j].item(), ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"top"</span>, color<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>(-0.5, 27.5, 27.5, -0.5)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-25-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>The issue here – we have an extra column and extra row since the start character will never be the second character of a bigram; end character will never be the first character of a bigram. The only exception if there is a word with no letters and the start char follows end char.</p>
<p>To remedy this:</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> torch.zeros((<span class="dv">27</span>, <span class="dv">27</span>), dtype<span class="op">=</span>torch.int32)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(<span class="st">''</span>.join(words))))</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>char_to_id <span class="op">=</span> {char:<span class="bu">id</span><span class="op">+</span><span class="dv">1</span> <span class="cf">for</span> <span class="bu">id</span>, char <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>char_to_id[<span class="st">'.'</span>] <span class="op">=</span> <span class="dv">0</span> <span class="co">#offset all other letters</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>id_to_char <span class="op">=</span> {<span class="bu">id</span>:char <span class="cf">for</span> char, <span class="bu">id</span> <span class="kw">in</span> char_to_id.items()}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> words:</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>  chs <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'.'</span>]</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    id1 <span class="op">=</span> char_to_id[ch1]</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    id2 <span class="op">=</span> char_to_id[ch2]</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    N[id1, id2] <span class="op">+=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">16</span>))</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(N, cmap<span class="op">=</span><span class="st">'Blues'</span>)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">27</span>):</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">27</span>):</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>        chstr <span class="op">=</span> id_to_char[i] <span class="op">+</span> id_to_char[j] <span class="co">#i to j (char 1 to char 2)</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>        plt.text(j, i, chstr, ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"bottom"</span>, color<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">#.item() since we are indexing tensors and want individual integer</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>        plt.text(j, i, N[i, j].item(), ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"top"</span>, color<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>(-0.5, 26.5, 26.5, -0.5)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-29-output-2.png" class="img-fluid"></p>
</div>
</div>
<ul>
<li>along first row: characters that start a word</li>
<li>along first column: characters that end word</li>
<li>in between: what characters follow each other</li>
</ul>
<p>This counts array has all info necessary for us to sample from for our bigram model. Start following these probabilities and sample from these counts.</p>
<p>Ex:</p>
<p>use <strong>first row</strong> to see how likely the different characters are to start a word:</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>N[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,
        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,
         134,  535,  929], dtype=torch.int32)</code></pre>
</div>
</div>
<p>Take frequency vector and convert to float so that we can normalize these counts (and turn into probability); frequency <span class="math inline">\(\rightarrow\)</span> probability</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> N[<span class="dv">0</span>].<span class="bu">float</span>()</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> p<span class="op">/</span>p.<span class="bu">sum</span>()</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>p <span class="co">#probability for any single character to be the first in a word</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,
        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,
        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])</code></pre>
</div>
</div>
<p>Now we get a probability distribution vector <code>p</code>.</p>
<p>Want to sample from this distribution – use <code>torch.multinomial</code> to return samples from a multinomial probability distribution</p>
<ul>
<li>use <code>torch.Generator</code> to make this deterministic/seed it for reproducible reasons – using this generator object can always give you the same random numbers</li>
</ul>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co">#ex</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> torch.rand(<span class="dv">3</span>, generator<span class="op">=</span>g)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> p<span class="op">/</span>p.<span class="bu">sum</span>()</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>p <span class="co">#can draw reproducible samples</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>tensor([0.6064, 0.3033, 0.0903])</code></pre>
</div>
</div>
<p><code>torch.multinomial</code> will take a tensor of probability distributions</p>
<ul>
<li>we can ask it for a number of samples (e.g.&nbsp;100 samples)</li>
<li>can specify <code>replacement=True</code> if we want to sample with replacement (default is w/o replacement)</li>
</ul>
<p>looking at our tensor prob distribution, expect to see a lot of 0’s since its probability is 0.6064, half as many 1’s, very few 2’s</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>torch.multinomial(p, num_samples<span class="op">=</span><span class="dv">100</span>, replacement<span class="op">=</span><span class="va">True</span>, generator<span class="op">=</span>g)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>tensor([1, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0,
        0, 1, 1, 1])</code></pre>
</div>
</div>
<p>Now taking a sample of 1 from the original frequency vector</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="co">#tensor that wraps 10 -- need `.item()` to get integer</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>ix <span class="op">=</span> torch.multinomial(p, num_samples<span class="op">=</span><span class="dv">1</span>, replacement<span class="op">=</span><span class="va">True</span>, generator<span class="op">=</span>g).item()</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="co">#check which character we're sampling w/ dict</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>id_to_char[ix]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>'.'</code></pre>
</div>
</div>
<p>interpretation: first character is “m” in our generation</p>
<p>To draw next character: look at row vector of counts where we start with “m” as first character</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="co">#begin at index 0 (start token)</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>ix <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">#get row for index we are currently on </span></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> N[ix].<span class="bu">float</span>()</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">#normalize -&gt; prob</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> p<span class="op">/</span>p.<span class="bu">sum</span>()</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">#draw a sample of 1</span></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> torch.multinomial(p, num_samples<span class="op">=</span><span class="dv">1</span>, replacement<span class="op">=</span><span class="va">True</span>, generator<span class="op">=</span>g).item()</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(id_to_char[ix])</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">#end token</span></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ix<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>j
u
n
i
d
e
.</code></pre>
</div>
</div>
<p>Join word together and iterate:</p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> []</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> N[ix].<span class="bu">float</span>()</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> p<span class="op">/</span>p.<span class="bu">sum</span>()</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>        ix <span class="op">=</span> torch.multinomial(p, num_samples<span class="op">=</span><span class="dv">1</span>, replacement<span class="op">=</span><span class="va">True</span>, generator<span class="op">=</span>g).item()</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>        out.append(id_to_char[ix])</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> ix<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">''</span>.join(out))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>junide.
janasah.
p.
cony.
a.
nn.
kohin.
tolian.
juee.
ksahnaauranilevias.
dedainrwieta.
ssonielylarte.
faveumerifontume.
phynslenaruani.
core.
yaenon.
ka.
jabdinerimikimaynin.
anaasn.
ssorionsush.</code></pre>
</div>
</div>
<p>We can see that this model is not great.</p>
<p>Instead of <code>p</code> having structure, what if it was a uniform distribution? – so this model is slightly better, just not great</p>
<p>What we’re doing every iteration:</p>
<ul>
<li>fetch row form counts matrix</li>
<li>converting to float and renormalizing</li>
</ul>
<p>This is very efficient.</p>
<p>Solution: prepare a matrix that normalizes upfront</p>
<p>*Be careful with <code>P.sum()</code> – we want to simultaneously+in parallel divide all the rows by their sums</p>
<p><code>torch.sum()</code> takes in a parameter <code>keepdim=False</code> by default.</p>
<ul>
<li>If True, the output tensor is of same size as input except where dimensions are size 1.</li>
<li>If false, the dimension is squeezed out -&gt; output tenser has 1 or <code>len(dim)</code> fewer dimension(s)</li>
</ul>
<p>so <code>torch.sum()</code> collapses dimension to be size 1 and squeeze out that dimension.</p>
<p>Want to specify dimension to take sum across:</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co">#similar to our previous for loop contents</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="co">#grab floating point copy of N</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> N.<span class="bu">float</span>()</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>P</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>tensor([[0.0000e+00, 4.4100e+03, 1.3060e+03, 1.5420e+03, 1.6900e+03, 1.5310e+03,
         4.1700e+02, 6.6900e+02, 8.7400e+02, 5.9100e+02, 2.4220e+03, 2.9630e+03,
         1.5720e+03, 2.5380e+03, 1.1460e+03, 3.9400e+02, 5.1500e+02, 9.2000e+01,
         1.6390e+03, 2.0550e+03, 1.3080e+03, 7.8000e+01, 3.7600e+02, 3.0700e+02,
         1.3400e+02, 5.3500e+02, 9.2900e+02],
        [6.6400e+03, 5.5600e+02, 5.4100e+02, 4.7000e+02, 1.0420e+03, 6.9200e+02,
         1.3400e+02, 1.6800e+02, 2.3320e+03, 1.6500e+03, 1.7500e+02, 5.6800e+02,
         2.5280e+03, 1.6340e+03, 5.4380e+03, 6.3000e+01, 8.2000e+01, 6.0000e+01,
         3.2640e+03, 1.1180e+03, 6.8700e+02, 3.8100e+02, 8.3400e+02, 1.6100e+02,
         1.8200e+02, 2.0500e+03, 4.3500e+02],
        [1.1400e+02, 3.2100e+02, 3.8000e+01, 1.0000e+00, 6.5000e+01, 6.5500e+02,
         0.0000e+00, 0.0000e+00, 4.1000e+01, 2.1700e+02, 1.0000e+00, 0.0000e+00,
         1.0300e+02, 0.0000e+00, 4.0000e+00, 1.0500e+02, 0.0000e+00, 0.0000e+00,
         8.4200e+02, 8.0000e+00, 2.0000e+00, 4.5000e+01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 8.3000e+01, 0.0000e+00],
        [9.7000e+01, 8.1500e+02, 0.0000e+00, 4.2000e+01, 1.0000e+00, 5.5100e+02,
         0.0000e+00, 2.0000e+00, 6.6400e+02, 2.7100e+02, 3.0000e+00, 3.1600e+02,
         1.1600e+02, 0.0000e+00, 0.0000e+00, 3.8000e+02, 1.0000e+00, 1.1000e+01,
         7.6000e+01, 5.0000e+00, 3.5000e+01, 3.5000e+01, 0.0000e+00, 0.0000e+00,
         3.0000e+00, 1.0400e+02, 4.0000e+00],
        [5.1600e+02, 1.3030e+03, 1.0000e+00, 3.0000e+00, 1.4900e+02, 1.2830e+03,
         5.0000e+00, 2.5000e+01, 1.1800e+02, 6.7400e+02, 9.0000e+00, 3.0000e+00,
         6.0000e+01, 3.0000e+01, 3.1000e+01, 3.7800e+02, 0.0000e+00, 1.0000e+00,
         4.2400e+02, 2.9000e+01, 4.0000e+00, 9.2000e+01, 1.7000e+01, 2.3000e+01,
         0.0000e+00, 3.1700e+02, 1.0000e+00],
        [3.9830e+03, 6.7900e+02, 1.2100e+02, 1.5300e+02, 3.8400e+02, 1.2710e+03,
         8.2000e+01, 1.2500e+02, 1.5200e+02, 8.1800e+02, 5.5000e+01, 1.7800e+02,
         3.2480e+03, 7.6900e+02, 2.6750e+03, 2.6900e+02, 8.3000e+01, 1.4000e+01,
         1.9580e+03, 8.6100e+02, 5.8000e+02, 6.9000e+01, 4.6300e+02, 5.0000e+01,
         1.3200e+02, 1.0700e+03, 1.8100e+02],
        [8.0000e+01, 2.4200e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2300e+02,
         4.4000e+01, 1.0000e+00, 1.0000e+00, 1.6000e+02, 0.0000e+00, 2.0000e+00,
         2.0000e+01, 0.0000e+00, 4.0000e+00, 6.0000e+01, 0.0000e+00, 0.0000e+00,
         1.1400e+02, 6.0000e+00, 1.8000e+01, 1.0000e+01, 0.0000e+00, 4.0000e+00,
         0.0000e+00, 1.4000e+01, 2.0000e+00],
        [1.0800e+02, 3.3000e+02, 3.0000e+00, 0.0000e+00, 1.9000e+01, 3.3400e+02,
         1.0000e+00, 2.5000e+01, 3.6000e+02, 1.9000e+02, 3.0000e+00, 0.0000e+00,
         3.2000e+01, 6.0000e+00, 2.7000e+01, 8.3000e+01, 0.0000e+00, 0.0000e+00,
         2.0100e+02, 3.0000e+01, 3.1000e+01, 8.5000e+01, 1.0000e+00, 2.6000e+01,
         0.0000e+00, 3.1000e+01, 1.0000e+00],
        [2.4090e+03, 2.2440e+03, 8.0000e+00, 2.0000e+00, 2.4000e+01, 6.7400e+02,
         2.0000e+00, 2.0000e+00, 1.0000e+00, 7.2900e+02, 9.0000e+00, 2.9000e+01,
         1.8500e+02, 1.1700e+02, 1.3800e+02, 2.8700e+02, 1.0000e+00, 1.0000e+00,
         2.0400e+02, 3.1000e+01, 7.1000e+01, 1.6600e+02, 3.9000e+01, 1.0000e+01,
         0.0000e+00, 2.1300e+02, 2.0000e+01],
        [2.4890e+03, 2.4450e+03, 1.1000e+02, 5.0900e+02, 4.4000e+02, 1.6530e+03,
         1.0100e+02, 4.2800e+02, 9.5000e+01, 8.2000e+01, 7.6000e+01, 4.4500e+02,
         1.3450e+03, 4.2700e+02, 2.1260e+03, 5.8800e+02, 5.3000e+01, 5.2000e+01,
         8.4900e+02, 1.3160e+03, 5.4100e+02, 1.0900e+02, 2.6900e+02, 8.0000e+00,
         8.9000e+01, 7.7900e+02, 2.7700e+02],
        [7.1000e+01, 1.4730e+03, 1.0000e+00, 4.0000e+00, 4.0000e+00, 4.4000e+02,
         0.0000e+00, 0.0000e+00, 4.5000e+01, 1.1900e+02, 2.0000e+00, 2.0000e+00,
         9.0000e+00, 5.0000e+00, 2.0000e+00, 4.7900e+02, 1.0000e+00, 0.0000e+00,
         1.1000e+01, 7.0000e+00, 2.0000e+00, 2.0200e+02, 5.0000e+00, 6.0000e+00,
         0.0000e+00, 1.0000e+01, 0.0000e+00],
        [3.6300e+02, 1.7310e+03, 2.0000e+00, 2.0000e+00, 2.0000e+00, 8.9500e+02,
         1.0000e+00, 0.0000e+00, 3.0700e+02, 5.0900e+02, 2.0000e+00, 2.0000e+01,
         1.3900e+02, 9.0000e+00, 2.6000e+01, 3.4400e+02, 0.0000e+00, 0.0000e+00,
         1.0900e+02, 9.5000e+01, 1.7000e+01, 5.0000e+01, 2.0000e+00, 3.4000e+01,
         0.0000e+00, 3.7900e+02, 2.0000e+00],
        [1.3140e+03, 2.6230e+03, 5.2000e+01, 2.5000e+01, 1.3800e+02, 2.9210e+03,
         2.2000e+01, 6.0000e+00, 1.9000e+01, 2.4800e+03, 6.0000e+00, 2.4000e+01,
         1.3450e+03, 6.0000e+01, 1.4000e+01, 6.9200e+02, 1.5000e+01, 3.0000e+00,
         1.8000e+01, 9.4000e+01, 7.7000e+01, 3.2400e+02, 7.2000e+01, 1.6000e+01,
         0.0000e+00, 1.5880e+03, 1.0000e+01],
        [5.1600e+02, 2.5900e+03, 1.1200e+02, 5.1000e+01, 2.4000e+01, 8.1800e+02,
         1.0000e+00, 0.0000e+00, 5.0000e+00, 1.2560e+03, 7.0000e+00, 1.0000e+00,
         5.0000e+00, 1.6800e+02, 2.0000e+01, 4.5200e+02, 3.8000e+01, 0.0000e+00,
         9.7000e+01, 3.5000e+01, 4.0000e+00, 1.3900e+02, 3.0000e+00, 2.0000e+00,
         0.0000e+00, 2.8700e+02, 1.1000e+01],
        [6.7630e+03, 2.9770e+03, 8.0000e+00, 2.1300e+02, 7.0400e+02, 1.3590e+03,
         1.1000e+01, 2.7300e+02, 2.6000e+01, 1.7250e+03, 4.4000e+01, 5.8000e+01,
         1.9500e+02, 1.9000e+01, 1.9060e+03, 4.9600e+02, 5.0000e+00, 2.0000e+00,
         4.4000e+01, 2.7800e+02, 4.4300e+02, 9.6000e+01, 5.5000e+01, 1.1000e+01,
         6.0000e+00, 4.6500e+02, 1.4500e+02],
        [8.5500e+02, 1.4900e+02, 1.4000e+02, 1.1400e+02, 1.9000e+02, 1.3200e+02,
         3.4000e+01, 4.4000e+01, 1.7100e+02, 6.9000e+01, 1.6000e+01, 6.8000e+01,
         6.1900e+02, 2.6100e+02, 2.4110e+03, 1.1500e+02, 9.5000e+01, 3.0000e+00,
         1.0590e+03, 5.0400e+02, 1.1800e+02, 2.7500e+02, 1.7600e+02, 1.1400e+02,
         4.5000e+01, 1.0300e+02, 5.4000e+01],
        [3.3000e+01, 2.0900e+02, 2.0000e+00, 1.0000e+00, 0.0000e+00, 1.9700e+02,
         1.0000e+00, 0.0000e+00, 2.0400e+02, 6.1000e+01, 1.0000e+00, 1.0000e+00,
         1.6000e+01, 1.0000e+00, 1.0000e+00, 5.9000e+01, 3.9000e+01, 0.0000e+00,
         1.5100e+02, 1.6000e+01, 1.7000e+01, 4.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 1.2000e+01, 0.0000e+00],
        [2.8000e+01, 1.3000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3000e+01, 0.0000e+00, 0.0000e+00,
         1.0000e+00, 2.0000e+00, 0.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00,
         1.0000e+00, 2.0000e+00, 0.0000e+00, 2.0600e+02, 0.0000e+00, 3.0000e+00,
         0.0000e+00, 0.0000e+00, 0.0000e+00],
        [1.3770e+03, 2.3560e+03, 4.1000e+01, 9.9000e+01, 1.8700e+02, 1.6970e+03,
         9.0000e+00, 7.6000e+01, 1.2100e+02, 3.0330e+03, 2.5000e+01, 9.0000e+01,
         4.1300e+02, 1.6200e+02, 1.4000e+02, 8.6900e+02, 1.4000e+01, 1.6000e+01,
         4.2500e+02, 1.9000e+02, 2.0800e+02, 2.5200e+02, 8.0000e+01, 2.1000e+01,
         3.0000e+00, 7.7300e+02, 2.3000e+01],
        [1.1690e+03, 1.2010e+03, 2.1000e+01, 6.0000e+01, 9.0000e+00, 8.8400e+02,
         2.0000e+00, 2.0000e+00, 1.2850e+03, 6.8400e+02, 2.0000e+00, 8.2000e+01,
         2.7900e+02, 9.0000e+01, 2.4000e+01, 5.3100e+02, 5.1000e+01, 1.0000e+00,
         5.5000e+01, 4.6100e+02, 7.6500e+02, 1.8500e+02, 1.4000e+01, 2.4000e+01,
         0.0000e+00, 2.1500e+02, 1.0000e+01],
        [4.8300e+02, 1.0270e+03, 1.0000e+00, 1.7000e+01, 0.0000e+00, 7.1600e+02,
         2.0000e+00, 2.0000e+00, 6.4700e+02, 5.3200e+02, 3.0000e+00, 0.0000e+00,
         1.3400e+02, 4.0000e+00, 2.2000e+01, 6.6700e+02, 0.0000e+00, 0.0000e+00,
         3.5200e+02, 3.5000e+01, 3.7400e+02, 7.8000e+01, 1.5000e+01, 1.1000e+01,
         2.0000e+00, 3.4100e+02, 1.0500e+02],
        [1.5500e+02, 1.6300e+02, 1.0300e+02, 1.0300e+02, 1.3600e+02, 1.6900e+02,
         1.9000e+01, 4.7000e+01, 5.8000e+01, 1.2100e+02, 1.4000e+01, 9.3000e+01,
         3.0100e+02, 1.5400e+02, 2.7500e+02, 1.0000e+01, 1.6000e+01, 1.0000e+01,
         4.1400e+02, 4.7400e+02, 8.2000e+01, 3.0000e+00, 3.7000e+01, 8.6000e+01,
         3.4000e+01, 1.3000e+01, 4.5000e+01],
        [8.8000e+01, 6.4200e+02, 1.0000e+00, 0.0000e+00, 1.0000e+00, 5.6800e+02,
         0.0000e+00, 0.0000e+00, 1.0000e+00, 9.1100e+02, 0.0000e+00, 3.0000e+00,
         1.4000e+01, 0.0000e+00, 8.0000e+00, 1.5300e+02, 0.0000e+00, 0.0000e+00,
         4.8000e+01, 0.0000e+00, 0.0000e+00, 7.0000e+00, 7.0000e+00, 0.0000e+00,
         0.0000e+00, 1.2100e+02, 0.0000e+00],
        [5.1000e+01, 2.8000e+02, 1.0000e+00, 0.0000e+00, 8.0000e+00, 1.4900e+02,
         2.0000e+00, 1.0000e+00, 2.3000e+01, 1.4800e+02, 0.0000e+00, 6.0000e+00,
         1.3000e+01, 2.0000e+00, 5.8000e+01, 3.6000e+01, 0.0000e+00, 0.0000e+00,
         2.2000e+01, 2.0000e+01, 8.0000e+00, 2.5000e+01, 0.0000e+00, 2.0000e+00,
         0.0000e+00, 7.3000e+01, 1.0000e+00],
        [1.6400e+02, 1.0300e+02, 1.0000e+00, 4.0000e+00, 5.0000e+00, 3.6000e+01,
         3.0000e+00, 0.0000e+00, 1.0000e+00, 1.0200e+02, 0.0000e+00, 0.0000e+00,
         3.9000e+01, 1.0000e+00, 1.0000e+00, 4.1000e+01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 3.1000e+01, 7.0000e+01, 5.0000e+00, 0.0000e+00, 3.0000e+00,
         3.8000e+01, 3.0000e+01, 1.9000e+01],
        [2.0070e+03, 2.1430e+03, 2.7000e+01, 1.1500e+02, 2.7200e+02, 3.0100e+02,
         1.2000e+01, 3.0000e+01, 2.2000e+01, 1.9200e+02, 2.3000e+01, 8.6000e+01,
         1.1040e+03, 1.4800e+02, 1.8260e+03, 2.7100e+02, 1.5000e+01, 6.0000e+00,
         2.9100e+02, 4.0100e+02, 1.0400e+02, 1.4100e+02, 1.0600e+02, 4.0000e+00,
         2.8000e+01, 2.3000e+01, 7.8000e+01],
        [1.6000e+02, 8.6000e+02, 4.0000e+00, 2.0000e+00, 2.0000e+00, 3.7300e+02,
         0.0000e+00, 1.0000e+00, 4.3000e+01, 3.6400e+02, 2.0000e+00, 2.0000e+00,
         1.2300e+02, 3.5000e+01, 4.0000e+00, 1.1000e+02, 2.0000e+00, 0.0000e+00,
         3.2000e+01, 4.0000e+00, 4.0000e+00, 7.3000e+01, 2.0000e+00, 3.0000e+00,
         1.0000e+00, 1.4700e+02, 4.5000e+01]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>p.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>torch.Size([27])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>P.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>torch.Size([27, 27])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>P.<span class="bu">sum</span>(<span class="dv">0</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>tensor([32033., 33885.,  2645.,  3532.,  5496., 20423.,   905.,  1927.,  7616.,
        17701.,  2900.,  5040., 13958.,  6642., 18327.,  7934.,  1026.,   272.,
        12700.,  8106.,  5570.,  3135.,  2573.,   929.,   697.,  9776.,  2398.])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>P.<span class="bu">sum</span>(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>tensor([[32033., 33885.,  2645.,  3532.,  5496., 20423.,   905.,  1927.,  7616.,
         17701.,  2900.,  5040., 13958.,  6642., 18327.,  7934.,  1026.,   272.,
         12700.,  8106.,  5570.,  3135.,  2573.,   929.,   697.,  9776.,  2398.]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co">#ex:</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>P.<span class="bu">sum</span>(<span class="dv">0</span>).shape <span class="co">#sums along the 1st entry for P.shape; does not keep dimensions -&gt; squeezes out that dimension and we get a 1d vector of size 27</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>torch.Size([27])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>P.<span class="bu">sum</span>(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>).shape <span class="co">#still has 2 dimensions</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>torch.Size([1, 27])</code></pre>
</div>
</div>
<p>We don’t want this 1x27 row vector b/c this will give us sums down the columns – set dimension to 1 instead to sum across rows.</p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>P.<span class="bu">sum</span>(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co">#27x1 vector of counts</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>tensor([[32033.],
        [33885.],
        [ 2645.],
        [ 3532.],
        [ 5496.],
        [20423.],
        [  905.],
        [ 1927.],
        [ 7616.],
        [17701.],
        [ 2900.],
        [ 5040.],
        [13958.],
        [ 6642.],
        [18327.],
        [ 7934.],
        [ 1026.],
        [  272.],
        [12700.],
        [ 8106.],
        [ 5570.],
        [ 3135.],
        [ 2573.],
        [  929.],
        [  697.],
        [ 9776.],
        [ 2398.]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>P.<span class="bu">sum</span>(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>torch.Size([27, 1])</code></pre>
</div>
</div>
<p>Be careful – is it possible to take a 27x27 array and divide it by 27x1 array?</p>
<p>Depends on <strong>broadcasting rules</strong>, which determine if 2 tensors can be combined in a binary operation.</p>
<p>Conditions:</p>
<ul>
<li>each tensor has at least 1 dimension</li>
<li>when iterating over dimension sizes, starting at trailing dimension, dimension sizes must be =, or one of them is 1, or one of them dne</li>
</ul>
<p>takes this 27x1 column vector and copies it 27 times to make both be 27x27 before element-wise division</p>
<ul>
<li>normalizes it</li>
</ul>
<p>End up w/ same counts as when <code>keepdim=True</code> but the shape is not the same – when one of the dimensions doesn’t exist, internally it is 1, which gets copied. We end up normalizing the columns rather than rows.</p>
<p>Broadcasting starts from right to left (aligns right) and if dimension dne, create a 1</p>
<ul>
<li>when <code>keepdim=False</code> the column vector of sums turns into a row vector</li>
<li>row vector gets replicated vertically</li>
</ul>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> N.<span class="bu">float</span>()</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> P <span class="op">/</span> P.<span class="bu">sum</span>(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) </span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="co">#for efficiency, don't want to do this -- use in place operations</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> N.<span class="bu">float</span>()</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>P <span class="op">/=</span> P.<span class="bu">sum</span>(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> []</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> P[ix].<span class="bu">float</span>()</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>        ix <span class="op">=</span> torch.multinomial(p, num_samples<span class="op">=</span><span class="dv">1</span>, replacement<span class="op">=</span><span class="va">True</span>, generator<span class="op">=</span>g).item()</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>        out.append(id_to_char[ix])</span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> ix<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">''</span>.join(out))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>junide.
janasah.
p.
cony.
a.</code></pre>
</div>
</div>
<p>Elements of our array <code>P</code> are parameters of our language model – summarize statistics</p>
<ul>
<li>sample iteratively the next character</li>
</ul>
<p>Want to evaluate quality of this model</p>
<ul>
<li>training loss tells us this in a single number</li>
</ul>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> words[:<span class="dv">3</span>]:</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>  chs <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'.'</span>]</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>    id1 <span class="op">=</span> char_to_id[ch1]</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>    id2 <span class="op">=</span> char_to_id[ch2]</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">#prob that model assigns to each bigram</span></span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> P[id1, id2]</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>ch1<span class="sc">}{</span>ch2<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>prob<span class="sc">:.4f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>.e: 0.0478
em: 0.0377
mm: 0.0253
ma: 0.3899
a.: 0.1960
.o: 0.0123
ol: 0.0780
li: 0.1777
iv: 0.0152
vi: 0.3541
ia: 0.1381
a.: 0.1960
.a: 0.1377
av: 0.0246
va: 0.2495
a.: 0.1960</code></pre>
</div>
</div>
<p>if everything was equally likely, would expect everything to be ~4% but that is not the case here</p>
<ul>
<li>w/ a very good model, would expect these probabilities to be near 1 (correctly predicting what comes next)</li>
</ul>
<p>how to summarize these probabilities into a single number measuring the quality of the model?</p>
<p>Likelihood: product of all probabilities; probability of the entire dataset assigned by our trained model (higher=better)</p>
<ul>
<li>if you take product of all probabilities, will get a very small number</li>
<li>solution: log likelihood</li>
<li>take log of probability</li>
<li>log: monotonic transformation</li>
</ul>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> words[:<span class="dv">3</span>]:</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>  chs <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'.'</span>]</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>    id1 <span class="op">=</span> char_to_id[ch1]</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>    id2 <span class="op">=</span> char_to_id[ch2]</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">#prob that model assigns to each bigram</span></span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> P[id1, id2]</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>    logprob <span class="op">=</span> torch.log(prob)</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>ch1<span class="sc">}{</span>ch2<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>prob<span class="sc">:.4f}</span><span class="ss"> </span><span class="sc">{</span>logprob<span class="sc">:.4f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>.e: 0.0478 -3.0408
em: 0.0377 -3.2793
mm: 0.0253 -3.6772
ma: 0.3899 -0.9418
a.: 0.1960 -1.6299
.o: 0.0123 -4.3982
ol: 0.0780 -2.5508
li: 0.1777 -1.7278
iv: 0.0152 -4.1867
vi: 0.3541 -1.0383
ia: 0.1381 -1.9796
a.: 0.1960 -1.6299
.a: 0.1377 -1.9829
av: 0.0246 -3.7045
va: 0.2495 -1.3882
a.: 0.1960 -1.6299</code></pre>
</div>
</div>
<ul>
<li>higher probabilities –&gt; get closer and closer to 0</li>
<li>lower probabilites –&gt; more and more negative number</li>
</ul>
<p>log likelihood = sum of logs of each probability</p>
<p><span class="math inline">\(\log(a*b*c) = \log(a) + \log(b) + \log(c)\)</span></p>
<p>implementation: start log likelihood at 0 and accumulate</p>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> words[:<span class="dv">3</span>]:</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>  chs <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'.'</span>]</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>    id1 <span class="op">=</span> char_to_id[ch1]</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>    id2 <span class="op">=</span> char_to_id[ch2]</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">#prob that model assigns to each bigram</span></span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> P[id1, id2]</span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>    logprob <span class="op">=</span> torch.log(prob)</span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>    log_likelihood <span class="op">+=</span> logprob</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>ch1<span class="sc">}{</span>ch2<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>prob<span class="sc">:.4f}</span><span class="ss"> </span><span class="sc">{</span>logprob<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>log_likelihood<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>.e: 0.0478 -3.0408
em: 0.0377 -3.2793
mm: 0.0253 -3.6772
ma: 0.3899 -0.9418
a.: 0.1960 -1.6299
.o: 0.0123 -4.3982
ol: 0.0780 -2.5508
li: 0.1777 -1.7278
iv: 0.0152 -4.1867
vi: 0.3541 -1.0383
ia: 0.1381 -1.9796
a.: 0.1960 -1.6299
.a: 0.1377 -1.9829
av: 0.0246 -3.7045
va: 0.2495 -1.3882
a.: 0.1960 -1.6299
-38.78563690185547</code></pre>
</div>
</div>
<p>highest log likelihood = 0</p>
<p>we want a loss fn (to minimize) –&gt; invert this –&gt; <strong>negative log likelihood</strong></p>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> words[:<span class="dv">3</span>]:</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>  chs <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'.'</span>]</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>    id1 <span class="op">=</span> char_to_id[ch1]</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>    id2 <span class="op">=</span> char_to_id[ch2]</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> P[id1, id2]</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>    logprob <span class="op">=</span> torch.log(prob)</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>    log_likelihood <span class="op">+=</span> logprob</span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>ch1<span class="sc">}{</span>ch2<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>prob<span class="sc">:.4f}</span><span class="ss"> </span><span class="sc">{</span>logprob<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>log_likelihood<span class="op">=</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>neg_log_likelihood <span class="op">=</span> <span class="op">-</span>log_likelihood</span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>neg_log_likelihood<span class="op">=</span><span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>.e: 0.0478 -3.0408
em: 0.0377 -3.2793
mm: 0.0253 -3.6772
ma: 0.3899 -0.9418
a.: 0.1960 -1.6299
.o: 0.0123 -4.3982
ol: 0.0780 -2.5508
li: 0.1777 -1.7278
iv: 0.0152 -4.1867
vi: 0.3541 -1.0383
ia: 0.1381 -1.9796
a.: 0.1960 -1.6299
.a: 0.1377 -1.9829
av: 0.0246 -3.7045
va: 0.2495 -1.3882
a.: 0.1960 -1.6299
log_likelihood=tensor(-38.7856)
neg_log_likelihood=tensor(38.7856)</code></pre>
</div>
</div>
<p>nice loss fn b/c the lowest it can get is 0; higher = worse predictions</p>
<p><strong>important note</strong>: want to make it average instead of a sum</p>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> words[:<span class="dv">3</span>]:</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>  chs <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'.'</span>]</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>    id1 <span class="op">=</span> char_to_id[ch1]</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>    id2 <span class="op">=</span> char_to_id[ch2]</span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> P[id1, id2]</span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>    logprob <span class="op">=</span> torch.log(prob)</span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>    log_likelihood <span class="op">+=</span> logprob</span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a>    n <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>ch1<span class="sc">}{</span>ch2<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>prob<span class="sc">:.4f}</span><span class="ss"> </span><span class="sc">{</span>logprob<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-15"><a href="#cb90-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>log_likelihood<span class="op">=</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb90-16"><a href="#cb90-16" aria-hidden="true" tabindex="-1"></a>neg_log_likelihood <span class="op">=</span> <span class="op">-</span>log_likelihood</span>
<span id="cb90-17"><a href="#cb90-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>neg_log_likelihood<span class="op">=</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb90-18"><a href="#cb90-18" aria-hidden="true" tabindex="-1"></a><span class="co">#normalized</span></span>
<span id="cb90-19"><a href="#cb90-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>neg_log_likelihood<span class="op">/</span>n<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>.e: 0.0478 -3.0408
em: 0.0377 -3.2793
mm: 0.0253 -3.6772
ma: 0.3899 -0.9418
a.: 0.1960 -1.6299
.o: 0.0123 -4.3982
ol: 0.0780 -2.5508
li: 0.1777 -1.7278
iv: 0.0152 -4.1867
vi: 0.3541 -1.0383
ia: 0.1381 -1.9796
a.: 0.1960 -1.6299
.a: 0.1377 -1.9829
av: 0.0246 -3.7045
va: 0.2495 -1.3882
a.: 0.1960 -1.6299
log_likelihood=tensor(-38.7856)
neg_log_likelihood=tensor(38.7856)
2.424102306365967</code></pre>
</div>
</div>
<p>2.424102306365967 is loss for training</p>
<ul>
<li>want to find parameters that minimize this loss</li>
<li>goal: maximize likelihood wrt model parameters</li>
<li>in this case model params = probabilites</li>
<li>in future: won’t be in table format but calculated by NNs</li>
</ul>
<p>maximizing likelihood equivalent to maximizing log likelihood (scaling), minimizing negative log likelihood</p>
<ul>
<li>this summarizes the quality of your model</li>
<li>lower = better</li>
</ul>
<p>Can take our loop and apply any word to it:</p>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> [<span class="st">"andrej"</span>]:</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>  chs <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'.'</span>]</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>    id1 <span class="op">=</span> char_to_id[ch1]</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>    id2 <span class="op">=</span> char_to_id[ch2]</span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> P[id1, id2]</span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>    logprob <span class="op">=</span> torch.log(prob)</span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>    log_likelihood <span class="op">+=</span> logprob</span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>    n <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>ch1<span class="sc">}{</span>ch2<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>prob<span class="sc">:.4f}</span><span class="ss"> </span><span class="sc">{</span>logprob<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>log_likelihood<span class="op">=</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a>neg_log_likelihood <span class="op">=</span> <span class="op">-</span>log_likelihood</span>
<span id="cb92-17"><a href="#cb92-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>neg_log_likelihood<span class="op">=</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb92-18"><a href="#cb92-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>neg_log_likelihood<span class="op">/</span>n<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb92-19"><a href="#cb92-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-20"><a href="#cb92-20" aria-hidden="true" tabindex="-1"></a><span class="co">#can see that ej is very unlikely</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>.a: 0.1377 -1.9829
an: 0.1605 -1.8296
nd: 0.0384 -3.2594
dr: 0.0771 -2.5620
re: 0.1336 -2.0127
ej: 0.0027 -5.9171
j.: 0.0245 -3.7098
log_likelihood=tensor(-21.2735)
neg_log_likelihood=tensor(21.2735)
3.03906512260437</code></pre>
</div>
</div>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> [<span class="st">"andrejq"</span>]:</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>  chs <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'.'</span>]</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>    id1 <span class="op">=</span> char_to_id[ch1]</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>    id2 <span class="op">=</span> char_to_id[ch2]</span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> P[id1, id2]</span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>    logprob <span class="op">=</span> torch.log(prob)</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>    log_likelihood <span class="op">+=</span> logprob</span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a>    n <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>ch1<span class="sc">}{</span>ch2<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>prob<span class="sc">:.4f}</span><span class="ss"> </span><span class="sc">{</span>logprob<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>log_likelihood<span class="op">=</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>neg_log_likelihood <span class="op">=</span> <span class="op">-</span>log_likelihood</span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>neg_log_likelihood<span class="op">=</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>neg_log_likelihood<span class="op">/</span>n<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a><span class="co">#get infinity since "jq" has prob of 0 for our model (infinite loss)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>.a: 0.1377 -1.9829
an: 0.1605 -1.8296
nd: 0.0384 -3.2594
dr: 0.0771 -2.5620
re: 0.1336 -2.0127
ej: 0.0027 -5.9171
jq: 0.0000 -inf
q.: 0.1029 -2.2736
log_likelihood=tensor(-inf)
neg_log_likelihood=tensor(inf)
inf</code></pre>
</div>
</div>
<p>Solution: <strong>model smoothing</strong></p>
<p>add fake counts</p>
<div class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> (N<span class="op">+</span><span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>P <span class="op">/=</span> P.<span class="bu">sum</span>(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co">#add 100</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> (N<span class="op">+</span><span class="dv">100</span>).<span class="bu">float</span>()</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>P <span class="op">/=</span> P.<span class="bu">sum</span>(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>the more you add, the more uniform model you’re going to</p>
<ul>
<li>1 is a decent count to add</li>
</ul>
<p>Recap:</p>
<ul>
<li>trained bigram character-level language model</li>
<li>trained bigram model by looking at counts of all bigrams and normalizing rows to get prob distributions</li>
<li>can use parameters of model to sample new words</li>
<li>can evaluate quality of the model in a single number</li>
</ul>
<p>Alternatively, cast into NN framework</p>
<ul>
<li>receive a single character as input</li>
<li>outputs probability distribution over next character in a sequence</li>
<li>can evaluate any setting of the parameters of NN using loss fn
<ul>
<li>compare prob assigned by model and the labels</li>
</ul></li>
<li>using gradient-based optimization to tune params of network
<ul>
<li>tune weights so NN is correctly probabilities for next character</li>
</ul></li>
</ul>
<p>first compile training set</p>
<div class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="co">#create train set</span></span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a><span class="co">#2 lists</span></span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>xs, ys <span class="op">=</span> [], [] <span class="co">#inputs, labels</span></span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a><span class="co">#iterates over all bigrams</span></span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> words[:<span class="dv">1</span>]:</span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a>  chs <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'.'</span>]</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a>    id1 <span class="op">=</span> char_to_id[ch1]</span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a>    id2 <span class="op">=</span> char_to_id[ch2]</span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(ch1, ch2)</span>
<span id="cb98-13"><a href="#cb98-13" aria-hidden="true" tabindex="-1"></a>    xs.append(id1)</span>
<span id="cb98-14"><a href="#cb98-14" aria-hidden="true" tabindex="-1"></a>    ys.append(id2)</span>
<span id="cb98-15"><a href="#cb98-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-16"><a href="#cb98-16" aria-hidden="true" tabindex="-1"></a><span class="co">#want tensors, not lists</span></span>
<span id="cb98-17"><a href="#cb98-17" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> torch.tensor(xs)</span>
<span id="cb98-18"><a href="#cb98-18" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> torch.tensor(ys)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>. e
e m
m m
m a
a .</code></pre>
</div>
</div>
<p>above: 5 examples for our NN; can summarize them below:</p>
<div class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>xs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>tensor([ 0,  5, 13, 13,  1])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>ys</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="67">
<pre><code>tensor([ 5, 13, 13,  1,  0])</code></pre>
</div>
</div>
<p>ex. when input to NN is 5, we want its weights to be arranged so that 13 has a high probability</p>
<p>Aside: <code>tensor</code> vs <code>Tensor</code> – <code>tensor</code> infers dtype automatically</p>
<p>how to feed in examples into NN?</p>
<ul>
<li>can’t just plug in integer index</li>
<li>weights act multiplicatively on input</li>
<li><strong>one hot encoding</strong> using <code>torch.nn.functional.one_hot</code>
<ul>
<li>takes in how long you want your tensor to be</li>
</ul></li>
</ul>
<div class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>xenc <span class="op">=</span> F.one_hot(xs, num_classes<span class="op">=</span><span class="dv">27</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>xenc.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="71">
<pre><code>torch.Size([5, 27])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>plt.imshow(xenc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="72">
<pre><code>&lt;matplotlib.image.AxesImage at 0x12fa84e90&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-63-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>5 examples –&gt; 5 rows</p>
<ul>
<li>appropriate bit turned on as 1 (the integer you are encoding) – everything else is 0</li>
<li>then vectors can feed into NN</li>
</ul>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>xenc.dtype</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="73">
<pre><code>torch.int64</code></pre>
</div>
</div>
<p>don’t want data type being plugged in to be integers</p>
<ul>
<li>cast to float (can’t specify data type otherwise)</li>
</ul>
<div class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="co">#cast to float</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>xenc <span class="op">=</span> F.one_hot(xs, num_classes<span class="op">=</span><span class="dv">27</span>).<span class="bu">float</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>xenc.dtype</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="75">
<pre><code>torch.float32</code></pre>
</div>
</div>
<p>NN perform <span class="math inline">\(\mathbf{Wx}+\mathbf{b}\)</span> where <span class="math inline">\(\mathbf{Wx}\)</span> is a dot product</p>
<div class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="co">#initalize weights by drawing from normal distribution randomly</span></span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">1</span>))</span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true" tabindex="-1"></a>W</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="76">
<pre><code>tensor([[ 0.3728],
        [ 0.6661],
        [-1.3309],
        [-0.1947],
        [ 0.4815],
        [ 0.8087],
        [ 1.5211],
        [ 0.1900],
        [ 1.8205],
        [-0.2856],
        [ 0.3763],
        [ 0.7953],
        [-0.4781],
        [ 1.0884],
        [ 0.9759],
        [ 0.1490],
        [-1.1983],
        [-0.3366],
        [-2.4450],
        [ 0.1783],
        [ 0.7675],
        [-0.8531],
        [ 2.5560],
        [ 1.6381],
        [ 0.5793],
        [ 1.1928],
        [-0.8738]])</code></pre>
</div>
</div>
<p>these weights then multiplied by inputs:</p>
<div class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>xenc <span class="op">@</span> W</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="79">
<pre><code>tensor([[0.3728],
        [0.8087],
        [1.0884],
        [1.0884],
        [0.6661]])</code></pre>
</div>
</div>
<p>output: 5x1</p>
<p>(5, 27) @ (27, 1)</p>
<ul>
<li>the 27 will multiply and add</li>
<li>we are seeing the 5 <strong>activations</strong> of this neuron on these 5 inputs+evaluate in parallel
<ul>
<li>fed into the same neuron and in parallel, PyTorch evaluates <span class="math inline">\(\mathbf{Wx}\)</span></li>
</ul></li>
</ul>
<p>we want 27 neurons</p>
<ul>
<li>will evaluate all 27 neurons on all 5 inputs</li>
<li>output will be 5x27</li>
<li>for every 1 of 27 neurons, what is the firing rate of those five examples?</li>
</ul>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">27</span>))</span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>xenc <span class="op">@</span> W</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="81">
<pre><code>tensor([[ 0.0127, -1.2721, -0.4441,  0.6871,  0.3346,  0.6414, -1.3974, -0.3442,
         -0.3517, -2.0894,  0.1947,  1.6428, -0.0271, -1.2485,  0.9322, -0.3985,
         -1.5168,  1.2239,  0.0095,  0.5157, -0.3521, -0.2780, -1.4493,  1.4259,
          0.5597, -0.7602,  0.4888],
        [ 1.2436, -1.4493,  0.1234, -1.7376, -0.1972, -0.3674,  0.3938,  0.7951,
          0.7886,  0.2017, -0.0832, -0.6972,  1.0421,  0.2409,  0.6100, -0.4380,
         -0.7845, -0.4896, -0.2760,  1.7668, -0.8636,  0.2208, -1.0639,  1.5194,
         -0.1063,  0.2183, -0.1574],
        [-0.1304,  0.2048, -1.2809, -0.5326,  0.5429, -0.5297, -0.5725, -1.0901,
          1.6027,  0.5139,  1.5645,  1.1537,  0.4501,  0.2249,  0.1239,  0.5280,
         -0.1994,  1.4687,  1.1634,  1.1873,  0.7968, -1.9044, -0.3376,  0.5999,
         -0.1986, -0.4938, -0.5324],
        [-0.1304,  0.2048, -1.2809, -0.5326,  0.5429, -0.5297, -0.5725, -1.0901,
          1.6027,  0.5139,  1.5645,  1.1537,  0.4501,  0.2249,  0.1239,  0.5280,
         -0.1994,  1.4687,  1.1634,  1.1873,  0.7968, -1.9044, -0.3376,  0.5999,
         -0.1986, -0.4938, -0.5324],
        [-2.3533, -1.5248,  0.3334, -1.0110,  1.1808,  0.0563,  1.5598, -0.5443,
         -0.4854,  0.1344,  0.3298,  1.9121, -0.0591,  0.1370, -0.1254, -1.0686,
         -0.6022, -0.7270, -0.1394, -0.1541, -1.2129,  1.4275, -0.4483, -0.4349,
         -0.0274, -0.1073, -0.7638]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="co">#ex</span></span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>(xenc <span class="op">@</span> W)[<span class="dv">3</span>, <span class="dv">13</span>]</span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true" tabindex="-1"></a><span class="co">#firing rate of 13th neuron looking at 3rd input</span></span>
<span id="cb120-5"><a href="#cb120-5" aria-hidden="true" tabindex="-1"></a><span class="co">#achieved w/ dot product between 3rd input and 13th column of W matrix</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="82">
<pre><code>tensor(0.2249)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>xenc[<span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="83">
<pre><code>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a>W[:, <span class="dv">13</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="84">
<pre><code>tensor([-1.2485,  0.1370,  0.3096,  1.3527, -0.6874,  0.2409, -1.0772,  1.1542,
         0.0684,  0.0760, -1.5029, -1.3511,  0.7832,  0.2249,  0.4600,  1.2742,
        -0.1361, -0.0859, -1.0860,  0.8799,  0.6354, -0.3532,  0.8553, -0.9668,
         1.9522, -0.5757, -0.7297])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb126"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a>(xenc[<span class="dv">3</span>] <span class="op">*</span> W[:, <span class="dv">13</span>]).<span class="bu">sum</span>()</span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a><span class="co">#shows it's being done efficiently by matrix multiplication</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="86">
<pre><code>tensor(0.2249)</code></pre>
</div>
</div>
<p>27 inputs, 27 neurons</p>
<ul>
<li>leave this to be a linear layer</li>
<li>for every single input example, trying to produce some kind of probability distribution for next character in the sequence</li>
<li>need to figure out how to interpret numbers that neurons take on</li>
</ul>
<p>what should the NN output?</p>
<ul>
<li>can’t output counts; some are negative; not integers</li>
<li>probabailities: positive, sum to 1</li>
<li>27 numbers are giving us log counts (not direct counts)</li>
<li>to get counts, take log count and exponentiate them
<ul>
<li>you will get numbers below 1 if you plug in neg numbers</li>
<li>greater than 1 if you plug in pos numbers</li>
</ul></li>
</ul>
<p>instead of numbers being all over the place, interpret as log counts and element-wise exponentiate</p>
<div class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>(xenc <span class="op">@</span> W).exp()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="87">
<pre><code>tensor([[1.0128, 0.2802, 0.6414, 1.9879, 1.3974, 1.8991, 0.2472, 0.7088, 0.7035,
         0.1238, 1.2150, 5.1697, 0.9733, 0.2869, 2.5402, 0.6713, 0.2194, 3.4003,
         1.0096, 1.6748, 0.7032, 0.7573, 0.2347, 4.1616, 1.7501, 0.4676, 1.6304],
        [3.4681, 0.2347, 1.1313, 0.1759, 0.8210, 0.6925, 1.4826, 2.2147, 2.2003,
         1.2234, 0.9202, 0.4980, 2.8351, 1.2724, 1.8405, 0.6453, 0.4563, 0.6129,
         0.7588, 5.8518, 0.4217, 1.2470, 0.3451, 4.5693, 0.8992, 1.2440, 0.8544],
        [0.8778, 1.2273, 0.2778, 0.5871, 1.7210, 0.5888, 0.5641, 0.3362, 4.9663,
         1.6717, 4.7804, 3.1699, 1.5685, 1.2522, 1.1319, 1.6955, 0.8192, 4.3437,
         3.2007, 3.2783, 2.2184, 0.1489, 0.7135, 1.8220, 0.8199, 0.6103, 0.5872],
        [0.8778, 1.2273, 0.2778, 0.5871, 1.7210, 0.5888, 0.5641, 0.3362, 4.9663,
         1.6717, 4.7804, 3.1699, 1.5685, 1.2522, 1.1319, 1.6955, 0.8192, 4.3437,
         3.2007, 3.2783, 2.2184, 0.1489, 0.7135, 1.8220, 0.8199, 0.6103, 0.5872],
        [0.0951, 0.2177, 1.3957, 0.3639, 3.2569, 1.0579, 4.7577, 0.5803, 0.6155,
         1.1439, 1.3907, 6.7671, 0.9427, 1.1469, 0.8821, 0.3435, 0.5476, 0.4834,
         0.8699, 0.8572, 0.2973, 4.1684, 0.6387, 0.6474, 0.9730, 0.8982, 0.4659]])</code></pre>
</div>
</div>
<p>negative numbers –&gt; less than 1; positive numbers –&gt; more positive</p>
<p>log counts: logits</p>
<div class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> xenc <span class="op">@</span> W <span class="co">#log-counts</span></span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> logits.exp() <span class="co">#equivalent N</span></span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> counts<span class="op">/</span>counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co">#normalize probs</span></span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a>probs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="88">
<pre><code>tensor([[0.0282, 0.0078, 0.0179, 0.0554, 0.0390, 0.0529, 0.0069, 0.0198, 0.0196,
         0.0035, 0.0339, 0.1441, 0.0271, 0.0080, 0.0708, 0.0187, 0.0061, 0.0948,
         0.0281, 0.0467, 0.0196, 0.0211, 0.0065, 0.1160, 0.0488, 0.0130, 0.0455],
        [0.0891, 0.0060, 0.0291, 0.0045, 0.0211, 0.0178, 0.0381, 0.0569, 0.0565,
         0.0314, 0.0236, 0.0128, 0.0729, 0.0327, 0.0473, 0.0166, 0.0117, 0.0157,
         0.0195, 0.1504, 0.0108, 0.0320, 0.0089, 0.1174, 0.0231, 0.0320, 0.0220],
        [0.0195, 0.0273, 0.0062, 0.0131, 0.0383, 0.0131, 0.0125, 0.0075, 0.1104,
         0.0372, 0.1063, 0.0705, 0.0349, 0.0278, 0.0252, 0.0377, 0.0182, 0.0966,
         0.0712, 0.0729, 0.0493, 0.0033, 0.0159, 0.0405, 0.0182, 0.0136, 0.0131],
        [0.0195, 0.0273, 0.0062, 0.0131, 0.0383, 0.0131, 0.0125, 0.0075, 0.1104,
         0.0372, 0.1063, 0.0705, 0.0349, 0.0278, 0.0252, 0.0377, 0.0182, 0.0966,
         0.0712, 0.0729, 0.0493, 0.0033, 0.0159, 0.0405, 0.0182, 0.0136, 0.0131],
        [0.0027, 0.0061, 0.0390, 0.0102, 0.0910, 0.0295, 0.1329, 0.0162, 0.0172,
         0.0319, 0.0388, 0.1890, 0.0263, 0.0320, 0.0246, 0.0096, 0.0153, 0.0135,
         0.0243, 0.0239, 0.0083, 0.1164, 0.0178, 0.0181, 0.0272, 0.0251, 0.0130]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>probs[<span class="dv">0</span>].<span class="bu">sum</span>() <span class="co">#every row sums to 1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="90">
<pre><code>tensor(1.0000)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>probs.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="91">
<pre><code>torch.Size([5, 27])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>probs[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="93">
<pre><code>tensor([0.0282, 0.0078, 0.0179, 0.0554, 0.0390, 0.0529, 0.0069, 0.0198, 0.0196,
        0.0035, 0.0339, 0.1441, 0.0271, 0.0080, 0.0708, 0.0187, 0.0061, 0.0948,
        0.0281, 0.0467, 0.0196, 0.0211, 0.0065, 0.1160, 0.0488, 0.0130, 0.0455])</code></pre>
</div>
</div>
<p>for every one of our five examples, we now have a row that came out of the NN</p>
<ul>
<li>b/c of the transformations here, we made sure that output of neural net are now probabilities</li>
<li><span class="math inline">\(\mathbf{Wx}\)</span> gives us logits –&gt; interpret as log counts –&gt; exponentiate to get something like counts –&gt; normalize for probabilities</li>
<li>all of these are differentiable operations that we can backprop through</li>
</ul>
<p>example walkthrough:</p>
<ol type="1">
<li>0th example – <code>.e</code></li>
<li>feed in <code>.</code> into NN (by getting index, one hot encoding it)</li>
<li>distr of probabilities come out (shape of 27) – 27 numbers – NN assignment for how likely the 27 characters are to come next</li>
<li>as we tune weights <span class="math inline">\(\mathbf{W}\)</span>, we will get different probabilities out for any character that you input</li>
<li>optimize+find a good <span class="math inline">\(\mathbf{W}\)</span> so probabilities are good</li>
<li>measure performance w/ loss fn</li>
</ol>
<p>Summary:</p>
<div class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="co">#randomly initialize 27 neurons' weights. each neuron receives 27 inputs</span></span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">27</span>), generator<span class="op">=</span>g)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb139"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="co">#forward pass</span></span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-3"><a href="#cb139-3" aria-hidden="true" tabindex="-1"></a><span class="co">#input to the network: one-hot encoding</span></span>
<span id="cb139-4"><a href="#cb139-4" aria-hidden="true" tabindex="-1"></a>xenc <span class="op">=</span> F.one_hot(xs, num_classes<span class="op">=</span><span class="dv">27</span>).<span class="bu">float</span>() <span class="co">#becomes 5x27 array 0s except for a few ones</span></span>
<span id="cb139-5"><a href="#cb139-5" aria-hidden="true" tabindex="-1"></a><span class="co">#predict log-counts</span></span>
<span id="cb139-6"><a href="#cb139-6" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> xenc <span class="op">@</span> W </span>
<span id="cb139-7"><a href="#cb139-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-8"><a href="#cb139-8" aria-hidden="true" tabindex="-1"></a><span class="co">#softmax:</span></span>
<span id="cb139-9"><a href="#cb139-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-10"><a href="#cb139-10" aria-hidden="true" tabindex="-1"></a><span class="co">#counts, equivalent to N</span></span>
<span id="cb139-11"><a href="#cb139-11" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> logits.exp()</span>
<span id="cb139-12"><a href="#cb139-12" aria-hidden="true" tabindex="-1"></a><span class="co">#probabilities for next character</span></span>
<span id="cb139-13"><a href="#cb139-13" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>softmax activation fn: <span class="math inline">\(\frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}\)</span></p>
<ul>
<li>takes <span class="math inline">\(z\)</span>s (logits), exponentiates them, and divides/normalizes them</li>
<li>outputs probability distributions (something that always sums to 1)</li>
<li>normalization fn</li>
<li>can put on top of any other linear layer and NN will output probabilities</li>
</ul>
<p>Note that everything above in forward pass is made up of differentiable equations. We know how to backprop through (sum, division, multiplication, exponentiation)</p>
<p>we get probabilities in 5x27 array</p>
<ul>
<li>for each example, we have a vector of probs that sum to 1</li>
</ul>
<div class="cell" data-execution_count="95">
<div class="sourceCode cell-code" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>neg_log_likelihood <span class="op">=</span> torch.zeros(<span class="dv">5</span>)</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># i-th bigram:</span></span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> xs[i].item() <span class="co"># input character index</span></span>
<span id="cb140-5"><a href="#cb140-5" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> ys[i].item() <span class="co"># label character index</span></span>
<span id="cb140-6"><a href="#cb140-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'--------'</span>)</span>
<span id="cb140-7"><a href="#cb140-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'bigram example </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>id_to_char[x]<span class="sc">}{</span>id_to_char[y]<span class="sc">}</span><span class="ss"> (indexes </span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">,</span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb140-8"><a href="#cb140-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'input to the neural net:'</span>, x)</span>
<span id="cb140-9"><a href="#cb140-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'output probabilities from the neural net:'</span>, probs[i])</span>
<span id="cb140-10"><a href="#cb140-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'label (actual next character):'</span>, y)</span>
<span id="cb140-11"><a href="#cb140-11" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> probs[i, y]</span>
<span id="cb140-12"><a href="#cb140-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'probability assigned by the net to the the correct character:'</span>, p.item())</span>
<span id="cb140-13"><a href="#cb140-13" aria-hidden="true" tabindex="-1"></a>    logp <span class="op">=</span> torch.log(p)</span>
<span id="cb140-14"><a href="#cb140-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'log likelihood:'</span>, logp.item())</span>
<span id="cb140-15"><a href="#cb140-15" aria-hidden="true" tabindex="-1"></a>    nll <span class="op">=</span> <span class="op">-</span>logp</span>
<span id="cb140-16"><a href="#cb140-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'negative log likelihood:'</span>, nll.item())</span>
<span id="cb140-17"><a href="#cb140-17" aria-hidden="true" tabindex="-1"></a>    neg_log_likelihood[i] <span class="op">=</span> nll</span>
<span id="cb140-18"><a href="#cb140-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-19"><a href="#cb140-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'========='</span>)</span>
<span id="cb140-20"><a href="#cb140-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'average negative log likelihood, i.e. loss ='</span>, neg_log_likelihood.mean().item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>--------
bigram example 1: .e (indexes 0,5)
input to the neural net: 0
output probabilities from the neural net: tensor([0.0282, 0.0078, 0.0179, 0.0554, 0.0390, 0.0529, 0.0069, 0.0198, 0.0196,
        0.0035, 0.0339, 0.1441, 0.0271, 0.0080, 0.0708, 0.0187, 0.0061, 0.0948,
        0.0281, 0.0467, 0.0196, 0.0211, 0.0065, 0.1160, 0.0488, 0.0130, 0.0455])
label (actual next character): 5
probability assigned by the net to the the correct character: 0.0529467947781086
log likelihood: -2.9384677410125732
negative log likelihood: 2.9384677410125732
--------
bigram example 2: em (indexes 5,13)
input to the neural net: 5
output probabilities from the neural net: tensor([0.0891, 0.0060, 0.0291, 0.0045, 0.0211, 0.0178, 0.0381, 0.0569, 0.0565,
        0.0314, 0.0236, 0.0128, 0.0729, 0.0327, 0.0473, 0.0166, 0.0117, 0.0157,
        0.0195, 0.1504, 0.0108, 0.0320, 0.0089, 0.1174, 0.0231, 0.0320, 0.0220])
label (actual next character): 13
probability assigned by the net to the the correct character: 0.03269490599632263
log likelihood: -3.4205360412597656
negative log likelihood: 3.4205360412597656
--------
bigram example 3: mm (indexes 13,13)
input to the neural net: 13
output probabilities from the neural net: tensor([0.0195, 0.0273, 0.0062, 0.0131, 0.0383, 0.0131, 0.0125, 0.0075, 0.1104,
        0.0372, 0.1063, 0.0705, 0.0349, 0.0278, 0.0252, 0.0377, 0.0182, 0.0966,
        0.0712, 0.0729, 0.0493, 0.0033, 0.0159, 0.0405, 0.0182, 0.0136, 0.0131])
label (actual next character): 13
probability assigned by the net to the the correct character: 0.02783992886543274
log likelihood: -3.5812840461730957
negative log likelihood: 3.5812840461730957
--------
bigram example 4: ma (indexes 13,1)
input to the neural net: 13
output probabilities from the neural net: tensor([0.0195, 0.0273, 0.0062, 0.0131, 0.0383, 0.0131, 0.0125, 0.0075, 0.1104,
        0.0372, 0.1063, 0.0705, 0.0349, 0.0278, 0.0252, 0.0377, 0.0182, 0.0966,
        0.0712, 0.0729, 0.0493, 0.0033, 0.0159, 0.0405, 0.0182, 0.0136, 0.0131])
label (actual next character): 1
probability assigned by the net to the the correct character: 0.0272862259298563
log likelihood: -3.6013731956481934
negative log likelihood: 3.6013731956481934
--------
bigram example 5: a. (indexes 1,0)
input to the neural net: 1
output probabilities from the neural net: tensor([0.0027, 0.0061, 0.0390, 0.0102, 0.0910, 0.0295, 0.1329, 0.0162, 0.0172,
        0.0319, 0.0388, 0.1890, 0.0263, 0.0320, 0.0246, 0.0096, 0.0153, 0.0135,
        0.0243, 0.0239, 0.0083, 0.1164, 0.0178, 0.0181, 0.0272, 0.0251, 0.0130])
label (actual next character): 0
probability assigned by the net to the the correct character: 0.0026549557223916054
log likelihood: -5.9313273429870605
negative log likelihood: 5.9313273429870605
=========
average negative log likelihood, i.e. loss = 3.8945975303649902</code></pre>
</div>
</div>
<p>overall loss: 3.8945975303649902</p>
<p>optimize a NN by starting w/ some random guess</p>
<ul>
<li>loss made up of differentiable operations</li>
<li>minimize loss by tuning <span class="math inline">\(\mathbf{W}\)</span>
<ul>
<li>computing gradients of loss wrt these matrices</li>
<li>find good setting of <span class="math inline">\(\mathbf{W}\)</span> using gradient-based optimization</li>
</ul></li>
</ul>
<p>we want to convert integers to vector</p>
<ul>
<li>our NN: single linear layer and softmax</li>
<li><code>loss.backward</code> (from micrograd) at output node initiated backprop; nudge parameters in opposite direction of gradient</li>
</ul>
<p>use negative log likelihood since we are doing classification</p>
<div class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb142"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a>probs.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="96">
<pre><code>torch.Size([5, 27])</code></pre>
</div>
</div>
<p>want to pluck out the right probabilities</p>
<p>e.g.&nbsp;first example – looking at probability at index 5; second example at index 13, etc.</p>
<div class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb144"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a>probs[<span class="dv">0</span>, <span class="dv">5</span>], probs[<span class="dv">1</span>, <span class="dv">13</span>], probs[<span class="dv">2</span>, <span class="dv">13</span>], probs[<span class="dv">3</span>, <span class="dv">1</span>], probs[<span class="dv">4</span>, <span class="dv">0</span>]</span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a><span class="co">#probabilities we want</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="97">
<pre><code>(tensor(0.0529),
 tensor(0.0327),
 tensor(0.0278),
 tensor(0.0273),
 tensor(0.0027))</code></pre>
</div>
</div>
<p>want a more efficient way to access these probs – in PyTorch, can pass in all of these integers in the vectors</p>
<div class="cell" data-execution_count="98">
<div class="sourceCode cell-code" id="cb146"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a>torch.arange(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="98">
<pre><code>tensor([0, 1, 2, 3, 4])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="101">
<div class="sourceCode cell-code" id="cb148"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a>probs[torch.arange(<span class="dv">5</span>), ys]</span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a><span class="co">#plucks out probs that NN assigns to the correct next character</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="101">
<pre><code>tensor([0.0529, 0.0327, 0.0278, 0.0273, 0.0027])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="102">
<div class="sourceCode cell-code" id="cb150"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="op">-</span>probs[torch.arange(<span class="dv">5</span>), ys].log().mean()</span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a>loss <span class="co">#vectorized form of earlier expression</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="102">
<pre><code>tensor(3.8946)</code></pre>
</div>
</div>
<p>pass in <code>requires_grad=True</code> to your weight matrix</p>
<div class="cell" data-execution_count="103">
<div class="sourceCode cell-code" id="cb152"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">27</span>), generator<span class="op">=</span>g, requires_grad<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="104">
<div class="sourceCode cell-code" id="cb153"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a><span class="co">#forward pass</span></span>
<span id="cb153-2"><a href="#cb153-2" aria-hidden="true" tabindex="-1"></a>xenc <span class="op">=</span> F.one_hot(xs, num_classes<span class="op">=</span><span class="dv">27</span>).<span class="bu">float</span>() </span>
<span id="cb153-3"><a href="#cb153-3" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> xenc <span class="op">@</span> W </span>
<span id="cb153-4"><a href="#cb153-4" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> logits.exp()</span>
<span id="cb153-5"><a href="#cb153-5" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb153-6"><a href="#cb153-6" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="op">-</span>probs[torch.arange(<span class="dv">5</span>), ys].log().mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="105">
<div class="sourceCode cell-code" id="cb154"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="co">#backward pass</span></span>
<span id="cb154-2"><a href="#cb154-2" aria-hidden="true" tabindex="-1"></a><span class="co">#make sure gradients are reset (set to None more efficent than 0)</span></span>
<span id="cb154-3"><a href="#cb154-3" aria-hidden="true" tabindex="-1"></a>W.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb154-4"><a href="#cb154-4" aria-hidden="true" tabindex="-1"></a>loss.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>.backward()</code> fills in gradients all the way back to parameters in <span class="math inline">\(\mathbf{W}\)</span></p>
<div class="cell" data-execution_count="106">
<div class="sourceCode cell-code" id="cb155"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a>W.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="106">
<pre><code>tensor([[ 0.0121,  0.0020,  0.0025,  0.0008,  0.0034, -0.1975,  0.0005,  0.0046,
          0.0027,  0.0063,  0.0016,  0.0056,  0.0018,  0.0016,  0.0100,  0.0476,
          0.0121,  0.0005,  0.0050,  0.0011,  0.0068,  0.0022,  0.0006,  0.0040,
          0.0024,  0.0307,  0.0292],
        [-0.1970,  0.0017,  0.0079,  0.0020,  0.0121,  0.0062,  0.0217,  0.0026,
          0.0025,  0.0010,  0.0205,  0.0017,  0.0198,  0.0022,  0.0046,  0.0041,
          0.0082,  0.0016,  0.0180,  0.0106,  0.0093,  0.0062,  0.0010,  0.0066,
          0.0131,  0.0101,  0.0018],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0058,  0.0159,  0.0050,  0.0104,  0.0398,  0.0058,  0.0019,  0.0067,
          0.0019,  0.0060,  0.0140,  0.0046,  0.0023, -0.1964,  0.0022,  0.0063,
          0.0058,  0.0009,  0.0183,  0.0043,  0.0097,  0.0060,  0.0100,  0.0005,
          0.0024,  0.0004,  0.0094],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0125, -0.1705,  0.0194,  0.0133,  0.0270,  0.0080,  0.0105,  0.0100,
          0.0490,  0.0066,  0.0030,  0.0316,  0.0052, -0.1893,  0.0059,  0.0045,
          0.0234,  0.0049,  0.0260,  0.0023,  0.0083,  0.0031,  0.0053,  0.0081,
          0.0482,  0.0187,  0.0051],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="108">
<div class="sourceCode cell-code" id="cb157"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a>W.grad.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="108">
<pre><code>torch.Size([27, 27])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="107">
<div class="sourceCode cell-code" id="cb159"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a>W.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="107">
<pre><code>torch.Size([27, 27])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="110">
<div class="sourceCode cell-code" id="cb161"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3.7693049907684326</code></pre>
</div>
</div>
<p>27x27 – same as W.grad’s shape</p>
<p>every element of <code>W.grad</code> tells us the influence of that weight on the loss fn</p>
<p>Ex. 0.0121 tells us this has a positive influence in loss; increases loss</p>
<p>Use gradient info to update weights of NN (no need to do a loop since we just have 1 tensor)</p>
<div class="cell" data-execution_count="109">
<div class="sourceCode cell-code" id="cb163"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a>W.data <span class="op">+=</span> <span class="op">-</span><span class="fl">0.1</span><span class="op">*</span>W.grad <span class="co">#update</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="111">
<div class="sourceCode cell-code" id="cb164"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb164-1"><a href="#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="co">#expect loss to decrease</span></span>
<span id="cb164-2"><a href="#cb164-2" aria-hidden="true" tabindex="-1"></a><span class="co">#forward again</span></span>
<span id="cb164-3"><a href="#cb164-3" aria-hidden="true" tabindex="-1"></a>xenc <span class="op">=</span> F.one_hot(xs, num_classes<span class="op">=</span><span class="dv">27</span>).<span class="bu">float</span>() </span>
<span id="cb164-4"><a href="#cb164-4" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> xenc <span class="op">@</span> W </span>
<span id="cb164-5"><a href="#cb164-5" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> logits.exp()</span>
<span id="cb164-6"><a href="#cb164-6" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb164-7"><a href="#cb164-7" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="op">-</span>probs[torch.arange(<span class="dv">5</span>), ys].log().mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="112">
<div class="sourceCode cell-code" id="cb165"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(loss.item()) <span class="co">#slightly lower</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3.7492127418518066</code></pre>
</div>
</div>
<div class="cell" data-execution_count="113">
<div class="sourceCode cell-code" id="cb167"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb167-1"><a href="#cb167-1" aria-hidden="true" tabindex="-1"></a><span class="co">#reset W.grad; backward pass</span></span>
<span id="cb167-2"><a href="#cb167-2" aria-hidden="true" tabindex="-1"></a>W.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb167-3"><a href="#cb167-3" aria-hidden="true" tabindex="-1"></a>loss.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="114">
<div class="sourceCode cell-code" id="cb168"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="co">#update</span></span>
<span id="cb168-2"><a href="#cb168-2" aria-hidden="true" tabindex="-1"></a>W.data <span class="op">+=</span> <span class="op">-</span><span class="fl">0.1</span><span class="op">*</span>W.grad </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="115">
<div class="sourceCode cell-code" id="cb169"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a><span class="co">#forward again...</span></span>
<span id="cb169-2"><a href="#cb169-2" aria-hidden="true" tabindex="-1"></a>xenc <span class="op">=</span> F.one_hot(xs, num_classes<span class="op">=</span><span class="dv">27</span>).<span class="bu">float</span>() </span>
<span id="cb169-3"><a href="#cb169-3" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> xenc <span class="op">@</span> W </span>
<span id="cb169-4"><a href="#cb169-4" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> logits.exp()</span>
<span id="cb169-5"><a href="#cb169-5" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb169-6"><a href="#cb169-6" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="op">-</span>probs[torch.arange(<span class="dv">5</span>), ys].log().mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="116">
<div class="sourceCode cell-code" id="cb170"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(loss.item()) <span class="co">#lower</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3.7291626930236816</code></pre>
</div>
</div>
<p>we are now doing gradient descent</p>
<p>still only iterating first word “emma”:</p>
<div class="cell" data-execution_count="125">
<div class="sourceCode cell-code" id="cb172"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a><span class="co">#create the dataset</span></span>
<span id="cb172-2"><a href="#cb172-2" aria-hidden="true" tabindex="-1"></a>xs, ys <span class="op">=</span> [], []</span>
<span id="cb172-3"><a href="#cb172-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> w <span class="kw">in</span> words[:<span class="dv">1</span>]:</span>
<span id="cb172-4"><a href="#cb172-4" aria-hidden="true" tabindex="-1"></a>  chs <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> <span class="bu">list</span>(w) <span class="op">+</span> [<span class="st">'.'</span>]</span>
<span id="cb172-5"><a href="#cb172-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb172-6"><a href="#cb172-6" aria-hidden="true" tabindex="-1"></a>    ix1 <span class="op">=</span> char_to_id[ch1]</span>
<span id="cb172-7"><a href="#cb172-7" aria-hidden="true" tabindex="-1"></a>    ix2 <span class="op">=</span> char_to_id[ch2]</span>
<span id="cb172-8"><a href="#cb172-8" aria-hidden="true" tabindex="-1"></a>    xs.append(ix1)</span>
<span id="cb172-9"><a href="#cb172-9" aria-hidden="true" tabindex="-1"></a>    ys.append(ix2)</span>
<span id="cb172-10"><a href="#cb172-10" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> torch.tensor(xs)</span>
<span id="cb172-11"><a href="#cb172-11" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> torch.tensor(ys)</span>
<span id="cb172-12"><a href="#cb172-12" aria-hidden="true" tabindex="-1"></a>num <span class="op">=</span> xs.nelement() <span class="co">#counts # examples -- 5 bigrams in "emma"</span></span>
<span id="cb172-13"><a href="#cb172-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'number of examples: '</span>, num)</span>
<span id="cb172-14"><a href="#cb172-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb172-15"><a href="#cb172-15" aria-hidden="true" tabindex="-1"></a><span class="co">#initialize the 'network'</span></span>
<span id="cb172-16"><a href="#cb172-16" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb172-17"><a href="#cb172-17" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">27</span>), generator<span class="op">=</span>g, requires_grad<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>number of examples:  5</code></pre>
</div>
</div>
<div class="cell" data-execution_count="126">
<div class="sourceCode cell-code" id="cb174"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a><span class="co">#gradient descent</span></span>
<span id="cb174-2"><a href="#cb174-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>): <span class="co">#10 iteractions of forward backward update</span></span>
<span id="cb174-3"><a href="#cb174-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb174-4"><a href="#cb174-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">#forward pass</span></span>
<span id="cb174-5"><a href="#cb174-5" aria-hidden="true" tabindex="-1"></a>  xenc <span class="op">=</span> F.one_hot(xs, num_classes<span class="op">=</span><span class="dv">27</span>).<span class="bu">float</span>() <span class="co">#input to the network: one-hot encoding</span></span>
<span id="cb174-6"><a href="#cb174-6" aria-hidden="true" tabindex="-1"></a>  logits <span class="op">=</span> xenc <span class="op">@</span> W <span class="co">#predict log-counts</span></span>
<span id="cb174-7"><a href="#cb174-7" aria-hidden="true" tabindex="-1"></a>  counts <span class="op">=</span> logits.exp() <span class="co">#counts, equivalent to N</span></span>
<span id="cb174-8"><a href="#cb174-8" aria-hidden="true" tabindex="-1"></a>  probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>) <span class="co">#probs for next character</span></span>
<span id="cb174-9"><a href="#cb174-9" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> <span class="op">-</span>probs[torch.arange(num), ys].log().mean()</span>
<span id="cb174-10"><a href="#cb174-10" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(loss.item())</span>
<span id="cb174-11"><a href="#cb174-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb174-12"><a href="#cb174-12" aria-hidden="true" tabindex="-1"></a>  <span class="co">#backward pass</span></span>
<span id="cb174-13"><a href="#cb174-13" aria-hidden="true" tabindex="-1"></a>  W.grad <span class="op">=</span> <span class="va">None</span> <span class="co">#zero the gradient</span></span>
<span id="cb174-14"><a href="#cb174-14" aria-hidden="true" tabindex="-1"></a>  loss.backward()</span>
<span id="cb174-15"><a href="#cb174-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb174-16"><a href="#cb174-16" aria-hidden="true" tabindex="-1"></a>  <span class="co">#update</span></span>
<span id="cb174-17"><a href="#cb174-17" aria-hidden="true" tabindex="-1"></a>  W.data <span class="op">+=</span> <span class="op">-</span><span class="fl">0.1</span><span class="op">*</span>W.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3.7693049907684326
3.7492127418518066
3.7291626930236816
3.7091546058654785
3.6891887187957764
3.6692662239074707
3.6493873596191406
3.629552125930786
3.6097614765167236
3.5900158882141113</code></pre>
</div>
</div>
<p>gives us some improvement on loss fn</p>
<p>now w/ whole dataset:</p>
<div class="cell" data-execution_count="127">
<div class="sourceCode cell-code" id="cb176"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb176-1"><a href="#cb176-1" aria-hidden="true" tabindex="-1"></a><span class="co">#whole dataset</span></span>
<span id="cb176-2"><a href="#cb176-2" aria-hidden="true" tabindex="-1"></a>xs, ys <span class="op">=</span> [], []</span>
<span id="cb176-3"><a href="#cb176-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb176-4"><a href="#cb176-4" aria-hidden="true" tabindex="-1"></a>  chs <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> <span class="bu">list</span>(w) <span class="op">+</span> [<span class="st">'.'</span>]</span>
<span id="cb176-5"><a href="#cb176-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb176-6"><a href="#cb176-6" aria-hidden="true" tabindex="-1"></a>    ix1 <span class="op">=</span> char_to_id[ch1]</span>
<span id="cb176-7"><a href="#cb176-7" aria-hidden="true" tabindex="-1"></a>    ix2 <span class="op">=</span> char_to_id[ch2]</span>
<span id="cb176-8"><a href="#cb176-8" aria-hidden="true" tabindex="-1"></a>    xs.append(ix1)</span>
<span id="cb176-9"><a href="#cb176-9" aria-hidden="true" tabindex="-1"></a>    ys.append(ix2)</span>
<span id="cb176-10"><a href="#cb176-10" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> torch.tensor(xs)</span>
<span id="cb176-11"><a href="#cb176-11" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> torch.tensor(ys)</span>
<span id="cb176-12"><a href="#cb176-12" aria-hidden="true" tabindex="-1"></a>num <span class="op">=</span> xs.nelement()</span>
<span id="cb176-13"><a href="#cb176-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'number of examples: '</span>, num)</span>
<span id="cb176-14"><a href="#cb176-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb176-15"><a href="#cb176-15" aria-hidden="true" tabindex="-1"></a><span class="co">#initialize the 'network'</span></span>
<span id="cb176-16"><a href="#cb176-16" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb176-17"><a href="#cb176-17" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">27</span>), generator<span class="op">=</span>g, requires_grad<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>number of examples:  228146</code></pre>
</div>
</div>
<div class="cell" data-execution_count="128">
<div class="sourceCode cell-code" id="cb178"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb178-1"><a href="#cb178-1" aria-hidden="true" tabindex="-1"></a><span class="co">#gradient descent</span></span>
<span id="cb178-2"><a href="#cb178-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb178-3"><a href="#cb178-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb178-4"><a href="#cb178-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">#forward pass</span></span>
<span id="cb178-5"><a href="#cb178-5" aria-hidden="true" tabindex="-1"></a>  xenc <span class="op">=</span> F.one_hot(xs, num_classes<span class="op">=</span><span class="dv">27</span>).<span class="bu">float</span>() </span>
<span id="cb178-6"><a href="#cb178-6" aria-hidden="true" tabindex="-1"></a>  logits <span class="op">=</span> xenc <span class="op">@</span> W <span class="co">#predict log-counts</span></span>
<span id="cb178-7"><a href="#cb178-7" aria-hidden="true" tabindex="-1"></a>  counts <span class="op">=</span> logits.exp() <span class="co">#counts &lt;=&gt; N</span></span>
<span id="cb178-8"><a href="#cb178-8" aria-hidden="true" tabindex="-1"></a>  probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>) <span class="co">#probs for next character</span></span>
<span id="cb178-9"><a href="#cb178-9" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> <span class="op">-</span>probs[torch.arange(num), ys].log().mean()</span>
<span id="cb178-10"><a href="#cb178-10" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(loss.item())</span>
<span id="cb178-11"><a href="#cb178-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb178-12"><a href="#cb178-12" aria-hidden="true" tabindex="-1"></a>  <span class="co">#backward pass</span></span>
<span id="cb178-13"><a href="#cb178-13" aria-hidden="true" tabindex="-1"></a>  W.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb178-14"><a href="#cb178-14" aria-hidden="true" tabindex="-1"></a>  loss.backward()</span>
<span id="cb178-15"><a href="#cb178-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb178-16"><a href="#cb178-16" aria-hidden="true" tabindex="-1"></a>  <span class="co">#update</span></span>
<span id="cb178-17"><a href="#cb178-17" aria-hidden="true" tabindex="-1"></a>  W.data <span class="op">+=</span> <span class="op">-</span><span class="fl">0.1</span><span class="op">*</span>W.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3.758953332901001
3.758039712905884
3.757127523422241
3.756216526031494
3.7553062438964844
3.754396915435791
3.7534890174865723
3.7525813579559326
3.7516753673553467
3.750770330429077</code></pre>
</div>
</div>
<p>now optimizing entire training set</p>
<p>can afford larger learning rate</p>
<div class="cell" data-execution_count="129">
<div class="sourceCode cell-code" id="cb180"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a><span class="co">#whole dataset</span></span>
<span id="cb180-2"><a href="#cb180-2" aria-hidden="true" tabindex="-1"></a>xs, ys <span class="op">=</span> [], []</span>
<span id="cb180-3"><a href="#cb180-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb180-4"><a href="#cb180-4" aria-hidden="true" tabindex="-1"></a>  chs <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> <span class="bu">list</span>(w) <span class="op">+</span> [<span class="st">'.'</span>]</span>
<span id="cb180-5"><a href="#cb180-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb180-6"><a href="#cb180-6" aria-hidden="true" tabindex="-1"></a>    ix1 <span class="op">=</span> char_to_id[ch1]</span>
<span id="cb180-7"><a href="#cb180-7" aria-hidden="true" tabindex="-1"></a>    ix2 <span class="op">=</span> char_to_id[ch2]</span>
<span id="cb180-8"><a href="#cb180-8" aria-hidden="true" tabindex="-1"></a>    xs.append(ix1)</span>
<span id="cb180-9"><a href="#cb180-9" aria-hidden="true" tabindex="-1"></a>    ys.append(ix2)</span>
<span id="cb180-10"><a href="#cb180-10" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> torch.tensor(xs)</span>
<span id="cb180-11"><a href="#cb180-11" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> torch.tensor(ys)</span>
<span id="cb180-12"><a href="#cb180-12" aria-hidden="true" tabindex="-1"></a>num <span class="op">=</span> xs.nelement()</span>
<span id="cb180-13"><a href="#cb180-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'number of examples: '</span>, num)</span>
<span id="cb180-14"><a href="#cb180-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-15"><a href="#cb180-15" aria-hidden="true" tabindex="-1"></a><span class="co">#initialize the 'network'</span></span>
<span id="cb180-16"><a href="#cb180-16" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb180-17"><a href="#cb180-17" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">27</span>), generator<span class="op">=</span>g, requires_grad<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>number of examples:  228146</code></pre>
</div>
</div>
<div class="cell" data-execution_count="131">
<div class="sourceCode cell-code" id="cb182"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb182-1"><a href="#cb182-1" aria-hidden="true" tabindex="-1"></a><span class="co">#gradient descent</span></span>
<span id="cb182-2"><a href="#cb182-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb182-3"><a href="#cb182-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb182-4"><a href="#cb182-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">#forward pass</span></span>
<span id="cb182-5"><a href="#cb182-5" aria-hidden="true" tabindex="-1"></a>  xenc <span class="op">=</span> F.one_hot(xs, num_classes<span class="op">=</span><span class="dv">27</span>).<span class="bu">float</span>() </span>
<span id="cb182-6"><a href="#cb182-6" aria-hidden="true" tabindex="-1"></a>  logits <span class="op">=</span> xenc <span class="op">@</span> W <span class="co">#predict log-counts</span></span>
<span id="cb182-7"><a href="#cb182-7" aria-hidden="true" tabindex="-1"></a>  counts <span class="op">=</span> logits.exp() <span class="co">#counts &lt;=&gt; N</span></span>
<span id="cb182-8"><a href="#cb182-8" aria-hidden="true" tabindex="-1"></a>  probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>) <span class="co">#probs for next character</span></span>
<span id="cb182-9"><a href="#cb182-9" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> <span class="op">-</span>probs[torch.arange(num), ys].log().mean()</span>
<span id="cb182-10"><a href="#cb182-10" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(loss.item())</span>
<span id="cb182-11"><a href="#cb182-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb182-12"><a href="#cb182-12" aria-hidden="true" tabindex="-1"></a>  <span class="co">#backward pass</span></span>
<span id="cb182-13"><a href="#cb182-13" aria-hidden="true" tabindex="-1"></a>  W.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb182-14"><a href="#cb182-14" aria-hidden="true" tabindex="-1"></a>  loss.backward()</span>
<span id="cb182-15"><a href="#cb182-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb182-16"><a href="#cb182-16" aria-hidden="true" tabindex="-1"></a>  <span class="co">#update</span></span>
<span id="cb182-17"><a href="#cb182-17" aria-hidden="true" tabindex="-1"></a>  W.data <span class="op">+=</span> <span class="op">-</span><span class="dv">50</span><span class="op">*</span>W.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>2.4726526737213135
2.4724342823028564
2.4722204208374023
2.472010850906372
2.4718058109283447
2.4716055393218994
2.4714088439941406
2.4712166786193848
2.4710280895233154
2.47084379196167
2.4706625938415527
2.4704854488372803
2.4703118801116943
2.4701414108276367
2.4699742794036865
2.4698104858398438
2.4696500301361084
2.4694924354553223
2.4693377017974854
2.4691858291625977
2.4690372943878174
2.468891143798828
2.468747615814209
2.46860671043396
2.468468427658081
2.4683327674865723
2.4681990146636963
2.4680681228637695
2.4679393768310547
2.4678127765655518
2.46768856048584
2.46756649017334
2.4674463272094727
2.4673283100128174
2.467211961746216
2.467097759246826
2.4669857025146484
2.4668753147125244
2.466766595840454
2.4666597843170166
2.466554880142212
2.466451406478882
2.4663493633270264
2.4662492275238037
2.4661500453948975
2.4660532474517822
2.4659576416015625
2.4658634662628174
2.465770721435547
2.465679407119751
2.4655892848968506
2.465500593185425
2.4654133319854736
2.465327501296997
2.465242624282837
2.4651589393615723
2.4650766849517822
2.4649956226348877
2.4649155139923096
2.4648361206054688
2.46475887298584
2.464682102203369
2.464606285095215
2.464531660079956
2.4644579887390137
2.464385509490967
2.4643137454986572
2.464243173599243
2.4641737937927246
2.464104652404785
2.464036703109741
2.4639699459075928
2.4639036655426025
2.4638383388519287
2.463773727416992
2.463710308074951
2.4636473655700684
2.463585376739502
2.4635238647460938
2.463463544845581
2.4634039402008057
2.4633448123931885
2.4632863998413086
2.463228702545166
2.463172197341919
2.4631154537200928
2.463060140609741
2.4630050659179688
2.4629504680633545
2.4628970623016357
2.462843894958496
2.462791681289673
2.4627397060394287
2.462688684463501
2.4626379013061523
2.462587833404541
2.462538242340088
2.462489128112793
2.4624407291412354
2.462393045425415</code></pre>
</div>
</div>
<p>achieving roughly same result w/ gradient-based optimization (loss is still around 2.47) as explicit approach</p>
<p>Gradient-based approach a lot more flexible – can make NN more complex</p>
<ul>
<li>take multiple previous characters and feeding them into more complex NN</li>
<li>outputs will still be logits (go through same process)</li>
<li>even as we complexify all the way to transformers, only thing that will change: forward pass where we take in some previous characters and calculate logits for next character in sequence (get more complex)</li>
<li>same optimization machine</li>
<li>table approach would get too large once we have many previous characters</li>
</ul>
<p>Additional notes:</p>
<ol type="1">
<li><code>xenc</code> (one hot vectors) multiplied by <span class="math inline">\(\mathbf{W}\)</span> matrix – e.g.&nbsp;if you have a one hot vector in 5th dimension, multiplying that with w ends up plucking out fifth row of <span class="math inline">\(\mathbf{W}\)</span>
<ul>
<li>logits becomes fifth row of <span class="math inline">\(\mathbf{W}\)</span> from matrix multiplication</li>
<li>array from earlier approach = array <span class="math inline">\(\mathbf{W}\)</span> at the end of optimization w/ gradient-based approach (using loss fn to guide us)</li>
</ul></li>
<li>smoothing – increasing the count –&gt; probability gets more and more uniform –&gt; more and more even
<ul>
<li>gradient-based framework has an equivalent to smoothing:</li>
<li><span class="math inline">\(\mathbf{W}\)</span> – initialized randomly; think about it as initalizating all entries of <span class="math inline">\(\mathbf{W}\)</span> to 0 –&gt; logits become all zero –&gt; exponentiating those logits becomes all 1 –&gt; probabilities become all uniform</li>
<li>when entires of <span class="math inline">\(\mathbf{W}\)</span> are all 0, probabilities will be uniform</li>
<li>trying to incentivize <span class="math inline">\(\mathbf{W}\)</span> to be near 0 &lt;=&gt; label smoothing
<ul>
<li>more you incentivize that in loss fn, the smoother the distribution you’re going to achieve</li>
<li>=&gt; brings us to <strong>regularization</strong> where we can augment loss fn to have a small component called <strong>regularization loss</strong></li>
<li>ex. can take <span class="math inline">\(\mathbf{W}\)</span> and square all of its entries (no more signs – neg and pos get squashed to be positive numbers), then sum (or mean)
<ul>
<li>achieve zero loss if <span class="math inline">\(\mathbf{W}\)</span> is exactly 0; but if non-zero, you accumulate loss</li>
<li>can add to loss</li>
</ul></li>
</ul></li>
</ul></li>
</ol>
<div class="cell" data-execution_count="132">
<div class="sourceCode cell-code" id="cb184"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb184-1"><a href="#cb184-1" aria-hidden="true" tabindex="-1"></a><span class="co">#whole dataset</span></span>
<span id="cb184-2"><a href="#cb184-2" aria-hidden="true" tabindex="-1"></a>xs, ys <span class="op">=</span> [], []</span>
<span id="cb184-3"><a href="#cb184-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb184-4"><a href="#cb184-4" aria-hidden="true" tabindex="-1"></a>  chs <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> <span class="bu">list</span>(w) <span class="op">+</span> [<span class="st">'.'</span>]</span>
<span id="cb184-5"><a href="#cb184-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb184-6"><a href="#cb184-6" aria-hidden="true" tabindex="-1"></a>    ix1 <span class="op">=</span> char_to_id[ch1]</span>
<span id="cb184-7"><a href="#cb184-7" aria-hidden="true" tabindex="-1"></a>    ix2 <span class="op">=</span> char_to_id[ch2]</span>
<span id="cb184-8"><a href="#cb184-8" aria-hidden="true" tabindex="-1"></a>    xs.append(ix1)</span>
<span id="cb184-9"><a href="#cb184-9" aria-hidden="true" tabindex="-1"></a>    ys.append(ix2)</span>
<span id="cb184-10"><a href="#cb184-10" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> torch.tensor(xs)</span>
<span id="cb184-11"><a href="#cb184-11" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> torch.tensor(ys)</span>
<span id="cb184-12"><a href="#cb184-12" aria-hidden="true" tabindex="-1"></a>num <span class="op">=</span> xs.nelement()</span>
<span id="cb184-13"><a href="#cb184-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'number of examples: '</span>, num)</span>
<span id="cb184-14"><a href="#cb184-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb184-15"><a href="#cb184-15" aria-hidden="true" tabindex="-1"></a><span class="co">#initialize the 'network'</span></span>
<span id="cb184-16"><a href="#cb184-16" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb184-17"><a href="#cb184-17" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">27</span>), generator<span class="op">=</span>g, requires_grad<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>number of examples:  228146</code></pre>
</div>
</div>
<div class="cell" data-execution_count="133">
<div class="sourceCode cell-code" id="cb186"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb186-1"><a href="#cb186-1" aria-hidden="true" tabindex="-1"></a><span class="co">#gradient descent</span></span>
<span id="cb186-2"><a href="#cb186-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb186-3"><a href="#cb186-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb186-4"><a href="#cb186-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">#forward pass</span></span>
<span id="cb186-5"><a href="#cb186-5" aria-hidden="true" tabindex="-1"></a>  xenc <span class="op">=</span> F.one_hot(xs, num_classes<span class="op">=</span><span class="dv">27</span>).<span class="bu">float</span>() </span>
<span id="cb186-6"><a href="#cb186-6" aria-hidden="true" tabindex="-1"></a>  logits <span class="op">=</span> xenc <span class="op">@</span> W <span class="co">#predict log-counts</span></span>
<span id="cb186-7"><a href="#cb186-7" aria-hidden="true" tabindex="-1"></a>  counts <span class="op">=</span> logits.exp() <span class="co">#counts &lt;=&gt; N</span></span>
<span id="cb186-8"><a href="#cb186-8" aria-hidden="true" tabindex="-1"></a>  probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>) <span class="co">#probs for next character</span></span>
<span id="cb186-9"><a href="#cb186-9" aria-hidden="true" tabindex="-1"></a>  <span class="co">#regularization strength: 0.01</span></span>
<span id="cb186-10"><a href="#cb186-10" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> <span class="op">-</span>probs[torch.arange(num), ys].log().mean() <span class="op">+</span>  <span class="fl">0.01</span><span class="op">*</span>(W<span class="op">**</span><span class="dv">2</span>).mean()</span>
<span id="cb186-11"><a href="#cb186-11" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(loss.item())</span>
<span id="cb186-12"><a href="#cb186-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb186-13"><a href="#cb186-13" aria-hidden="true" tabindex="-1"></a>  <span class="co">#backward pass</span></span>
<span id="cb186-14"><a href="#cb186-14" aria-hidden="true" tabindex="-1"></a>  W.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb186-15"><a href="#cb186-15" aria-hidden="true" tabindex="-1"></a>  loss.backward()</span>
<span id="cb186-16"><a href="#cb186-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb186-17"><a href="#cb186-17" aria-hidden="true" tabindex="-1"></a>  <span class="co">#update</span></span>
<span id="cb186-18"><a href="#cb186-18" aria-hidden="true" tabindex="-1"></a>  W.data <span class="op">+=</span> <span class="op">-</span><span class="dv">50</span><span class="op">*</span>W.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3.768618583679199
3.3788068294525146
3.161090850830078
3.0271859169006348
2.9344840049743652
2.867231607437134
2.8166542053222656
2.777146577835083
2.745253801345825
2.7188305854797363
2.696505308151245
2.6773719787597656
2.6608052253723145
2.6463515758514404
2.633665084838867
2.622471570968628
2.6125476360321045
2.6037068367004395
2.595794439315796
2.5886807441711426
2.5822560787200928
2.576429843902588
2.5711236000061035
2.566272735595703
2.5618226528167725
2.5577261447906494
2.5539441108703613
2.550442695617676
2.5471930503845215
2.5441696643829346
2.5413522720336914
2.538722038269043
2.536262035369873
2.5339579582214355
2.531797409057617
2.529768228530884
2.527860164642334
2.5260636806488037
2.5243704319000244
2.522773265838623
2.52126407623291
2.519836664199829
2.5184857845306396
2.5172054767608643
2.515990734100342
2.5148372650146484
2.5137407779693604
2.51269793510437
2.511704921722412
2.5107579231262207
2.509855031967163
2.5089924335479736
2.5081686973571777
2.507380247116089
2.5066258907318115
2.5059030055999756
2.5052106380462646
2.5045459270477295
2.5039076805114746
2.503295421600342
2.5027058124542236
2.5021398067474365
2.501594305038452
2.5010695457458496
2.500562906265259
2.500075578689575
2.4996049404144287
2.499150514602661
2.4987120628356934
2.49828839302063
2.4978787899017334
2.497483015060425
2.4970996379852295
2.4967286586761475
2.496370315551758
2.4960227012634277
2.4956860542297363
2.4953596591949463
2.4950432777404785
2.494736433029175
2.494438886642456
2.494149684906006
2.4938690662384033
2.4935970306396484
2.4933321475982666
2.493075132369995
2.4928252696990967
2.492582321166992
2.4923462867736816
2.492116928100586
2.4918932914733887
2.491675853729248
2.491464614868164
2.491258382797241
2.491057872772217
2.4908623695373535
2.4906721115112305
2.4904870986938477
2.4903063774108887
2.490130662918091</code></pre>
</div>
</div>
<p>now optimization has 2 components:</p>
<ol type="1">
<li>trying to make all probs work out</li>
<li>trying to make all <span class="math inline">\(\mathbf{W}\)</span> entries be 0
<ul>
<li>if non-zero, you feel a loss; only way to achieve minimization of this is for <span class="math inline">\(\mathbf{W}\)</span> to be 0</li>
<li>will want to be 0; probs will want to be uniform</li>
<li>simultaneously want to match up probs as indicated by the data</li>
<li>strength of regularization is controlling amount of counts added (to “smooth”) –&gt; the more the second part of regularization loss (“penalty term”) will dominate –&gt; the more the weights unable to grow (since they will accumulate way too much loss)</li>
</ul></li>
</ol>
<p>Sampling from this NN</p>
<ul>
<li>before, we would grab a row of <code>P</code> (probability row) from which we sampled the next index; break when 0</li>
<li>now: p comes from NN; encode <code>ix</code> into one hot row of <code>xenc</code>
<ul>
<li><code>xenc</code> multiplies with <code>W</code> –&gt; plucls out corresponding row of <code>W</code> corresponding to <code>ix</code> –&gt; gets logits</li>
<li>normalize logits –&gt; exponentiate to get counts –&gt; normalize to get distribution –&gt; sample from the distribution</li>
</ul></li>
<li>end up w/ same result and previous approach since they are identical models; <span class="math inline">\(\mathbf{W}\)</span> is log counts of what we’ve estimated before</li>
</ul>
<div class="cell" data-execution_count="134">
<div class="sourceCode cell-code" id="cb188"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb188-1"><a href="#cb188-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb188-2"><a href="#cb188-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb188-3"><a href="#cb188-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb188-4"><a href="#cb188-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb188-5"><a href="#cb188-5" aria-hidden="true" tabindex="-1"></a>  out <span class="op">=</span> []</span>
<span id="cb188-6"><a href="#cb188-6" aria-hidden="true" tabindex="-1"></a>  ix <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb188-7"><a href="#cb188-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb188-8"><a href="#cb188-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb188-9"><a href="#cb188-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ----------</span></span>
<span id="cb188-10"><a href="#cb188-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># BEFORE:</span></span>
<span id="cb188-11"><a href="#cb188-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">#p = P[ix]</span></span>
<span id="cb188-12"><a href="#cb188-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ----------</span></span>
<span id="cb188-13"><a href="#cb188-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># NOW:</span></span>
<span id="cb188-14"><a href="#cb188-14" aria-hidden="true" tabindex="-1"></a>    xenc <span class="op">=</span> F.one_hot(torch.tensor([ix]), num_classes<span class="op">=</span><span class="dv">27</span>).<span class="bu">float</span>()</span>
<span id="cb188-15"><a href="#cb188-15" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> xenc <span class="op">@</span> W <span class="co"># predict log-counts</span></span>
<span id="cb188-16"><a href="#cb188-16" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> logits.exp() <span class="co"># counts, equivalent to N</span></span>
<span id="cb188-17"><a href="#cb188-17" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>) <span class="co"># probabilities for next character</span></span>
<span id="cb188-18"><a href="#cb188-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ----------</span></span>
<span id="cb188-19"><a href="#cb188-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb188-20"><a href="#cb188-20" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> torch.multinomial(p, num_samples<span class="op">=</span><span class="dv">1</span>, replacement<span class="op">=</span><span class="va">True</span>, generator<span class="op">=</span>g).item()</span>
<span id="cb188-21"><a href="#cb188-21" aria-hidden="true" tabindex="-1"></a>    out.append(id_to_char[ix])</span>
<span id="cb188-22"><a href="#cb188-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ix <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb188-23"><a href="#cb188-23" aria-hidden="true" tabindex="-1"></a>      <span class="cf">break</span></span>
<span id="cb188-24"><a href="#cb188-24" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">''</span>.join(out))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>junide.
janasah.
p.
cfay.
a.</code></pre>
</div>
</div>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>